#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†ä¸‹è½½å·¥å…·
ä»Hugging Faceä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†å¹¶è½¬æ¢ä¸ºç³»ç»Ÿé¢„ç½®æ–‡æ¡£æ ¼å¼
"""

import os
import json
import argparse
from datetime import datetime
from typing import Dict, List, Any
from tqdm import tqdm

def download_wikipedia_dataset(dataset_name: str = "fjcanyue/wikipedia-zh-cn", 
                             split: str = "train", 
                             max_samples: int = 1000,
                             output_file: str = "data/preloaded_documents.json") -> Dict[str, str]:
    """
    ä»Hugging Faceä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        split: æ•°æ®é›†åˆ†å‰² (train, validation, test)
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, str]: æ–‡æ¡£å­—å…¸ {doc_id: content}
    """
    
    print(f"ğŸ” å¼€å§‹ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“Š åˆ†å‰²: {split}, æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    print(f"ğŸ’¡ åªä¸‹è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶: wikipedia-zh-cn-20240901.json")
    
    try:
        # ä½¿ç”¨datasetsåº“ä¸‹è½½
        from datasets import load_dataset
        print("ğŸ“¦ ä½¿ç”¨datasetsåº“ä¸‹è½½...")
        
        import os
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10åˆ†é’Ÿè¶…æ—¶
        
        dataset = load_dataset(dataset_name, split=split, streaming=True, trust_remote_code=True)
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        
        documents = {}
        sample_count = 0
        print(f"ğŸ”„ å¤„ç†å‰{max_samples}ä¸ªæ ·æœ¬...")
        
        for i, sample in enumerate(tqdm(dataset, desc="å¤„ç†æ ·æœ¬")):
            if sample_count >= max_samples:
                break
                
            title = sample.get("title", "")
            text = sample.get("text", "")
            tags = sample.get("tags", "")
            
            # åªä¿ç•™çº¯æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…å«æ ‡é¢˜å’Œæ ‡ç­¾
            content = text.strip()
            
            doc_id = f"wiki_{i:06d}"
            documents[doc_id] = content
            sample_count += 1
            
        print(f"âœ… æˆåŠŸå¤„ç† {sample_count} ä¸ªæ ·æœ¬")
        return documents
        
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…datasetsåº“: pip install datasets")
        return {}
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return {}

def save_documents(documents: Dict[str, str], output_file: str):
    """ä¿å­˜æ–‡æ¡£åˆ°JSONæ–‡ä»¶"""
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # æ„å»ºè¾“å‡ºæ•°æ®
    output_data = {
        "export_time": datetime.now().isoformat(),
        "dataset_info": {
            "name": "Wikipedia Chinese",
            "description": "ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†",
            "total_documents": len(documents),
            "source": "Hugging Face - fjcanyue/wikipedia-zh-cn"
        },
        "documents": documents
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ–‡æ¡£å·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š å…±ä¿å­˜ {len(documents)} ä¸ªæ–‡æ¡£")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†ä¸‹è½½å·¥å…·")
    parser.add_argument("--dataset", default="fjcanyue/wikipedia-zh-cn",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--split", default="train", 
                       choices=["train", "validation", "test"],
                       help="æ•°æ®é›†åˆ†å‰²")
    parser.add_argument("--max-samples", type=int, default=1000,
                       help="æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--output", default="data/preloaded_documents.json",
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ“š ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†ä¸‹è½½å·¥å…·")
    print("=" * 60)
    
    print(f"ğŸ” ä¸‹è½½æ•°æ®é›†: {args.dataset}")
    documents = download_wikipedia_dataset(
        dataset_name=args.dataset,
        split=args.split,
        max_samples=args.max_samples,
        output_file=args.output
    )
    
    if documents:
        save_documents(documents, args.output)
        print("\nâœ… å®Œæˆï¼")
        print(f"ğŸ’¡ ç°åœ¨å¯ä»¥å¯åŠ¨ç³»ç»Ÿï¼Œé¢„ç½®æ–‡æ¡£å°†è‡ªåŠ¨åŠ è½½")
        print(f"ğŸ“– æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {args.output}")
    else:
        print("\nâŒ å¤±è´¥ï¼")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. å®‰è£…datasetsåº“: pip install datasets")
        print("   3. ç¡®è®¤æ•°æ®é›†åç§°æ­£ç¡®")

if __name__ == "__main__":
    main()
