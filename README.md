# ğŸ”¬ AI System Testbed

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/tylerelyt/test_bed)

An advanced AI-powered search platform featuring three core capabilities: **Search & Recommendation**, **Retrieval-Augmented Generation (RAG)**, and **Image Search**. Built with modern MLOps practices for production-ready deployment.

## ğŸŒŸ Features

### ğŸ¯ Three Core Capabilities

#### 1. ğŸ” Search & Recommendation System
- **Intelligent Indexing**: TF-IDF based inverted index with Chinese word segmentation
- **CTR Prediction**: Advanced machine learning models (Logistic Regression & Wide & Deep) for click-through rate prediction
- **Real-time Ranking**: Dynamic ranking strategy adjustment based on user behavior
- **Knowledge Graph**: LLM-based NER technology for enhanced semantic search
- **A/B Testing**: Experiment management for ranking algorithm comparison

#### 2. ğŸ¤– Retrieval-Augmented Generation (RAG)
- **Hybrid Retrieval**: Combines inverted index and knowledge graph for comprehensive information retrieval
- **LLM Integration**: Seamless integration with Ollama for local LLM inference
- **Prompt Engineering**: Optimized prompt templates with full transparency
- **Context Management**: Intelligent context selection and ranking for accurate responses
- **Multi-source RAG**: Retrieval from documents, knowledge graphs, and structured data

#### 3. ğŸ–¼ï¸ Image Search System
- **CLIP-powered**: OpenAI CLIP model via Hugging Face Transformers
- **Multi-modal Search**: Image-to-image and text-to-image search capabilities
- **Semantic Understanding**: 512-dimensional embedding vectors for precise similarity matching
- **Real-time Processing**: Sub-second search response with efficient similarity calculation
- **Scalable Storage**: Unlimited image library with optimized storage management

### ğŸ—ï¸ Shared Infrastructure
- **Microservice Architecture**: Decoupled services (Data, Index, Model, Image, Experiment)
- **Unified Service Management**: Centralized service discovery and management
- **MLOps Pipeline**: Complete workflow from data collection to model deployment
- **Monitoring & Observability**: Real-time performance tracking and health checks
- **Web Interface**: Modern Gradio-based UI with responsive design
- **Production Ready**: Comprehensive error handling, logging, and scalability features

## ğŸš€ Quick Start

### Requirements

- Python 3.8+
- Memory: At least 2GB
- Storage: At least 1GB available space
- GPU (optional): For better CLIP model performance

### Optional Dependencies

- Ollama (for RAG/KG): local LLM inference service, default at `http://localhost:11434`
- datasets (for data tools): `pip install datasets`, used by `tools/wikipedia_downloader.py`

### Installation

```bash
# Clone the repository
git clone https://github.com/tylerelyt/test_bed.git
cd test_bed

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Preloaded Dataset (Read-Only)

The system comes with 100 preloaded Chinese Wikipedia documents as core dataset:
- **Immutable**: Preloaded documents are core; deleting or editing them via the UI is not supported in this version
- **Auto-loading**: Automatically loads `data/preloaded_documents.json` at startup
- **User Documents**: Importing new documents via the UI is not supported in this version (offline builds are possible via `offline_index` or the tooling)
- **Data Source**: From Hugging Face `fjcanyue/wikipedia-zh-cn` dataset

The preloaded documents provide rich foundational data covering mathematics, philosophy, literature, history, computer science, and other domains.

### Start the System

```bash
# Method 1: Using startup script
./quick_start.sh

# Method 2: Direct startup
python start_system.py
```

After the system starts, visit http://localhost:7861 to use the interface.

### Configuration

You can override default settings via environment variables (see `src/search_engine/config.py` for defaults):

```bash
# Web UI port (default 7861)
export UI_PORT=7861

# Ollama configuration (used by RAG & KG)
export OLLAMA_URL=http://localhost:11434

# Default RAG model
export LLM_MODEL="llama3.1:8b"

# Default KG/NER model
export KG_LLM_MODEL="qwen2.5:7b"
```

### System Architecture Overview

The platform is organized into **three main functional areas** with shared infrastructure:

#### ğŸ” Search & Recommendation Module
- **Index Building Tab**: Offline index construction, document management, and knowledge graph building
- **Search Tab**: Online retrieval and ranking with CTR-based optimization  
- **Training Tab**: CTR data collection and Wide & Deep model training

#### ğŸ¤– RAG Module
- **RAG Q&A Tab**: Retrieval-Augmented Generation system with Ollama integration
- **Knowledge Graph Integration**: Semantic search with LLM-based entity recognition
- **Multi-source Retrieval**: Documents, graphs, and structured data integration

> Note: RAG/KG rely on a locally running Ollama service and available models. If Ollama is not running or the model hasn't been pulled, the page will show a connection error, but other parts of the system remain available.

#### ğŸ–¼ï¸ Image Search Module
- **Image Search Tab**: CLIP-based image retrieval supporting image-to-image and text-to-image search
- **Image Management**: Upload, indexing, and library management
- **Multi-modal Understanding**: Cross-modal semantic search capabilities

#### ğŸ—ï¸ Shared Infrastructure
- **Service Management**: Unified service discovery and orchestration
- **Monitoring Tab**: System performance monitoring and health checks
- **Data Pipeline**: Centralized data processing and storage
- **Web Interface**: Modern responsive UI with Gradio framework

## ğŸ–¼ï¸ Image Search System

### Overview

The image search system leverages OpenAI's CLIP model to provide intelligent image retrieval capabilities:

- **ğŸ“¤ Image Upload**: Store images with descriptions and tags
- **ğŸ” Image-to-Image Search**: Find visually similar images using query images
- **ğŸ’¬ Text-to-Image Search**: Search images using natural language descriptions
- **ğŸ“‹ Image Management**: Comprehensive image library management

### Technical Details

- **Model**: OpenAI CLIP ViT-B/32 via Hugging Face Transformers
- **Embedding Dimension**: 512-dimensional vectors
- **Similarity Metric**: Cosine similarity
- **Supported Formats**: JPG, PNG, GIF, BMP, and more
- **Performance**: Sub-second search response times

### Usage Examples

#### Text-to-Image Search
```python
# Examples of search queries
"a red car on the street"
"cat sleeping on a bed"
"beautiful sunset landscape"
"person running"  # Non-English queries are also supported
```

#### Upload and Index Images
1. Navigate to "ğŸ–¼ï¸ Image Search System" â†’ "ğŸ“¤ Image Upload"
2. Select image files and add descriptions/tags
3. Click "ğŸ“¤ Upload Image" to index

#### Search Similar Images
1. Go to "ğŸ” Image-to-Image" tab
2. Upload a query image
3. Adjust the number of results (1-20)
4. View results in table and gallery format

For detailed usage instructions, see [Image Search Guide](docs/IMAGE_SEARCH_GUIDE.md).

## ğŸ“– User Guide

### Basic Usage

1. **Index Building**: The system automatically builds indexes on startup, with manual document addition support
2. **Search Testing**: Enter queries in the search box to retrieve relevant documents
3. **Click Feedback**: Clicking search results records user behavior for model training
4. **Model Training**: After collecting sufficient data, train CTR prediction models

### Advanced Features

#### 1. Batch Data Import

```python
from src.search_engine.data_utils import import_ctr_data
result = import_ctr_data("path/to/your/data.json")
```

#### 2. Custom Ranking Strategy

```python
from src.search_engine.service_manager import get_index_service
index_service = get_index_service()
results = index_service.search("query terms", top_k=10)
```

#### 3. Experiment Management

The system supports A/B testing with configurable ranking strategies for comparison in the monitoring interface.

## ğŸ—ï¸ Architecture Design

### System Architecture

```mermaid
graph TB
    subgraph "ğŸ–¥ï¸ Web Interface Layer"
        Portal["Portal<br/>ğŸšª Main Entry"]
    end
    
    subgraph "ğŸ“± Application Layer"
        SearchMod["ğŸ” Search & Recommendation<br/>â€¢ Index Building<br/>â€¢ Text Search<br/>â€¢ CTR Training"]
        RAGMod["ğŸ¤– RAG Module<br/>â€¢ RAG Q&A<br/>â€¢ Knowledge Graph<br/>â€¢ Multi-source Retrieval"]
        ImageMod["ğŸ–¼ï¸ Image Search<br/>â€¢ Image Upload<br/>â€¢ Image-to-Image<br/>â€¢ Text-to-Image"]
    end
    
    subgraph "ğŸ—ï¸ Service Layer"
        DataSvc["DataService<br/>ğŸ“Š CTR Data Management"]
        IndexSvc["IndexService<br/>ğŸ“š Text Indexing & Search"]
        ModelSvc["ModelService<br/>ğŸ¤– ML Model Management"]
        ImageSvc["ImageService<br/>ğŸ–¼ï¸ CLIP-based Search"]
        ExpSvc["ExperimentService<br/>ğŸ§ª A/B Testing"]
    end
    
    subgraph "ğŸ“Š Infrastructure Layer"
        Monitor["Monitoring<br/>ğŸ“ˆ Performance Tracking"]
        Storage["Storage<br/>ğŸ’¾ Data Persistence"]
        ServiceMgr["ServiceManager<br/>ğŸ”§ Service Orchestration"]
    end
    
    Portal --> SearchMod
    Portal --> RAGMod
    Portal --> ImageMod
    
    SearchMod --> DataSvc
    SearchMod --> IndexSvc
    SearchMod --> ModelSvc
    
    RAGMod --> IndexSvc
    RAGMod --> ModelSvc
    
    ImageMod --> ImageSvc
    
    DataSvc --> ServiceMgr
    IndexSvc --> ServiceMgr
    ModelSvc --> ServiceMgr
    ImageSvc --> ServiceMgr
    ExpSvc --> ServiceMgr
    
    ServiceMgr --> Monitor
    ServiceMgr --> Storage
```

### Data Flow

```mermaid
graph LR
    subgraph "ğŸ” Search & Recommendation Flow"
        A1[User Query] --> A2[Index Retrieval]
        A2 --> A3[Initial Ranking]
        A3 --> A4[CTR Prediction]
        A4 --> A5[Re-ranking]
        A5 --> A6[Results Display]
        A6 --> A7[User Click]
        A7 --> A8[Behavior Recording]
        A8 --> A9[Model Training]
        A9 --> A4
    end
    
    subgraph "ğŸ¤– RAG Flow"
        B1[User Question] --> B2[Document Retrieval]
        B2 --> B3[Knowledge Graph Query]
        B3 --> B4[Context Assembly]
        B4 --> B5[LLM Generation]
        B5 --> B6[Response Display]
    end
    
    subgraph "ğŸ–¼ï¸ Image Search Flow"
        C1[Image/Text Query] --> C2[CLIP Encoding]
        C2 --> C3[Similarity Calculation]
        C3 --> C4[Result Ranking]
        C4 --> C5[Image Gallery Display]
        C5 --> C6[User Interaction]
        C6 --> C7[Usage Analytics]
    end
```

## ğŸ“Š Performance Metrics

### Text Search Performance
- **Search Latency**: < 100ms (10K documents)
- **Concurrent Users**: 100+ concurrent users
- **Memory Usage**: < 500MB (basic configuration)
- **Storage Efficiency**: Compression ratio > 70%

### Image Search Performance
- **CLIP Encoding**: 1-3 seconds per image (CPU)
- **Search Response**: Sub-second similarity calculation
- **Embedding Storage**: ~2KB per image
- **Supported Scale**: Unlimited image library size

## ğŸ› ï¸ Development Guide

### Project Structure

```
Testbed/
â”œâ”€â”€ src/                          # Source code
â”‚   â””â”€â”€ search_engine/           
â”‚       â”œâ”€â”€ data_service.py            # Data service (CTR data management)
â”‚       â”œâ”€â”€ index_service.py           # Index service (text search & indexing)
â”‚       â”œâ”€â”€ model_service.py           # Model service (CTR & Wide&Deep models)
â”‚       â”œâ”€â”€ image_service.py           # Image service (CLIP-based image search)
â”‚       â”œâ”€â”€ experiment_service.py      # Experiment management service
â”‚       â”œâ”€â”€ service_manager.py         # Service manager (unified service access)
â”‚       â”œâ”€â”€ data_utils.py              # Data processing utilities
â”‚       â”œâ”€â”€ portal.py                  # Main UI entry point
â”‚       â”œâ”€â”€ index_tab/                 # Index building & knowledge graph UI
â”‚       â”‚   â”œâ”€â”€ index_tab.py
â”‚       â”‚   â”œâ”€â”€ knowledge_graph.py
â”‚       â”‚   â”œâ”€â”€ ner_service.py
â”‚       â”‚   â””â”€â”€ offline_index.py
â”‚       â”œâ”€â”€ search_tab/                # Text search UI
â”‚       â”‚   â”œâ”€â”€ search_tab.py
â”‚       â”‚   â””â”€â”€ search_engine.py
â”‚       â”œâ”€â”€ image_tab/                 # Image search UI
â”‚       â”‚   â””â”€â”€ image_tab.py
â”‚       â”œâ”€â”€ training_tab/              # Model training UI
â”‚       â”‚   â”œâ”€â”€ training_tab.py
â”‚       â”‚   â”œâ”€â”€ ctr_model.py
â”‚       â”‚   â”œâ”€â”€ ctr_wide_deep_model.py
â”‚       â”‚   â””â”€â”€ ctr_config.py
â”‚       â”œâ”€â”€ rag_tab/                   # RAG Q&A system UI
â”‚       â”‚   â”œâ”€â”€ rag_tab.py
â”‚       â”‚   â””â”€â”€ rag_service.py
â”‚       â””â”€â”€ monitoring_tab/            # System monitoring UI
â”‚           â””â”€â”€ monitoring_tab.py
â”œâ”€â”€ models/                       # Model files and data storage
â”‚   â”œâ”€â”€ ctr_model.pkl                 # Trained CTR model
â”‚   â”œâ”€â”€ wide_deep_ctr_model.h5        # Wide & Deep model
â”‚   â”œâ”€â”€ index_data.json               # Text search index
â”‚   â”œâ”€â”€ knowledge_graph.pkl           # Knowledge graph data
â”‚   â””â”€â”€ images/                       # Image storage and embeddings
â”‚       â”œâ”€â”€ image_index.json
â”‚       â””â”€â”€ image_embeddings.npy
â”œâ”€â”€ data/                         # Training and experiment data
â”‚   â””â”€â”€ preloaded_documents.json     # Preloaded Chinese Wikipedia documents
â”œâ”€â”€ docs/                         # Comprehensive documentation
â”‚   â”œâ”€â”€ IMAGE_SEARCH_GUIDE.md         # Image search usage guide
â”‚   â”œâ”€â”€ KNOWLEDGE_GRAPH_GUIDE.md      # Knowledge graph guide
â”‚   â”œâ”€â”€ RAG_GUIDE.md                  # RAG system guide
â”‚   â””â”€â”€ WIDE_DEEP_CTR_GUIDE.md        # CTR model guide
â”œâ”€â”€ examples/                     # Example scripts
â”œâ”€â”€ tools/                        # Utility and monitoring tools
â”œâ”€â”€ test/ & tests/                # Test suites
â”œâ”€â”€ start_system.py               # System startup script
â”œâ”€â”€ quick_start.sh                # Quick start script
â””â”€â”€ requirements.txt              # Python dependencies
```

### Extension Development

#### Adding New Ranking Algorithms

1. Create new ranking module in `src/search_engine/ranking/`
2. Implement `RankingInterface` interface
3. Register new algorithm in `IndexService`

#### Adding New Features

1. Define new features in `CTRSampleConfig`
2. Calculate feature values in `DataService.record_impression`
3. Update model training logic

#### Adding New Image Search Features

1. Extend `ImageService` class with new methods
2. Update `image_tab.py` UI components
3. Test with various image types and queries

## ğŸ§ª Testing

```bash
# Run unit tests
python -m pytest test/

# Run integration tests
python test/test_integration.py

# Performance testing
python test/test_performance.py

# Test image search functionality
python -c "from src.search_engine.image_service import ImageService; print('Image service test passed')"
```

## ğŸ“ˆ Monitoring

The system provides multi-dimensional monitoring:

- **System Monitoring**: CPU, memory, disk usage
- **Business Monitoring**: Search QPS, click-through rate, response time
- **Data Monitoring**: Data quality, model performance metrics
- **Image Search Monitoring**: CLIP model performance, search accuracy
- **Alert Mechanism**: Anomaly detection and automatic alerting

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [jieba](https://github.com/fxsjy/jieba) - Chinese word segmentation
- [scikit-learn](https://scikit-learn.org/) - Machine learning library
- [Gradio](https://gradio.app/) - Web interface framework
- [pandas](https://pandas.pydata.org/) - Data processing
- [Hugging Face Transformers](https://huggingface.co/transformers/) - CLIP model implementation
- [OpenAI CLIP](https://github.com/openai/CLIP) - Original CLIP model

## ğŸ“ Contact

- Project Homepage: https://github.com/tylerelyt/test_bed
- Issue Tracker: https://github.com/tylerelyt/test_bed/issues
- Email: tylerelyt@gmail.com
