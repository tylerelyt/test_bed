#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„å›¾åƒç”ŸæˆæœåŠ¡ - Stable Diffusion XL
å¯ä»¥åœ¨å•ç‹¬çš„ conda ç¯å¢ƒä¸­è¿è¡Œï¼Œé€šè¿‡ HTTP API ä¸ Testbed é€šä¿¡

ä½¿ç”¨æ–¹æ³•ï¼š
1. åˆ›å»ºç‹¬ç«‹ç¯å¢ƒï¼šconda create -n testbed-image python=3.10 -y
2. æ¿€æ´»ç¯å¢ƒï¼šconda activate testbed-image
3. å®‰è£…ä¾èµ–ï¼špip install diffusers transformers accelerate safetensors torch pillow flask flask-cors
4. å¯åŠ¨æœåŠ¡ï¼špython image_generation_service.py
5. æœåŠ¡å°†åœ¨ http://localhost:5001 è¿è¡Œ
"""

import os
import time
import torch
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import hashlib

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡
pipe = None
model_loaded = False
model_name = "Stable Diffusion v1.5"
model_id = "runwayml/stable-diffusion-v1-5"
output_dir = Path("models/generated_images")
output_dir.mkdir(parents=True, exist_ok=True)
generation_history = []

def load_model():
    """åŠ è½½ SD 1.5 æ¨¡å‹"""
    global pipe, model_loaded
    
    if model_loaded:
        return True, "æ¨¡å‹å·²åŠ è½½"
    
    try:
        from diffusers import StableDiffusionPipeline
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} (è®¾å¤‡: {device})")
        print(f"   é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½çº¦4GBï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        start_time = time.time()
        
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            safety_checker=None
        )
        
        pipe = pipe.to(device)
        
        # å¯ç”¨å†…å­˜ä¼˜åŒ–
        if device == "cuda":
            try:
                pipe.enable_attention_slicing()
                print("  âœ“ å¯ç”¨ Attention Slicing")
            except:
                pass
            
            try:
                pipe.enable_vae_slicing()
                print("  âœ“ å¯ç”¨ VAE Slicing")
            except:
                pass
        
        load_time = time.time() - start_time
        model_loaded = True
        
        message = f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.1f}ç§’)"
        print(message)
        return True, message
        
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        print(error_msg)
        return False, error_msg

@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'ok',
        'model_loaded': model_loaded,
        'model_name': model_name if model_loaded else None
    })

@app.route('/load_model', methods=['POST'])
def api_load_model():
    """åŠ è½½æ¨¡å‹ API"""
    success, message = load_model()
    return jsonify({
        'success': success,
        'message': message
    })

@app.route('/generate', methods=['POST'])
def generate():
    """ç”Ÿæˆå›¾åƒ API"""
    global pipe, model_loaded
    
    if not model_loaded:
        return jsonify({
            'success': False,
            'message': 'æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ /load_model'
        }), 400
    
    try:
        data = request.json
        prompt = data.get('prompt', '')
        negative_prompt = data.get('negative_prompt', '')
        num_inference_steps = int(data.get('num_inference_steps', 50))
        guidance_scale = float(data.get('guidance_scale', 7.5))
        width = int(data.get('width', 512))
        height = int(data.get('height', 512))
        seed = int(data.get('seed', -1))
        num_images = int(data.get('num_images', 1))
        
        if seed == -1:
            seed = int(time.time() * 1000) % (2**32)
        
        generator = torch.Generator(device=pipe.device).manual_seed(seed)
        
        print(f"ğŸ¨ ç”Ÿæˆå›¾åƒ: {prompt[:50]}...")
        start_time = time.time()
        
        # ç”Ÿæˆå›¾åƒ
        output = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            width=width,
            height=height,
            generator=generator,
            num_images_per_prompt=num_images
        )
        
        images = output.images
        generation_time = time.time() - start_time
        
        # ä¿å­˜å›¾åƒ
        saved_paths = []
        for i, image in enumerate(images):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:8]
            filename = f"gen_{timestamp}_{prompt_hash}_{seed}_{i}.png"
            filepath = output_dir / filename
            
            # ä¿å­˜å›¾åƒï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
            from PIL import PngImagePlugin
            pnginfo = PngImagePlugin.PngInfo()
            pnginfo.add_text("prompt", prompt)
            pnginfo.add_text("negative_prompt", negative_prompt)
            pnginfo.add_text("steps", str(num_inference_steps))
            pnginfo.add_text("guidance_scale", str(guidance_scale))
            pnginfo.add_text("seed", str(seed))
            pnginfo.add_text("size", f"{width}x{height}")
            
            image.save(filepath, pnginfo=pnginfo)
            saved_paths.append(str(filepath))
        
        # è®°å½•å†å²
        history_entry = {
            "timestamp": datetime.now().isoformat(),
            "prompt": prompt,
            "seed": seed,
            "steps": num_inference_steps,
            "guidance_scale": guidance_scale,
            "size": f"{width}x{height}",
            "num_images": num_images,
            "generation_time": generation_time,
            "paths": saved_paths
        }
        generation_history.append(history_entry)
        
        print(f"âœ… ç”Ÿæˆå®Œæˆ (è€—æ—¶: {generation_time:.2f}ç§’)")
        
        return jsonify({
            'success': True,
            'message': f'ç”Ÿæˆäº† {num_images} å¼ å›¾åƒ',
            'paths': saved_paths,
            'generation_time': generation_time,
            'metadata': {
                'prompt': prompt,
                'negative_prompt': negative_prompt,
                'steps': num_inference_steps,
                'guidance_scale': guidance_scale,
                'seed': seed,
                'size': f"{width}x{height}"
            }
        })
        
    except Exception as e:
        import traceback
        error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'message': error_msg
        }), 500

@app.route('/image/<path:filename>', methods=['GET'])
def serve_image(filename):
    """æä¾›å›¾åƒæ–‡ä»¶"""
    filepath = output_dir / filename
    if filepath.exists():
        return send_file(filepath, mimetype='image/png')
    else:
        return jsonify({'error': 'Image not found'}), 404

@app.route('/history', methods=['GET'])
def get_history():
    """è·å–ç”Ÿæˆå†å²"""
    limit = request.args.get('limit', 20, type=int)
    return jsonify({
        'history': generation_history[-limit:]
    })

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ¨ Stable Diffusion XL å›¾åƒç”ŸæˆæœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://localhost:5001")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    print("\nğŸ’¡ æç¤º: é¦–æ¬¡ä½¿ç”¨å‰è¯·è°ƒç”¨ POST /load_model åŠ è½½æ¨¡å‹")
    print("   ç„¶åä½¿ç”¨ POST /generate ç”Ÿæˆå›¾åƒ\n")
    
    app.run(host='0.0.0.0', port=5001, debug=False)

