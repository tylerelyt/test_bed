dataset: domain_corpus
dataset_dir: data/llmops
do_train: true
finetuning_type: lora
fp16: true
gradient_accumulation_steps: 8
learning_rate: 1.0e-05
logging_steps: 50
logging_strategy: steps
lora_alpha: 32
lora_dropout: 0.05
lora_rank: 16
lora_target: all
max_seq_length: 2048
model_name_or_path: Qwen/Qwen2-1.5B
num_train_epochs: 1
output_dir: checkpoints/domain-cpt
overwrite_output_dir: true
per_device_train_batch_size: 2
save_steps: 500
save_strategy: steps
stage: pt
