#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æ™ºèƒ½ä½“å¾ªç¯æ¼”ç¤ºç•Œé¢

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡æ¿é€‰æ‹©
2. ä¸Šä¸‹æ–‡è£…é…
3. LLMæ¨ç†
4. ä¸Šä¸‹æ–‡æ›´æ–°
"""

import gradio as gr
import json
import sys
import os
import time
import asyncio
from typing import Dict, Any, Tuple

# ç¡®ä¿èƒ½å¯¼å…¥MCPæ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from search_engine.mcp.mcp_client_manager import get_mcp_client_manager


def create_smart_agent_demo():
    """åˆ›å»ºç®€åŒ–çš„æ™ºèƒ½ä½“å¾ªç¯æ¼”ç¤ºç•Œé¢"""
    
    # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯ç®¡ç†å™¨
    mcp_manager = get_mcp_client_manager()
    if not mcp_manager.is_connected("unified_server"):
        print("ğŸ”„ è¿æ¥MCPæœåŠ¡å™¨...")
        mcp_manager.connect("unified_server")
    
    def analyze_prompt_requirements(template_content: str) -> dict:
        """åˆ†æpromptæ¨¡æ¿çš„å‚æ•°è¦æ±‚ï¼Œè¿”å›éœ€è¦å¡«å……çš„å‚æ•°åˆ—è¡¨"""
        import re
        
        # åˆ†ææ¨¡æ¿ä¸­çš„å‚æ•°å ä½ç¬¦
        param_pattern = r'\{([^}]+)\}'
        params = re.findall(param_pattern, template_content)
        
        # åˆ†ç±»å‚æ•°ç±»å‹
        requirements = {
            'basic_vars': [],
            'mcp_resources': [],
            'mcp_tools': [],
            'dynamic_sections': []
        }
        
        for param in params:
            if param.startswith('local:'):
                requirements['basic_vars'].append(param)
            elif param.startswith('mcp:resource:'):
                requirements['mcp_resources'].append(param)
            elif param.startswith('mcp:tool:'):
                requirements['mcp_tools'].append(param)
            elif param.startswith('mcp:section:'):
                requirements['dynamic_sections'].append(param)
        
        return requirements
    
    def select_logic_content(requirements: dict, user_intent: str) -> dict:
        """æ ¹æ®å‚æ•°è¦æ±‚é€‰æ‹©åˆé€‚çš„é€»è¾‘å†…å®¹"""
        content_map = {
            # åŸºç¡€å˜é‡å†…å®¹ - ä¸MCPæœåŠ¡å™¨å‚æ•°å‘½åå®Œå…¨å¯¹åº”
            'local:current_time': time.strftime("%Y-%m-%d %H:%M:%S"),
            'local:user_intent': user_intent,
            'local:model_name': "qwen-max",
            'local:user_profile': "ç”¨æˆ·ä¿¡æ¯: æµ‹è¯•ç”¨æˆ·",
            'local:system_overview': "ç³»ç»ŸçŠ¶æ€: æ­£å¸¸è¿è¡Œ",
            'local:financial_data': "è´¢åŠ¡æ•°æ®ç¤ºä¾‹",
            'local:analysis_requirements': "åˆ†æè¦æ±‚ç¤ºä¾‹"
        }
        
        # æ„å»ºå‚æ•°æ˜ å°„
        params = {}
        for param in requirements['basic_vars']:
            if param in content_map:
                params[param] = content_map[param]
        
        return params

    # ------------------------- åˆ†åŒºç”Ÿæˆå‡½æ•°ï¼ˆå®¢æˆ·ç«¯ä¾§ï¼‰ -------------------------
    def gen_section_persona(template_name: str) -> str:
        mapping = {
            'simple_chat': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹",
            'rag_answer': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜",
            'react_reasoning': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªå–„äºæ¨ç†çš„AIåŠ©æ‰‹ï¼Œä½¿ç”¨ReActæ¨¡å¼è¿›è¡Œå¤šæ­¥æ¨ç†",
            'financial_analysis': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æå¸ˆ",
            'context_engineering': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šä¸‹æ–‡å·¥ç¨‹ä¸“å®¶ï¼Œæ“…é•¿åŠ¨æ€å†³ç­–å’Œæ™ºèƒ½æ¨ç†"
        }
        return mapping.get(template_name, "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹")

    def gen_section_current_status(user_intent: str) -> str:
        return f"""[å½“å‰çŠ¶æ€] å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
ç”¨æˆ·æ„å›¾: {user_intent}
æ¨¡å‹: qwen-max"""

    def gen_section_conversation_history() -> str:
        """ç”Ÿæˆå¯¹è¯å†å²åˆ†åŒº - å†…èšMCPèµ„æºè·å–"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            history_obj = loop.run_until_complete(mcp_manager.get_resource("conversation://current/history"))
            loop.close()
            history_text = json.dumps(history_obj, ensure_ascii=False, indent=2) if isinstance(history_obj, (dict, list)) else str(history_obj or "[]")
        except Exception:
            history_text = "[]"
        return f"[å†å²] {history_text}"

    def gen_section_user_profile() -> str:
        return "[ç”¨æˆ·ä¿¡æ¯] ç”¨æˆ·ä¿¡æ¯: æµ‹è¯•ç”¨æˆ·"

    def gen_section_system_overview() -> str:
        return "[ç³»ç»ŸçŠ¶æ€] ç³»ç»ŸçŠ¶æ€: æ­£å¸¸è¿è¡Œ"

    def gen_section_available_tools(user_intent: str = "") -> str:
        """ç”Ÿæˆå¯ç”¨å·¥å…·åˆ†åŒº - åŸºäºLLMçš„åŠ¨æ€å·¥å…·é€‰æ‹©"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            all_tools = loop.run_until_complete(mcp_manager.list_tools())
            loop.close()
            
            if not all_tools or not user_intent.strip():
                # å¦‚æœæ²¡æœ‰å·¥å…·æˆ–æ²¡æœ‰ç”¨æˆ·æ„å›¾ï¼Œè¿”å›æ‰€æœ‰å·¥å…·
                tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        desc = tool.get('description', 'å·¥å…·æè¿°')
                        input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                        output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                        input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                        output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                        tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                tools_text = "\n".join(tool_lines) if tool_lines else "[æ— å¯ç”¨å·¥å…·]"
                return f"[å¯ç”¨å·¥å…·] {tools_text}"
            
            # æ„å»ºå·¥å…·æè¿°ç”¨äºLLMé€‰æ‹©
            tool_descriptions = []
            for tool in all_tools:
                if isinstance(tool, dict):
                    name = tool.get('name', 'unknown')
                    desc = tool.get('description', 'å·¥å…·æè¿°')
                    input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                    input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                    tool_descriptions.append(f"- {name}: {desc} (å‚æ•°: {input_props})")
            
            # æ„å»ºLLMå·¥å…·é€‰æ‹©æç¤ºè¯
            selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å·¥å…·é€‰æ‹©ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œä»ä»¥ä¸‹å¯ç”¨å·¥å…·ä¸­é€‰æ‹©æœ€ç›¸å…³çš„å·¥å…·ï¼ˆæœ€å¤š3ä¸ªï¼‰ï¼š

**ç”¨æˆ·æ„å›¾**: {user_intent}

**å¯ç”¨å·¥å…·åˆ—è¡¨**:
{chr(10).join(tool_descriptions)}

**é€‰æ‹©è¦æ±‚**:
1. ä»”ç»†åˆ†æç”¨æˆ·æ„å›¾å’Œéœ€æ±‚
2. é€‰æ‹©æœ€èƒ½å¸®åŠ©å®Œæˆä»»åŠ¡çš„å·¥å…·
3. å¦‚æœç”¨æˆ·æ„å›¾ä¸éœ€è¦ç‰¹å®šå·¥å…·ï¼Œå¯ä»¥é€‰æ‹©é€šç”¨å·¥å…·
4. æœ€å¤šé€‰æ‹©3ä¸ªæœ€ç›¸å…³çš„å·¥å…·

**è¾“å‡ºæ ¼å¼**:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "selected_tools": ["å·¥å…·å1", "å·¥å…·å2", "å·¥å…·å3"],
    "reasoning": "é€‰æ‹©ç†ç”±"
}}

ç°åœ¨è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·æ„å›¾é€‰æ‹©å·¥å…·ï¼š
ç”¨æˆ·æ„å›¾: "{user_intent}"

è¯·è¾“å‡ºJSONæ ¼å¼çš„é€‰æ‹©ç»“æœï¼š"""
            
            # è°ƒç”¨LLMè¿›è¡Œå·¥å…·é€‰æ‹©
            try:
                import openai
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                
                response = client.chat.completions.create(
                    model="qwen-max",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·é€‰æ‹©ä¸“å®¶ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"},
                        {"role": "user", "content": selection_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                llm_response = response.choices[0].message.content.strip()
                selection_result = json.loads(llm_response)
                selected_tool_names = selection_result.get("selected_tools", [])
                
                # æ ¹æ®LLMé€‰æ‹©çš„å·¥å…·åç§°æ„å»ºå·¥å…·ä¿¡æ¯
                selected_tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        if name in selected_tool_names:
                            desc = tool.get('description', 'å·¥å…·æè¿°')
                            input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                            output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                            input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                            output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                            selected_tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                
                tools_text = "\n".join(selected_tool_lines) if selected_tool_lines else "[æ— ç›¸å…³å·¥å…·]"
                
            except Exception as llm_error:
                # LLMé€‰æ‹©å¤±è´¥ï¼Œå›é€€åˆ°æ˜¾ç¤ºæ‰€æœ‰å·¥å…·
                tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        desc = tool.get('description', 'å·¥å…·æè¿°')
                        input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                        output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                        input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                        output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                        tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                tools_text = "\n".join(tool_lines) if tool_lines else "[æ— å¯ç”¨å·¥å…·]"
                
        except Exception:
            tools_text = "[å·¥å…·è·å–å¤±è´¥]"
        
        # è¿”å›å¸¦æ ‡è®°çš„æ ¼å¼ï¼Œä½œä¸ºç‹¬ç«‹çš„é€»è¾‘åˆ†åŒº
        return f"[å¯ç”¨å·¥å…·] {tools_text}"

    def gen_section_user_question(user_input: str) -> str:
        return f"[ç”¨æˆ·é—®é¢˜] {user_input}"

    def gen_section_tao_output_example(template_name: str) -> str:
        return """[ç¤ºä¾‹è¾“å‡º] è¯·ä¸¥æ ¼è¿”å›ä»¥ä¸‹JSONï¼ˆä¸åŠ è§£é‡Šã€ä»…æ­¤ä¸€æ¡ï¼‰ï¼š
{
  "reasoning": "ç®€è¦ã€è‡ªç„¶è¯­è¨€ã€è¯´æ˜å…³é”®ä¿¡æ¯ä¸é€‰æ‹©ç†ç”±",
  "action": "final_answer",
  "observation": "ç›´æ¥é¢å‘ç”¨æˆ·çš„æœ€ç»ˆå›ç­”ï¼Œå®Œæ•´å¯ç”¨"
}"""

    def gen_section_financial_data(financial_data: str = "", analysis_requirements: str = "") -> str:
        data = financial_data or "è´¢åŠ¡æ•°æ®ç¤ºä¾‹"
        req = analysis_requirements or "åˆ†æè¦æ±‚ç¤ºä¾‹"
        return f"[è´¢åŠ¡æ•°æ®] {data}\n[åˆ†æè¦æ±‚] {req}"
    
    def generate_section_params(selected_template: str, user_intent: str) -> dict:
        """ç»Ÿä¸€ç”Ÿæˆåˆ†åŒºå‚æ•° - é¿å…é‡å¤ä»£ç """
        return {
            "section_persona": gen_section_persona(selected_template),
            "section_current_status": gen_section_current_status(user_intent),
            "section_conversation_history": gen_section_conversation_history(),
            "section_user_profile": gen_section_user_profile(),
            "section_system_overview": gen_section_system_overview(),
            "section_available_tools": gen_section_available_tools(user_intent),
            "section_user_question": gen_section_user_question(user_intent),
            "section_tao_output_example": gen_section_tao_output_example(selected_template)
        }
    
    def execute_stage_1_template_selection(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ1: æ¨¡æ¿é€‰æ‹©"""
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage1_start = time.time()
            
            # è·å–æ‰€æœ‰å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                prompts = loop.run_until_complete(mcp_manager.list_prompts())
            finally:
                loop.close()
            
            if not prompts:
                return "âŒ æ— æ³•è·å–æç¤ºè¯æ¨¡æ¿"
            
            # æ„å»ºæ¨¡æ¿é€‰æ‹©æç¤ºè¯
            prompt_descriptions = []
            available_templates = [] # Store available template names
            for prompt in prompts:
                if isinstance(prompt, dict):
                    name = prompt.get("name", "")
                    description = prompt.get("description", "")
                    available_templates.append(name) # Add to available templates
                    prompt_descriptions.append(f"- {name}: {description}")
            
            # Ensure there are available templates
            if not available_templates:
                return "âŒ æ— æ³•è·å–æç¤ºè¯æ¨¡æ¿"
            
            selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ¨¡æ¿é€‰æ‹©ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œä»ä»¥ä¸‹å¯ç”¨æ¨¡æ¿ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ¿ï¼š

**ç”¨æˆ·æ„å›¾**: {user_intent}

**å¯ç”¨æ¨¡æ¿åˆ—è¡¨**:
{chr(10).join(prompt_descriptions)}

**é‡è¦**: ä½ åªèƒ½ä»ä»¥ä¸‹æ¨¡æ¿åç§°ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
{', '.join(available_templates)}

**é€‰æ‹©è¦æ±‚**:
1. ä»”ç»†åˆ†æç”¨æˆ·æ„å›¾
2. è€ƒè™‘æ¯ä¸ªæ¨¡æ¿çš„åŠŸèƒ½å’Œé€‚ç”¨åœºæ™¯
3. é€‰æ‹©æœ€èƒ½æ»¡è¶³éœ€æ±‚çš„æ¨¡æ¿
4. æä¾›é€‰æ‹©ç†ç”±
5. **å¿…é¡»ä»ä¸Šè¿°æ¨¡æ¿åç§°ä¸­é€‰æ‹©ï¼Œä¸èƒ½é€‰æ‹©ä¸å­˜åœ¨çš„æ¨¡æ¿**

**è¾“å‡ºæ ¼å¼**:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š
{{
    "selected_template": "æ¨¡æ¿åç§°",
    "reasoning": "é€‰æ‹©ç†ç”±",
    "confidence": 0.95
}}

**ç¤ºä¾‹**:
ç”¨æˆ·æ„å›¾: "å¸®æˆ‘æœç´¢å…³äºæœºå™¨å­¦ä¹ çš„ä¿¡æ¯"
{{
    "selected_template": "rag_answer",
    "reasoning": "ç”¨æˆ·éœ€è¦æœç´¢å’Œæ£€ç´¢ä¿¡æ¯ï¼Œrag_answeræ¨¡æ¿ä¸“é—¨ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆ",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "è¯·åˆ†æè¿™ä¸ªä»£ç çš„æ€§èƒ½é—®é¢˜"
{{
    "selected_template": "react_reasoning",
    "reasoning": "ç”¨æˆ·éœ€è¦åˆ†æå’Œæ¨ç†ï¼Œreact_reasoningæ¨¡æ¿æ”¯æŒå¤šæ­¥æ¨ç†å’Œæ€è€ƒè¿‡ç¨‹",
    "confidence": 0.85
}}

ç”¨æˆ·æ„å›¾: "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”ä¸é”™"
{{
    "selected_template": "simple_chat",
    "reasoning": "ç”¨æˆ·è¿›è¡Œç®€å•å¯¹è¯ï¼Œsimple_chatæ¨¡æ¿é€‚åˆæ—¥å¸¸äº¤æµ",
    "confidence": 0.95
}}

ç”¨æˆ·æ„å›¾: "è¯·å®¡æŸ¥è¿™æ®µPythonä»£ç "
{{
    "selected_template": "code_review",
    "reasoning": "ç”¨æˆ·éœ€è¦ä»£ç å®¡æŸ¥ï¼Œcode_reviewæ¨¡æ¿ä¸“é—¨ç”¨äºä»£ç è´¨é‡åˆ†æ",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "åˆ†æé˜¿é‡Œå·´å·´çš„è´¢åŠ¡æ•°æ®"
{{
    "selected_template": "financial_analysis",
    "reasoning": "ç”¨æˆ·éœ€è¦è´¢åŠ¡åˆ†æï¼Œfinancial_analysisæ¨¡æ¿ä¸“é—¨ç”¨äºè´¢åŠ¡æ•°æ®åˆ†æå’ŒæŠ¥å‘Š",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "ä½¿ç”¨ä¸Šä¸‹æ–‡å·¥ç¨‹æ–¹æ³•è§£å†³è¿™ä¸ªé—®é¢˜"
{{
    "selected_template": "context_engineering",
    "reasoning": "ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Œcontext_engineeringæ¨¡æ¿ä¸“é—¨ç”¨äºå®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿæ¨¡å¼",
    "confidence": 0.95
}}

ç°åœ¨è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·æ„å›¾é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ¿ï¼š

ç”¨æˆ·æ„å›¾: "{user_intent}"

**è®°ä½**: åªèƒ½ä» {', '.join(available_templates)} ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡æ¿åç§°ã€‚

è¯·è¾“å‡ºJSONæ ¼å¼çš„é€‰æ‹©ç»“æœï¼š"""
            
            # è°ƒç”¨LLMè¿›è¡Œæ¨¡æ¿é€‰æ‹©
            try:
                import openai
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                
                response = client.chat.completions.create(
                    model="qwen-max",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¨¡æ¿é€‰æ‹©ä¸“å®¶ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚ç¡®ä¿è¾“å‡ºçš„JSONæ ¼å¼å®Œå…¨æ­£ç¡®ã€‚ä½ åªèƒ½é€‰æ‹©æä¾›çš„æ¨¡æ¿åç§°åˆ—è¡¨ä¸­çš„æ¨¡æ¿ã€‚"},
                        {"role": "user", "content": selection_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                llm_response = response.choices[0].message.content.strip()
                
                # è§£æLLMå“åº”
                selection_result = json.loads(llm_response)
                selected_template = selection_result.get("selected_template")
                reasoning = selection_result.get("reasoning", "LLMé€‰æ‹©")
                confidence = selection_result.get("confidence", 0.8)
                
                # éªŒè¯é€‰æ‹©çš„æ¨¡æ¿æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
                if selected_template not in available_templates:
                    raise Exception(f"LLMé€‰æ‹©äº†ä¸å­˜åœ¨çš„æ¨¡æ¿ '{selected_template}'ã€‚å¯ç”¨æ¨¡æ¿: {', '.join(available_templates)}")
                
                llm_success = True
                
            except Exception as llm_error:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_error}")
            
            stage1_time = time.time() - stage1_start
            
            # æ„å»ºç»“æœ
            stage1_result = f"""## ğŸ¯ é˜¶æ®µ1: æ¨¡æ¿é€‰æ‹©

**é€‰æ‹©çŠ¶æ€**: âœ… æˆåŠŸ  
**é€‰æ‹©æ–¹æ³•**: LLMæ™ºèƒ½é€‰æ‹©  
**é€‰æ‹©æ¨¡æ¿**: `{selected_template}`  
**ç½®ä¿¡åº¦**: {confidence:.2f}  
**è€—æ—¶**: {stage1_time:.2f}ç§’

**ğŸ¤– é€‰æ‹©ç†ç”±**:
{reasoning}

**ğŸ“ å¯ç”¨æ¨¡æ¿**:
{chr(10).join(prompt_descriptions)}

**ğŸ” é€‰æ‹©è¿‡ç¨‹**:
- **ç”¨æˆ·æ„å›¾**: {user_intent}
- **æ¨¡æ¿æ•°é‡**: {len(prompts)} ä¸ª
- **é€‰æ‹©ç®—æ³•**: LLMé©±åŠ¨çš„æ™ºèƒ½é€‰æ‹©
- **æ•°æ®æº**: ç›´æ¥MCPæœåŠ¡å™¨è·å–
- **é€‰æ‹©ç»“æœ**: {selected_template}"""
            
            return stage1_result
            
        except Exception as e:
            return f"âŒ é˜¶æ®µ1æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_2_context_assembly(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ2: ä¸Šä¸‹æ–‡è£…é…"""
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage2_start = time.time()
            
            # 1. å…ˆæ‰§è¡Œé˜¶æ®µ1è·å–æ¨¡æ¿é€‰æ‹©ç»“æœ
            stage1_result = execute_stage_1_template_selection(user_intent)
            
            # 2. ä»é˜¶æ®µ1ç»“æœä¸­æå–é€‰æ‹©çš„æ¨¡æ¿
            selected_template = "simple_chat"  # é»˜è®¤å€¼
            if "é€‰æ‹©æ¨¡æ¿" in stage1_result:
                import re
                template_match = re.search(r'é€‰æ‹©æ¨¡æ¿.*?`([^`]+)`', stage1_result)
                if template_match:
                    selected_template = template_match.group(1)
            
            # 3. è·å–promptæ¨¡æ¿å¹¶åˆ†æå‚æ•°è¦æ±‚
            template_content = mcp_manager.get_prompt("unified_server", selected_template, {"user_input": user_intent})
            requirements = analyze_prompt_requirements(template_content)
            
            # 4. ä½¿ç”¨ç»Ÿä¸€çš„åˆ†åŒºå‚æ•°ç”Ÿæˆå‡½æ•°
            section_params = generate_section_params(selected_template, user_intent)

            # 5. ä»¥åˆ†åŒºå‚æ•°è¿›è¡Œæœ€ç»ˆè£…é…
            resolved_content = mcp_manager.get_prompt("unified_server", selected_template, section_params)
            
            stage2_time = time.time() - stage2_start
            
            # ç›´æ¥æ˜¾ç¤ºè£…é…åçš„çº¯æ–‡æœ¬ä¸Šä¸‹æ–‡å†…å®¹
            stage2_result = f"""ğŸ”§ **é˜¶æ®µ2å®Œæˆ** ({stage2_time:.2f}ç§’) | æ¨¡æ¿: {selected_template} | é•¿åº¦: {len(resolved_content)} å­—ç¬¦

---

{resolved_content}

---

âœ… **è£…é…å®Œæˆ** - ä»¥ä¸Šä¸ºè£…é…åçš„å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹"""
            
            return stage2_result
            
        except Exception as e:
            return f"âŒ é˜¶æ®µ2æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_3_llm_inference(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ3: LLMæ¨ç†"""
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage3_start = time.time()
            
            # 1. ä½¿ç”¨ä¸é˜¶æ®µ2ä¸€è‡´çš„å‚æ•°åŒ–è£…é…é€»è¾‘
            # å…ˆæ‰§è¡Œé˜¶æ®µ1è·å–æ¨¡æ¿é€‰æ‹©ç»“æœ
            stage1_result = execute_stage_1_template_selection(user_intent)
            
            # ä»é˜¶æ®µ1ç»“æœä¸­æå–é€‰æ‹©çš„æ¨¡æ¿
            selected_template = "simple_chat"  # é»˜è®¤å€¼
            if "é€‰æ‹©æ¨¡æ¿" in stage1_result:
                import re
                template_match = re.search(r'é€‰æ‹©æ¨¡æ¿.*?`([^`]+)`', stage1_result)
                if template_match:
                    selected_template = template_match.group(1)
            
            # 2. ä½¿ç”¨ç»Ÿä¸€çš„åˆ†åŒºå‚æ•°ç”Ÿæˆå‡½æ•°
            section_params = generate_section_params(selected_template, user_intent)
            
            # 3. ä»¥åˆ†åŒºå‚æ•°è¿›è¡Œæœ€ç»ˆè£…é…
            resolved_content = mcp_manager.get_prompt("unified_server", selected_template, section_params)
            
            # 4. è°ƒç”¨LLMï¼ˆæœŸæœ›è¿”å›TAO JSONï¼‰
            try:
                import openai
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                
                response = client.chat.completions.create(
                    model="qwen-max",
                    messages=[
                        {"role": "system", "content": resolved_content},
                        {"role": "user", "content": user_intent}
                    ],
                    max_tokens=1000,
                    temperature=0.7,
                    response_format={"type": "json_object"}
                )
                
                llm_response = response.choices[0].message.content
                llm_success = True
                
            except Exception as llm_error:
                llm_response = f"LLMè°ƒç”¨å¤±è´¥: {str(llm_error)}"
                llm_success = False
            
            stage3_time = time.time() - stage3_start
            
            if llm_success:
                # å°è¯•è§£æä¸ºJSONå¹¶ç¾åŒ–æ˜¾ç¤º
                pretty = llm_response
                try:
                    pretty = json.dumps(json.loads(llm_response), ensure_ascii=False, indent=2)
                except Exception:
                    pass
                stage3_result = f"""ğŸ¤– **é˜¶æ®µ3å®Œæˆ** ({stage3_time:.2f}ç§’) | æ¨¡å‹: qwen-max | é•¿åº¦: {len(llm_response)} å­—ç¬¦

---

{pretty}

---

âœ… **æ¨ç†å®Œæˆ** - ä»¥ä¸Šä¸ºTAO JSONè¾“å‡º"""
            else:
                stage3_result = f"""ğŸ¤– **é˜¶æ®µ3å¤±è´¥** ({stage3_time:.2f}ç§’)

---

âŒ **é”™è¯¯ä¿¡æ¯**: {llm_response}

---

âš ï¸ LLMæœåŠ¡è°ƒç”¨å¤±è´¥ï¼Œä½†MCPæ¶æ„è¿è¡Œæ­£å¸¸"""
            
            return stage3_result
            
        except Exception as e:
            return f"âŒ é˜¶æ®µ3æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_4_context_update(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ4: ä¸Šä¸‹æ–‡æ›´æ–°"""
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage4_start = time.time()

            # 1) é˜¶æ®µ1ä¸é˜¶æ®µ3ï¼ˆä»…è·å–LLMè¾“å‡ºæ–‡æœ¬ï¼Œä¸é‡è·‘LLMï¼‰
            stage1_result = execute_stage_1_template_selection(user_intent)
            stage3_result = execute_stage_3_llm_inference(user_intent)

            # ä»é˜¶æ®µ3ç»“æœä¸­æå– JSON
            sep = "\n---\n"
            llm_response = ""
            start_idx = stage3_result.find(sep)
            if start_idx != -1:
                start_idx += len(sep)
                end_idx = stage3_result.find(sep, start_idx)
                if end_idx != -1:
                    llm_response = stage3_result[start_idx:end_idx].strip()

            # 2) è§£æ Reasoning / Action / Observationï¼ˆä¼˜å…ˆJSONè§£æï¼‰
            import re
            reasoning = ""
            action = ""
            observation = ""
            if llm_response:
                try:
                    obj = json.loads(llm_response)
                    reasoning = str(obj.get("reasoning", "")).strip()
                    action = str(obj.get("action", "")).strip()
                    observation = str(obj.get("observation", "")).strip()
                except Exception:
                    pass
            if not reasoning and not action:
                m_reason = re.search(r"\*\*Reasoning[^*]*\*\*[:\s]*(.*?)(?=\*\*Action|$)", llm_response, re.DOTALL | re.IGNORECASE)
                m_action = re.search(r"\*\*Action[^*]*\*\*[:\s]*(.*)$", llm_response, re.DOTALL | re.IGNORECASE)
                if m_reason:
                    reasoning = m_reason.group(1).strip()
                if m_action:
                    action = m_action.group(1).strip()
                if "final_answer:" in action.lower():
                    ans = re.search(r"final_answer:\s*(.*)", action, re.IGNORECASE)
                    if ans:
                        observation = ans.group(1).strip()
                        action = "final_answer"

            # 3) å†™å…¥TAOåˆ°MCPå†å²
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                tao_record = {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "reasoning": reasoning,
                    "action": action,
                    "observation": observation
                }
                loop.run_until_complete(mcp_manager.add_conversation_turn(json.dumps(tao_record)))
                loop.close()
            except Exception as e:
                print(f"æ›´æ–°MCPèµ„æºå¤±è´¥: {e}")

            # 4) é‡æ–°è£…é…ä¸Šä¸‹æ–‡ï¼ˆæ­¤æ—¶å†å²å·²æ›´æ–°ï¼‰- ä½¿ç”¨ä¸é˜¶æ®µ2/3ä¸€è‡´çš„å‚æ•°åŒ–è£…é…
            selected_template = "simple_chat"
            if "é€‰æ‹©æ¨¡æ¿" in stage1_result:
                m = re.search(r'é€‰æ‹©æ¨¡æ¿.*?`([^`]+)`', stage1_result)
                if m:
                    selected_template = m.group(1)

            # ä½¿ç”¨ç»Ÿä¸€çš„åˆ†åŒºå‚æ•°ç”Ÿæˆå‡½æ•°
            section_params = generate_section_params(selected_template, user_intent)
            
            # ä»¥åˆ†åŒºå‚æ•°è¿›è¡Œæœ€ç»ˆè£…é…
            resolved_content = mcp_manager.get_prompt("unified_server", selected_template, section_params)

            # 5) è¾“å‡ºä¸é˜¶æ®µ2ä¸€è‡´çš„æ¸…çˆ½ä¸Šä¸‹æ–‡
            stage4_time = time.time() - stage4_start
            stage4_result = f"""ğŸ”„ **é˜¶æ®µ4å®Œæˆ** ({stage4_time:.2f}ç§’) | æ¨¡æ¿: {selected_template} | é•¿åº¦: {len(resolved_content)} å­—ç¬¦

---

{resolved_content}

---

âœ… **ä¸Šä¸‹æ–‡å·²æ›´æ–°** - [å†å²] å·²åŒ…å«æœ¬è½®å¯¹è¯çš„TAOè®°å½•"""

            return stage4_result
            
        except Exception as e:
            return f"âŒ é˜¶æ®µ4æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def run_complete_flow(user_intent: str, max_turns: int = 1) -> Tuple[str, str, str, str, str]:
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾", "", "", "", ""
        
        try:
            # æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
            stage1_result = execute_stage_1_template_selection(user_intent)
            stage2_result = execute_stage_2_context_assembly(user_intent)
            stage3_result = execute_stage_3_llm_inference(user_intent)
            stage4_result = execute_stage_4_context_update(user_intent)
            
            # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
            final_summary = f"""## ğŸ‰ æ™ºèƒ½ä½“å¾ªç¯å®Œæˆæ€»ç»“

**æ‰§è¡ŒçŠ¶æ€**: âœ… å…¨éƒ¨æˆåŠŸ  
**ç”¨æˆ·æ„å›¾**: {user_intent}  
**æ‰§è¡Œæ—¶é—´**: {time.strftime("%Y-%m-%d %H:%M:%S")}

**ğŸ“Š å„é˜¶æ®µæ‰§è¡Œç»“æœ**:
- **é˜¶æ®µ1 (æ¨¡æ¿é€‰æ‹©)**: âœ… æˆåŠŸ
- **é˜¶æ®µ2 (ä¸Šä¸‹æ–‡è£…é…)**: âœ… æˆåŠŸ  
- **é˜¶æ®µ3 (LLMæ¨ç†)**: âœ… æˆåŠŸ
- **é˜¶æ®µ4 (ä¸Šä¸‹æ–‡æ›´æ–°)**: âœ… æˆåŠŸ

**ğŸ”— MCPæ¶æ„éªŒè¯**:
- **æœåŠ¡å™¨è¿æ¥**: âœ… æ­£å¸¸
- **æ¨¡æ¿è·å–**: âœ… æ­£å¸¸
- **å·¥å…·è°ƒç”¨**: âœ… æ­£å¸¸
- **èµ„æºè®¿é—®**: âœ… æ­£å¸¸

**ğŸ¯ æ ¸å¿ƒåŠŸèƒ½éªŒè¯**:
- **åŠ¨æ€å·¥å…·é€‰æ‹©**: âœ… ä¸æ¨¡æ¿å­—æ®µåŒ¹é…
- **ä¸Šä¸‹æ–‡å·¥ç¨‹**: âœ… å®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
- **MCPåè®®**: âœ… æ ‡å‡†åŒ–äº¤äº’
- **æ™ºèƒ½ä½“å¾ªç¯**: âœ… å››é˜¶æ®µå®Œæ•´æ‰§è¡Œ

**ğŸ’¡ æŠ€æœ¯äº®ç‚¹**:
- å®Œå…¨åŸºäºMCPåè®®çš„åŠ¨æ€å‘ç°
- LLMé©±åŠ¨çš„æ™ºèƒ½æ¨¡æ¿å’Œå·¥å…·é€‰æ‹©
- æ ‡å‡†åŒ–çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æµç¨‹
- å®Œæ•´çš„æ™ºèƒ½ä½“å·¥ä½œå¾ªç¯"""
            
            return final_summary, stage1_result, stage2_result, stage3_result, stage4_result
            
        except Exception as e:
            error_msg = f"âŒ å®Œæ•´æµç¨‹æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            return error_msg, "", "", "", ""
    
    def clear_conversation_history() -> str:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        return "ğŸ”„ å¯¹è¯å†å²å·²æ¸…ç©ºï¼ˆå®é™…æ¸…ç©ºéœ€è¦MCPå·¥å…·è°ƒç”¨ï¼‰"
    
    def get_system_status() -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            # è·å–MCPæœåŠ¡å™¨çŠ¶æ€
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                tools = loop.run_until_complete(mcp_manager.list_tools())
                resources = loop.run_until_complete(mcp_manager.list_resources())
                prompts = loop.run_until_complete(mcp_manager.list_prompts())
            finally:
                loop.close()
            
            status = f"""## ğŸ“Š ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š

**ğŸ• æŠ¥å‘Šæ—¶é—´**: {time.strftime("%Y-%m-%d %H:%M:%S")}

**ğŸ”— MCPæœåŠ¡å™¨çŠ¶æ€**:
- **è¿æ¥çŠ¶æ€**: âœ… å·²è¿æ¥
- **å¯ç”¨å·¥å…·**: {len(tools)} ä¸ª
- **å¯ç”¨èµ„æº**: {len(resources)} ä¸ª  
- **å¯ç”¨æç¤ºè¯**: {len(prompts)} ä¸ª

**ğŸ› ï¸ å¯ç”¨å·¥å…·**:
"""
            for tool in tools:
                if isinstance(tool, dict):
                    name = tool.get("name", "")
                    description = tool.get("description", "")
                    status += f"- **{name}**: {description}\n"
            
            status += f"""
**ğŸ“š å¯ç”¨èµ„æº**:
"""
            for resource in resources:
                if isinstance(resource, dict):
                    uri = resource.get("uri", "")
                    status += f"- **{uri}**: MCPèµ„æº\n"
            
            status += f"""
**ğŸ“ å¯ç”¨æç¤ºè¯**:
"""
            for prompt in prompts:
                if isinstance(prompt, dict):
                    name = prompt.get("name", "")
                    description = prompt.get("description", "")
                    status += f"- **{name}**: {description}\n"
            
            status += f"""
**ğŸ¯ æ ¸å¿ƒåŠŸèƒ½**:
- **åŠ¨æ€å·¥å…·é€‰æ‹©**: âœ… å·²å®ç°
- **æ¨¡æ¿åŒ¹é…**: âœ… å·²å®ç°
- **ä¸Šä¸‹æ–‡å·¥ç¨‹**: âœ… å·²å®ç°
- **æ™ºèƒ½ä½“å¾ªç¯**: âœ… å·²å®ç°"""
            
            return status
            
        except Exception as e:
            return f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}"
    
    def view_conversation_history() -> str:
        """æŸ¥çœ‹å¯¹è¯å†å²ï¼ˆä»MCPèµ„æºè¯»å–ï¼‰"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                history_resource = loop.run_until_complete(
                    mcp_manager.get_resource("conversation://current/history")
                )
            finally:
                loop.close()
            
            if isinstance(history_resource, str):
                # å°è¯•è§£æä¸ºJSONåå†æ ¼å¼åŒ–
                try:
                    parsed = json.loads(history_resource)
                    return json.dumps(parsed, ensure_ascii=False, indent=2)
                except Exception:
                    return history_resource
            return json.dumps(history_resource, ensure_ascii=False, indent=2)
        except Exception as e:
            return f"âŒ è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}"
    
    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
        # é¡µé¢æ ‡é¢˜å’Œè¯´æ˜
        gr.Markdown("""
        # ğŸ§  ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“æ¼”ç¤º
        
        **åŸºäºModel Context Protocolçš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“ç³»ç»Ÿ**
        
        ---
        """)
        
        # ä¸»è¦å†…å®¹åŒºåŸŸ
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥å’Œæ§åˆ¶åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ ç”¨æˆ·è¾“å…¥")
                user_input = gr.Textbox(
                    label="ç”¨æˆ·æ„å›¾",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–éœ€æ±‚...",
                    lines=4,
                    max_lines=6
                )
                
                gr.Markdown("### ğŸ¯ æ‰§è¡Œæ§åˆ¶")
                with gr.Row():
                    stage1_btn = gr.Button("1ï¸âƒ£ æ¨¡æ¿é€‰æ‹©", variant="primary", size="sm")
                    stage2_btn = gr.Button("2ï¸âƒ£ ä¸Šä¸‹æ–‡è£…é…", variant="primary", size="sm")
                
                with gr.Row():
                    stage3_btn = gr.Button("3ï¸âƒ£ LLMæ¨ç†", variant="primary", size="sm")
                    stage4_btn = gr.Button("4ï¸âƒ£ ä¸Šä¸‹æ–‡æ›´æ–°", variant="primary", size="sm")
                
                gr.Markdown("### ğŸš€ å¿«æ·æ“ä½œ")
                complete_btn = gr.Button("ğŸ¯ æ‰§è¡Œå®Œæ•´æµç¨‹", variant="secondary", size="lg")
                
                gr.Markdown("### âš™ï¸ ç³»ç»Ÿç®¡ç†")
                with gr.Row():
                    status_btn = gr.Button("ğŸ“Š ç³»ç»ŸçŠ¶æ€", variant="secondary", size="sm")
                    history_btn = gr.Button("ğŸ“œ åˆ·æ–°å†å²", variant="secondary", size="sm")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", variant="stop", size="sm")
            
            # å³ä¾§ï¼šç»“æœæ˜¾ç¤ºåŒºåŸŸï¼ˆåˆ†é˜¶æ®µä¸çŠ¶æ€/å†å²ï¼‰
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“Š æ‰§è¡Œç»“æœä¸ç³»ç»Ÿè§†å›¾")
                with gr.Tabs():
                    with gr.Tab("ğŸ§© æ€»ç»“"):
                        output_summary = gr.Textbox(
                            label="æµç¨‹æ€»ç»“",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("1ï¸âƒ£ æ¨¡æ¿é€‰æ‹©"):
                        output_stage1 = gr.Textbox(
                            label="é˜¶æ®µ1è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("2ï¸âƒ£ ä¸Šä¸‹æ–‡è£…é…"):
                        output_stage2 = gr.Textbox(
                            label="é˜¶æ®µ2è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("3ï¸âƒ£ LLMæ¨ç†"):
                        output_stage3 = gr.Textbox(
                            label="é˜¶æ®µ3è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("4ï¸âƒ£ ä¸Šä¸‹æ–‡æ›´æ–°"):
                        output_stage4 = gr.Textbox(
                            label="é˜¶æ®µ4è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("ğŸ“Š ç³»ç»ŸçŠ¶æ€"):
                        status_output = gr.Textbox(
                            label="ç³»ç»ŸçŠ¶æ€",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("ğŸ“œ å¯¹è¯å†å²"):
                        history_output = gr.Textbox(
                            label="å¯¹è¯å†å² (TAO)",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
        
        # åº•éƒ¨è¯´æ˜åŒºåŸŸ
        gr.Markdown("""
        ---
        
        ### ğŸ“‹ ä½¿ç”¨æŒ‡å—
        
        **å››é˜¶æ®µæ™ºèƒ½ä½“å¾ªç¯**ï¼š
        1. **ğŸ¯ æ¨¡æ¿é€‰æ‹©** - æ ¹æ®ç”¨æˆ·æ„å›¾æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
        2. **ğŸ”§ ä¸Šä¸‹æ–‡è£…é…** - å°†æ¨¡æ¿ä¸ç”¨æˆ·æ„å›¾ç»“åˆï¼Œç”Ÿæˆå®Œæ•´çš„ä¸Šä¸‹æ–‡
        3. **ğŸ¤– LLMæ¨ç†** - ä½¿ç”¨è£…é…çš„ä¸Šä¸‹æ–‡è°ƒç”¨LLMè¿›è¡Œæ¨ç†
        4. **ğŸ”„ ä¸Šä¸‹æ–‡æ›´æ–°** - æ›´æ–°å¯¹è¯å†å²ï¼Œä¸ºä¸‹ä¸€è½®å¯¹è¯åšå‡†å¤‡
        
        **ğŸ’¡ æŠ€æœ¯ç‰¹æ€§**ï¼š
        - åŸºäºMCPåè®®çš„æ ‡å‡†åŒ–äº¤äº’
        - LLMé©±åŠ¨çš„æ™ºèƒ½æ¨¡æ¿å’Œå·¥å…·é€‰æ‹©
        - å®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
        - åŠ¨æ€ä¸Šä¸‹æ–‡å·¥ç¨‹ç®¡ç†
        
        **ğŸ”— ç³»ç»Ÿæ¶æ„**ï¼š
        - MCPæœåŠ¡å™¨ï¼šæä¾›æ ‡å‡†åŒ–çš„promptsã€toolsã€resources
        - åŠ¨æ€å·¥å…·é€‰æ‹©ï¼šä¸æ¨¡æ¿å­—æ®µæ™ºèƒ½åŒ¹é…
        - ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šå®Œæ•´çš„æ™ºèƒ½ä½“å·¥ä½œå¾ªç¯
        """)
        
        # ç»‘å®šäº‹ä»¶
        stage1_btn.click(
            fn=execute_stage_1_template_selection,
            inputs=[user_input],
            outputs=[output_stage1]
        )
        
        stage2_btn.click(
            fn=execute_stage_2_context_assembly,
            inputs=[user_input],
            outputs=[output_stage2]
        )
        
        stage3_btn.click(
            fn=execute_stage_3_llm_inference,
            inputs=[user_input],
            outputs=[output_stage3]
        )
        
        stage4_btn.click(
            fn=execute_stage_4_context_update,
            inputs=[user_input],
            outputs=[output_stage4]
        )
        
        complete_btn.click(
            fn=run_complete_flow,
            inputs=[user_input],
            outputs=[output_summary, output_stage1, output_stage2, output_stage3, output_stage4]
        )
        
        clear_btn.click(
            fn=clear_conversation_history,
            inputs=[],
            outputs=[history_output]
        )
        
        status_btn.click(
            fn=get_system_status,
            inputs=[],
            outputs=[status_output]
        )
        
        history_btn.click(
            fn=view_conversation_history,
            inputs=[],
            outputs=[history_output]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_smart_agent_demo()
    demo.launch(server_name="0.0.0.0", server_port=7862, share=False)
