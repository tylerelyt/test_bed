#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æ™ºèƒ½ä½“å¾ªç¯æ¼”ç¤ºç•Œé¢

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡æ¿é€‰æ‹©
2. ä¸Šä¸‹æ–‡è£…é…
3. LLMæ¨ç†
4. ä¸Šä¸‹æ–‡æ›´æ–°
"""

import gradio as gr
import json
import sys
import os
import time
import asyncio
from typing import Dict, Any, Tuple

# ç¡®ä¿èƒ½å¯¼å…¥MCPæ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from search_engine.mcp.mcp_client_manager import get_mcp_client_manager
from search_engine.mcp_tab.context_pipeline import ContextEngineeringPipeline

# å…¨å±€çŠ¶æ€ï¼šä¿å­˜é˜¶æ®µ1é€‰æ‹©çš„æ¨¡æ¿åç§°
_selected_template_name = None


def create_smart_agent_demo():
    """åˆ›å»ºç®€åŒ–çš„æ™ºèƒ½ä½“å¾ªç¯æ¼”ç¤ºç•Œé¢
    
    ä¼˜åŒ–ç‚¹ï¼š
    1. å¼•å…¥ContextEngineeringPipelineç®¡é“ç±»ï¼Œé¿å…é‡å¤æ‰§è¡Œ
    2. ä¿æŒåŸæœ‰UIæ¥å£ä¸å˜
    3. å†…éƒ¨ä½¿ç”¨ä¼˜åŒ–çš„çŠ¶æ€ç®¡ç†
    """
    
    # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯ç®¡ç†å™¨
    mcp_manager = get_mcp_client_manager()
    if not mcp_manager.is_connected("unified_server"):
        print("ğŸ”„ è¿æ¥MCPæœåŠ¡å™¨...")
        mcp_manager.connect("unified_server")
    
    # åˆå§‹åŒ–ä¸Šä¸‹æ–‡å·¥ç¨‹ç®¡é“ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
    pipeline = ContextEngineeringPipeline(mcp_manager)
    
    def analyze_prompt_requirements(template_content: str) -> dict:
        """åˆ†æpromptæ¨¡æ¿çš„å‚æ•°è¦æ±‚ï¼Œè¿”å›éœ€è¦å¡«å……çš„å‚æ•°åˆ—è¡¨"""
        import re
        
        # åˆ†ææ¨¡æ¿ä¸­çš„å‚æ•°å ä½ç¬¦
        param_pattern = r'\{([^}]+)\}'
        params = re.findall(param_pattern, template_content)
        
        # åˆ†ç±»å‚æ•°ç±»å‹
        requirements = {
            'basic_vars': [],
            'mcp_resources': [],
            'mcp_tools': [],
            'dynamic_sections': []
        }
        
        for param in params:
            if param.startswith('local:'):
                requirements['basic_vars'].append(param)
            elif param.startswith('mcp:resource:'):
                requirements['mcp_resources'].append(param)
            elif param.startswith('mcp:tool:'):
                requirements['mcp_tools'].append(param)
            elif param.startswith('mcp:section:'):
                requirements['dynamic_sections'].append(param)
        
        return requirements
    
    def select_logic_content(requirements: dict, user_intent: str) -> dict:
        """æ ¹æ®å‚æ•°è¦æ±‚é€‰æ‹©åˆé€‚çš„é€»è¾‘å†…å®¹"""
        content_map = {
            # åŸºç¡€å˜é‡å†…å®¹ - ä¸MCPæœåŠ¡å™¨å‚æ•°å‘½åå®Œå…¨å¯¹åº”
            'local:current_time': time.strftime("%Y-%m-%d %H:%M:%S"),
            'local:user_intent': user_intent,
            'local:model_name': "qwen-max",
            'local:user_profile': "ç”¨æˆ·ä¿¡æ¯: æµ‹è¯•ç”¨æˆ·",
            'local:system_overview': "ç³»ç»ŸçŠ¶æ€: æ­£å¸¸è¿è¡Œ",
            'local:financial_data': "è´¢åŠ¡æ•°æ®ç¤ºä¾‹",
            'local:analysis_requirements': "åˆ†æè¦æ±‚ç¤ºä¾‹"
        }
        
        # æ„å»ºå‚æ•°æ˜ å°„
        params = {}
        for param in requirements['basic_vars']:
            if param in content_map:
                params[param] = content_map[param]
        
        return params

    # ------------------------- åˆ†åŒºç”Ÿæˆå‡½æ•°ï¼ˆå®¢æˆ·ç«¯ä¾§ï¼‰ -------------------------
    def gen_section_persona(template_name: str) -> str:
        mapping = {
            'simple_chat': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹",
            'rag_answer': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜",
            'react_reasoning': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªå–„äºæ¨ç†çš„AIåŠ©æ‰‹ï¼Œä½¿ç”¨ReActæ¨¡å¼è¿›è¡Œå¤šæ­¥æ¨ç†",
            'financial_analysis': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æå¸ˆ",
            'context_engineering': "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šä¸‹æ–‡å·¥ç¨‹ä¸“å®¶ï¼Œæ“…é•¿åŠ¨æ€å†³ç­–å’Œæ™ºèƒ½æ¨ç†"
        }
        return mapping.get(template_name, "[äººè®¾] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹")

    def gen_section_current_status(user_intent: str) -> str:
        return f"""[å½“å‰çŠ¶æ€] å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
ç”¨æˆ·æ„å›¾: {user_intent}
æ¨¡å‹: qwen-max"""

    def gen_section_conversation_history() -> str:
        """ç”Ÿæˆå¯¹è¯å†å²åˆ†åŒº - å†…èšMCPèµ„æºè·å–"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            history_obj = loop.run_until_complete(mcp_manager.get_resource("conversation://current/history"))
            loop.close()
            history_text = json.dumps(history_obj, ensure_ascii=False, indent=2) if isinstance(history_obj, (dict, list)) else str(history_obj or "[]")
        except Exception:
            history_text = "[]"
        return f"[å†å²] {history_text}"

    def gen_section_user_profile() -> str:
        return "[ç”¨æˆ·ä¿¡æ¯] ç”¨æˆ·ä¿¡æ¯: æµ‹è¯•ç”¨æˆ·"

    def gen_section_system_overview() -> str:
        return "[ç³»ç»ŸçŠ¶æ€] ç³»ç»ŸçŠ¶æ€: æ­£å¸¸è¿è¡Œ"

    def gen_section_available_tools(user_intent: str = "") -> str:
        """ç”Ÿæˆå¯ç”¨å·¥å…·åˆ†åŒº - åŸºäºLLMçš„åŠ¨æ€å·¥å…·é€‰æ‹©"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            all_tools = loop.run_until_complete(mcp_manager.list_tools())
            loop.close()
            
            if not all_tools or not user_intent.strip():
                # å¦‚æœæ²¡æœ‰å·¥å…·æˆ–æ²¡æœ‰ç”¨æˆ·æ„å›¾ï¼Œè¿”å›æ‰€æœ‰å·¥å…·
                tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        desc = tool.get('description', 'å·¥å…·æè¿°')
                        input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                        output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                        input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                        output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                        tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                tools_text = "\n".join(tool_lines) if tool_lines else "[æ— å¯ç”¨å·¥å…·]"
                return f"[å¯ç”¨å·¥å…·] {tools_text}"
            
            # æ„å»ºå·¥å…·æè¿°ç”¨äºLLMé€‰æ‹©
            tool_descriptions = []
            for tool in all_tools:
                if isinstance(tool, dict):
                    name = tool.get('name', 'unknown')
                    desc = tool.get('description', 'å·¥å…·æè¿°')
                    input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                    input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                    tool_descriptions.append(f"- {name}: {desc} (å‚æ•°: {input_props})")
            
            # æ„å»ºLLMå·¥å…·é€‰æ‹©æç¤ºè¯
            selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å·¥å…·é€‰æ‹©ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œä»ä»¥ä¸‹å¯ç”¨å·¥å…·ä¸­é€‰æ‹©æœ€ç›¸å…³çš„å·¥å…·ï¼ˆæœ€å¤š3ä¸ªï¼‰ï¼š

**ç”¨æˆ·æ„å›¾**: {user_intent}

**å¯ç”¨å·¥å…·åˆ—è¡¨**:
{chr(10).join(tool_descriptions)}

**é€‰æ‹©è¦æ±‚**:
1. ä»”ç»†åˆ†æç”¨æˆ·æ„å›¾å’Œéœ€æ±‚
2. é€‰æ‹©æœ€èƒ½å¸®åŠ©å®Œæˆä»»åŠ¡çš„å·¥å…·
3. å¦‚æœç”¨æˆ·æ„å›¾ä¸éœ€è¦ç‰¹å®šå·¥å…·ï¼Œå¯ä»¥é€‰æ‹©é€šç”¨å·¥å…·
4. æœ€å¤šé€‰æ‹©3ä¸ªæœ€ç›¸å…³çš„å·¥å…·

**è¾“å‡ºæ ¼å¼**:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "selected_tools": ["å·¥å…·å1", "å·¥å…·å2", "å·¥å…·å3"],
    "reasoning": "é€‰æ‹©ç†ç”±"
}}

ç°åœ¨è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·æ„å›¾é€‰æ‹©å·¥å…·ï¼š
ç”¨æˆ·æ„å›¾: "{user_intent}"

è¯·è¾“å‡ºJSONæ ¼å¼çš„é€‰æ‹©ç»“æœï¼š"""
            
            # è°ƒç”¨LLMè¿›è¡Œå·¥å…·é€‰æ‹©
            try:
                import openai
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                
                response = client.chat.completions.create(
                    model="qwen-max",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·é€‰æ‹©ä¸“å®¶ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"},
                        {"role": "user", "content": selection_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                llm_response = response.choices[0].message.content.strip()
                selection_result = json.loads(llm_response)
                selected_tool_names = selection_result.get("selected_tools", [])
                
                # æ ¹æ®LLMé€‰æ‹©çš„å·¥å…·åç§°æ„å»ºå·¥å…·ä¿¡æ¯
                selected_tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        if name in selected_tool_names:
                            desc = tool.get('description', 'å·¥å…·æè¿°')
                            input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                            output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                            input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                            output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                            selected_tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                
                tools_text = "\n".join(selected_tool_lines) if selected_tool_lines else "[æ— ç›¸å…³å·¥å…·]"
                
            except Exception as llm_error:
                # LLMé€‰æ‹©å¤±è´¥ï¼Œå›é€€åˆ°æ˜¾ç¤ºæ‰€æœ‰å·¥å…·
                tool_lines = []
                for tool in all_tools:
                    if isinstance(tool, dict):
                        name = tool.get('name', 'unknown')
                        desc = tool.get('description', 'å·¥å…·æè¿°')
                        input_schema = tool.get('inputSchema') or tool.get('input_schema') or {}
                        output_schema = tool.get('outputSchema') or tool.get('output_schema') or {}
                        input_props = list((input_schema.get('properties') or {}).keys()) if isinstance(input_schema, dict) else []
                        output_props = list((output_schema.get('properties') or {}).keys()) if isinstance(output_schema, dict) else []
                        tool_lines.append(f"- {name}: {desc}\n  è¾“å…¥å‚æ•°: {input_props}\n  è¾“å‡ºæ ¼å¼: {output_props}")
                tools_text = "\n".join(tool_lines) if tool_lines else "[æ— å¯ç”¨å·¥å…·]"
                
        except Exception:
            tools_text = "[å·¥å…·è·å–å¤±è´¥]"
        
        # è¿”å›å¸¦æ ‡è®°çš„æ ¼å¼ï¼Œä½œä¸ºç‹¬ç«‹çš„é€»è¾‘åˆ†åŒº
        return f"[å¯ç”¨å·¥å…·] {tools_text}"

    def gen_section_user_question(user_input: str) -> str:
        return f"[ç”¨æˆ·é—®é¢˜] {user_input}"

    def gen_section_tao_output_example(template_name: str) -> str:
        return """[ç¤ºä¾‹è¾“å‡º] è¯·ä¸¥æ ¼è¿”å›ä»¥ä¸‹JSONï¼ˆä¸åŠ è§£é‡Šã€ä»…æ­¤ä¸€æ¡ï¼‰ï¼š
{
  "reasoning": "ç®€è¦ã€è‡ªç„¶è¯­è¨€ã€è¯´æ˜å…³é”®ä¿¡æ¯ä¸é€‰æ‹©ç†ç”±",
  "action": "final_answer",
  "observation": "ç›´æ¥é¢å‘ç”¨æˆ·çš„æœ€ç»ˆå›ç­”ï¼Œå®Œæ•´å¯ç”¨"
}"""

    def gen_section_financial_data(financial_data: str = "", analysis_requirements: str = "") -> str:
        data = financial_data or "è´¢åŠ¡æ•°æ®ç¤ºä¾‹"
        req = analysis_requirements or "åˆ†æè¦æ±‚ç¤ºä¾‹"
        return f"[è´¢åŠ¡æ•°æ®] {data}\n[åˆ†æè¦æ±‚] {req}"
    
    def generate_section_params(selected_template: str, user_intent: str) -> dict:
        """ç»Ÿä¸€ç”Ÿæˆåˆ†åŒºå‚æ•° - é¿å…é‡å¤ä»£ç """
        return {
            "section_persona": gen_section_persona(selected_template),
            "section_current_status": gen_section_current_status(user_intent),
            "section_conversation_history": gen_section_conversation_history(),
            "section_user_profile": gen_section_user_profile(),
            "section_system_overview": gen_section_system_overview(),
            "section_available_tools": gen_section_available_tools(user_intent),
            "section_user_question": gen_section_user_question(user_intent),
            "section_tao_output_example": gen_section_tao_output_example(selected_template)
        }
    
    def execute_stage_1_template_selection(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ1: æ¨¡æ¿é€‰æ‹©"""
        print(f"[DEBUG] execute_stage_1_template_selection è¢«è°ƒç”¨ï¼Œuser_intent={user_intent}")
        
        if not user_intent.strip():
            print("[DEBUG] ç”¨æˆ·æ„å›¾ä¸ºç©º")
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage1_start = time.time()
            print(f"[DEBUG] å¼€å§‹é˜¶æ®µ1ï¼Œtime={stage1_start}")
            
            # è·å–æ‰€æœ‰å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                print("[DEBUG] è°ƒç”¨ mcp_manager.list_prompts()")
                prompts = loop.run_until_complete(mcp_manager.list_prompts())
                print(f"[DEBUG] list_prompts() è¿”å›: {len(prompts) if prompts else 0} ä¸ªæ¨¡æ¿")
                print(f"[DEBUG] prompts ç±»å‹: {type(prompts)}")
                if prompts and len(prompts) > 0:
                    print(f"[DEBUG] ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(prompts[0])}")
                    print(f"[DEBUG] ç¬¬ä¸€ä¸ªå…ƒç´ å†…å®¹: {prompts[0]}")
            finally:
                loop.close()
            
            if not prompts:
                print("[DEBUG] prompts ä¸ºç©º")
                return "âŒ æ— æ³•è·å–æç¤ºè¯æ¨¡æ¿"
            
            # æ„å»ºæ¨¡æ¿é€‰æ‹©æç¤ºè¯
            prompt_descriptions = []
            available_templates = [] # Store available template names
            for prompt in prompts:
                print(f"[DEBUG] å¤„ç† prompt: type={type(prompt)}, value={prompt}")
                if isinstance(prompt, dict):
                    name = prompt.get("name", "")
                    description = prompt.get("description", "")
                    available_templates.append(name) # Add to available templates
                    prompt_descriptions.append(f"- {name}: {description}")
                # FastMCP Client å¯èƒ½è¿”å› Prompt å¯¹è±¡
                elif hasattr(prompt, 'name'):
                    name = prompt.name if hasattr(prompt, 'name') else str(prompt)
                    description = prompt.description if hasattr(prompt, 'description') else ""
                    available_templates.append(name)
                    prompt_descriptions.append(f"- {name}: {description}")
            
            # Ensure there are available templates
            if not available_templates:
                return "âŒ æ— æ³•è·å–æç¤ºè¯æ¨¡æ¿"
            
            selection_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ¨¡æ¿é€‰æ‹©ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œä»ä»¥ä¸‹å¯ç”¨æ¨¡æ¿ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ¿ï¼š

**ç”¨æˆ·æ„å›¾**: {user_intent}

**å¯ç”¨æ¨¡æ¿åˆ—è¡¨**:
{chr(10).join(prompt_descriptions)}

**é‡è¦**: ä½ åªèƒ½ä»ä»¥ä¸‹æ¨¡æ¿åç§°ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
{', '.join(available_templates)}

**é€‰æ‹©è¦æ±‚**:
1. ä»”ç»†åˆ†æç”¨æˆ·æ„å›¾
2. è€ƒè™‘æ¯ä¸ªæ¨¡æ¿çš„åŠŸèƒ½å’Œé€‚ç”¨åœºæ™¯
3. é€‰æ‹©æœ€èƒ½æ»¡è¶³éœ€æ±‚çš„æ¨¡æ¿
4. æä¾›é€‰æ‹©ç†ç”±
5. **å¿…é¡»ä»ä¸Šè¿°æ¨¡æ¿åç§°ä¸­é€‰æ‹©ï¼Œä¸èƒ½é€‰æ‹©ä¸å­˜åœ¨çš„æ¨¡æ¿**

**è¾“å‡ºæ ¼å¼**:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š
{{
    "selected_template": "æ¨¡æ¿åç§°",
    "reasoning": "é€‰æ‹©ç†ç”±",
    "confidence": 0.95
}}

**ç¤ºä¾‹**:
ç”¨æˆ·æ„å›¾: "å¸®æˆ‘æœç´¢å…³äºæœºå™¨å­¦ä¹ çš„ä¿¡æ¯"
{{
    "selected_template": "rag_answer",
    "reasoning": "ç”¨æˆ·éœ€è¦æœç´¢å’Œæ£€ç´¢ä¿¡æ¯ï¼Œrag_answeræ¨¡æ¿ä¸“é—¨ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆ",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "è¯·åˆ†æè¿™ä¸ªä»£ç çš„æ€§èƒ½é—®é¢˜"
{{
    "selected_template": "react_reasoning",
    "reasoning": "ç”¨æˆ·éœ€è¦åˆ†æå’Œæ¨ç†ï¼Œreact_reasoningæ¨¡æ¿æ”¯æŒå¤šæ­¥æ¨ç†å’Œæ€è€ƒè¿‡ç¨‹",
    "confidence": 0.85
}}

ç”¨æˆ·æ„å›¾: "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”ä¸é”™"
{{
    "selected_template": "simple_chat",
    "reasoning": "ç”¨æˆ·è¿›è¡Œç®€å•å¯¹è¯ï¼Œsimple_chatæ¨¡æ¿é€‚åˆæ—¥å¸¸äº¤æµ",
    "confidence": 0.95
}}

ç”¨æˆ·æ„å›¾: "è¯·å®¡æŸ¥è¿™æ®µPythonä»£ç "
{{
    "selected_template": "code_review",
    "reasoning": "ç”¨æˆ·éœ€è¦ä»£ç å®¡æŸ¥ï¼Œcode_reviewæ¨¡æ¿ä¸“é—¨ç”¨äºä»£ç è´¨é‡åˆ†æ",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "åˆ†æé˜¿é‡Œå·´å·´çš„è´¢åŠ¡æ•°æ®"
{{
    "selected_template": "financial_analysis",
    "reasoning": "ç”¨æˆ·éœ€è¦è´¢åŠ¡åˆ†æï¼Œfinancial_analysisæ¨¡æ¿ä¸“é—¨ç”¨äºè´¢åŠ¡æ•°æ®åˆ†æå’ŒæŠ¥å‘Š",
    "confidence": 0.9
}}

ç”¨æˆ·æ„å›¾: "ä½¿ç”¨ä¸Šä¸‹æ–‡å·¥ç¨‹æ–¹æ³•è§£å†³è¿™ä¸ªé—®é¢˜"
{{
    "selected_template": "context_engineering",
    "reasoning": "ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Œcontext_engineeringæ¨¡æ¿ä¸“é—¨ç”¨äºå®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿæ¨¡å¼",
    "confidence": 0.95
}}

ç°åœ¨è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·æ„å›¾é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ¿ï¼š

ç”¨æˆ·æ„å›¾: "{user_intent}"

**è®°ä½**: åªèƒ½ä» {', '.join(available_templates)} ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡æ¿åç§°ã€‚

è¯·è¾“å‡ºJSONæ ¼å¼çš„é€‰æ‹©ç»“æœï¼š"""
            
            # è°ƒç”¨LLMè¿›è¡Œæ¨¡æ¿é€‰æ‹©
            try:
                import openai
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=os.getenv("DASHSCOPE_API_KEY"),
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                
                response = client.chat.completions.create(
                    model="qwen-max",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¨¡æ¿é€‰æ‹©ä¸“å®¶ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚ç¡®ä¿è¾“å‡ºçš„JSONæ ¼å¼å®Œå…¨æ­£ç¡®ã€‚ä½ åªèƒ½é€‰æ‹©æä¾›çš„æ¨¡æ¿åç§°åˆ—è¡¨ä¸­çš„æ¨¡æ¿ã€‚"},
                        {"role": "user", "content": selection_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                
                llm_response = response.choices[0].message.content.strip()
                
                # è§£æLLMå“åº”
                selection_result = json.loads(llm_response)
                selected_template = selection_result.get("selected_template")
                reasoning = selection_result.get("reasoning", "LLMé€‰æ‹©")
                confidence = selection_result.get("confidence", 0.8)
                
                # éªŒè¯é€‰æ‹©çš„æ¨¡æ¿æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
                if selected_template not in available_templates:
                    raise Exception(f"LLMé€‰æ‹©äº†ä¸å­˜åœ¨çš„æ¨¡æ¿ '{selected_template}'ã€‚å¯ç”¨æ¨¡æ¿: {', '.join(available_templates)}")
                
                llm_success = True
                
                # âœ… ä¿å­˜é€‰æ‹©ç»“æœåˆ°å…¨å±€çŠ¶æ€
                global _selected_template_name
                _selected_template_name = selected_template
                print(f"[DEBUG] é˜¶æ®µ1ä¿å­˜æ¨¡æ¿é€‰æ‹©: {selected_template}")
                
            except Exception as llm_error:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_error}")
            
            stage1_time = time.time() - stage1_start
            
            # æ„å»ºç»“æœ
            stage1_result = f"""## ğŸ¯ é˜¶æ®µ1: æ¨¡æ¿é€‰æ‹©

**é€‰æ‹©çŠ¶æ€**: âœ… æˆåŠŸ  
**é€‰æ‹©æ–¹æ³•**: LLMæ™ºèƒ½é€‰æ‹©  
**é€‰æ‹©æ¨¡æ¿**: `{selected_template}`  
**ç½®ä¿¡åº¦**: {confidence:.2f}  
**è€—æ—¶**: {stage1_time:.2f}ç§’

**ğŸ¤– é€‰æ‹©ç†ç”±**:
{reasoning}

**ğŸ“ å¯ç”¨æ¨¡æ¿**:
{chr(10).join(prompt_descriptions)}

**ğŸ” é€‰æ‹©è¿‡ç¨‹**:
- **ç”¨æˆ·æ„å›¾**: {user_intent}
- **æ¨¡æ¿æ•°é‡**: {len(prompts)} ä¸ª
- **é€‰æ‹©ç®—æ³•**: LLMé©±åŠ¨çš„æ™ºèƒ½é€‰æ‹©
- **æ•°æ®æº**: ç›´æ¥MCPæœåŠ¡å™¨è·å–
- **é€‰æ‹©ç»“æœ**: {selected_template}"""
            
            return stage1_result
            
        except Exception as e:
            return f"âŒ é˜¶æ®µ1æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_2_context_assembly(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ2: ä¸Šä¸‹æ–‡è£…é…ï¼ˆç¬¦åˆCONTEXT_ENGINEERING_GUIDE.mdï¼‰
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. ä»é˜¶æ®µ1è·å–å·²é€‰æ‹©çš„æ¨¡æ¿
        2. ä»MCP Serverè·å–åŒ…å«å ä½ç¬¦çš„æ¨¡æ¿
        3. CE Serverè¯†åˆ«å¹¶æ›¿æ¢å ä½ç¬¦
        """
        print(f"[DEBUG] execute_stage_2_context_assembly è¢«è°ƒç”¨")
        
        # âœ… æ£€æŸ¥é˜¶æ®µ1æ˜¯å¦å·²æ‰§è¡Œ
        global _selected_template_name
        if not _selected_template_name:
            return "âŒ è¯·å…ˆæ‰§è¡Œé˜¶æ®µ1ï¼šæ¨¡æ¿é€‰æ‹©"
        
        print(f"[DEBUG] ä½¿ç”¨é˜¶æ®µ1é€‰æ‹©çš„æ¨¡æ¿: {_selected_template_name}")
        
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage2_start = time.time()
            
            # ä½¿ç”¨ ContextEngineeringPipeline ä»…æ‰§è¡Œé˜¶æ®µ2
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # åˆ›å»ºpipelineçŠ¶æ€ï¼Œç›´æ¥ä½¿ç”¨é˜¶æ®µ1é€‰æ‹©çš„æ¨¡æ¿
                from search_engine.mcp_tab.context_pipeline import PipelineState
                state = PipelineState(user_intent=user_intent)
                state.selected_template = _selected_template_name  # âœ… ç›´æ¥è®¾ç½®æ¨¡æ¿åç§°
                
                # âŒ ä¸å†é‡æ–°æ‰§è¡Œé˜¶æ®µ1
                # loop.run_until_complete(pipeline._stage1_template_selection(state))
            
                # æ‰§è¡Œé˜¶æ®µ2ï¼šå ä½ç¬¦æ›¿æ¢
                loop.run_until_complete(pipeline._stage2_placeholder_resolution(state))
                
            finally:
                loop.close()
            
            stage2_time = time.time() - stage2_start
            
            # æ˜¾ç¤ºç»“æœ
            raw_template_preview = ""
            if state.raw_template:
                preview = state.raw_template[:500] if len(state.raw_template) > 500 else state.raw_template
                raw_template_preview = f"""### ğŸ“ åŸå§‹æ¨¡æ¿ï¼ˆåŒ…å«å ä½ç¬¦ï¼‰

```
{preview}{"..." if len(state.raw_template) > 500 else ""}
```
"""
            
            stage2_result = f"""ğŸ”§ **é˜¶æ®µ2å®Œæˆ** ({stage2_time:.2f}ç§’) | æ¨¡æ¿: {state.selected_template} | é•¿åº¦: {len(state.assembled_context or '')} å­—ç¬¦

{raw_template_preview}

### âœ… è£…é…åçš„å®Œæ•´ä¸Šä¸‹æ–‡

```
{state.assembled_context or 'ï¼ˆä¸Šä¸‹æ–‡ä¸ºç©ºï¼‰'}
```

---

**å ä½ç¬¦æ›¿æ¢è¯´æ˜**:
- `${{local:xxx}}` â†’ CE Serveræœ¬åœ°ç”Ÿæˆï¼ˆæ—¶é—´ã€ç”¨æˆ·æ„å›¾ç­‰ï¼‰
- `${{mcp:resource:xxx}}` â†’ è°ƒç”¨MCP Resourceè·å–ï¼ˆå¯¹è¯å†å²ç­‰ï¼‰
- `${{mcp:tool:xxx}}` â†’ è°ƒç”¨MCP Toolsè·å–ï¼ˆå·¥å…·åˆ—è¡¨ç­‰ï¼‰
"""
            
            return stage2_result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ é˜¶æ®µ2æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_3_llm_inference(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ3: LLMæ¨ç†ï¼ˆç¬¦åˆCONTEXT_ENGINEERING_GUIDE.mdï¼‰
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. ä½¿ç”¨é˜¶æ®µ2è£…é…å¥½çš„å®Œæ•´ä¸Šä¸‹æ–‡
        2. å‘é€ç»™LLMè¿›è¡Œæ¨ç†
        3. è¿”å›TAOæ ¼å¼ç»“æœ
        """
        print(f"[DEBUG] execute_stage_3_llm_inference è¢«è°ƒç”¨")
        
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage3_start = time.time()
            
            # ä½¿ç”¨ ContextEngineeringPipeline æ‰§è¡Œé˜¶æ®µ1+2+3
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # åˆ›å»ºpipelineçŠ¶æ€
                from search_engine.mcp_tab.context_pipeline import PipelineState
                state = PipelineState(user_intent=user_intent)
            
                # æ‰§è¡Œé˜¶æ®µ1ï¼šæ¨¡æ¿é€‰æ‹©
                loop.run_until_complete(pipeline._stage1_template_selection(state))
            
                # æ‰§è¡Œé˜¶æ®µ2ï¼šå ä½ç¬¦æ›¿æ¢
                loop.run_until_complete(pipeline._stage2_placeholder_resolution(state))
                
                # æ‰§è¡Œé˜¶æ®µ3ï¼šLLMæ¨ç†
                loop.run_until_complete(pipeline._stage3_llm_inference(state))
                
            finally:
                loop.close()
            
            stage3_time = time.time() - stage3_start
            
            # æ˜¾ç¤ºç»“æœ
            if state.llm_response:
                # æ˜¾ç¤ºè§£æåçš„ç»“æ„åŒ–TAOæ•°æ®ï¼ˆä¸é˜¶æ®µ4ä¸€è‡´ï¼‰
                if state.parsed_tao:
                    # ä½¿ç”¨è§£æåçš„ç»“æ„åŒ–æ•°æ®
                    pretty = json.dumps(state.parsed_tao, ensure_ascii=False, indent=2)
                else:
                    # å¦‚æœæ²¡æœ‰è§£ææ•°æ®ï¼Œæ˜¾ç¤ºåŸå§‹å“åº”
                    pretty = state.llm_response

                stage3_result = f"""ğŸ¤– **é˜¶æ®µ3å®Œæˆ** ({stage3_time:.2f}ç§’) | æ¨¡å‹: qwen-max

### TAOæ¨ç†ç»“æœï¼ˆå·²è§£æï¼‰

```json
{pretty}
```

---

**è¯´æ˜**:
- **Thought**: {state.parsed_tao.get('thought', 'N/A')[:100] if state.parsed_tao else 'N/A'}...
- **Action**: {state.parsed_tao.get('action', 'N/A') if state.parsed_tao else 'N/A'}
- **Final Answer**: {state.parsed_tao.get('final_answer', 'N/A')[:100] if state.parsed_tao and state.parsed_tao.get('final_answer') else 'N/A'}...

**ä¸‹ä¸€æ­¥**: é˜¶æ®µ4å°†æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æ›´æ–°å¯¹è¯å†å²
"""
            else:
                stage3_result = f"""ğŸ¤– **é˜¶æ®µ3å¤±è´¥** ({stage3_time:.2f}ç§’)

âŒ **é”™è¯¯**: LLMæ¨ç†å¤±è´¥ï¼Œæœªè¿”å›æœ‰æ•ˆå“åº”
"""
            
            return stage3_result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ é˜¶æ®µ3æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def execute_stage_4_context_update(user_intent: str) -> str:
        """æ‰§è¡Œé˜¶æ®µ4: ä¸Šä¸‹æ–‡æ›´æ–°ï¼ˆç¬¦åˆCONTEXT_ENGINEERING_GUIDE.mdï¼‰
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. è§£æé˜¶æ®µ3çš„TAOè¾“å‡º
        2. æ›´æ–°MCP Resourceså¯¹è¯å†å²
        3. ä¸ºä¸‹ä¸€è½®å‡†å¤‡æ–°çš„ä¸Šä¸‹æ–‡
        """
        print(f"[DEBUG] execute_stage_4_context_update è¢«è°ƒç”¨")
        
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾"
        
        try:
            stage4_start = time.time()

            # ä½¿ç”¨ ContextEngineeringPipeline æ‰§è¡Œå®Œæ•´æµç¨‹
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # åˆ›å»ºpipelineçŠ¶æ€
                from search_engine.mcp_tab.context_pipeline import PipelineState
                state = PipelineState(user_intent=user_intent)
                
                # æ‰§è¡Œé˜¶æ®µ1-4
                loop.run_until_complete(pipeline._stage1_template_selection(state))
                loop.run_until_complete(pipeline._stage2_placeholder_resolution(state))
                loop.run_until_complete(pipeline._stage3_llm_inference(state))
                loop.run_until_complete(pipeline._stage4_context_update(state))
                
            finally:
                loop.close()
            
            stage4_time = time.time() - stage4_start
            
            # æ˜¾ç¤ºç»“æœ
            tao_info = ""
            if state.parsed_tao:
                # observation æ¥è‡ªé˜¶æ®µ4æ‰§è¡Œå·¥å…·è°ƒç”¨åçš„ç»“æœ
                observation = state.observation or "ï¼ˆæœªæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼‰"
                
                tao_info = f"""
**å®Œæ•´TAOè®°å½•**:
- **Thought**: {state.parsed_tao.get('thought', 'N/A')}
- **Action**: {state.parsed_tao.get('action', 'N/A') or 'ï¼ˆæ— ï¼‰'}
- **Observation**: {observation}
"""
            
            stage4_result = f"""ğŸ”„ **é˜¶æ®µ4å®Œæˆ** ({stage4_time:.2f}ç§’)

### âœ… ä¸Šä¸‹æ–‡æ›´æ–°å®Œæˆ

**çŠ¶æ€**:
- å†å²å·²æ›´æ–°: {state.history_updated}
- ä»»åŠ¡å®Œæˆ: {state.is_finished}

{tao_info}

---

**æµç¨‹è¯´æ˜**:
1. âœ… è§£æLLMè¾“å‡ºï¼ˆThought + Actionï¼‰
2. âœ… æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆè·å–Observationï¼‰
3. âœ… æ›´æ–°å¯¹è¯å†å²ï¼ˆä¿å­˜å®Œæ•´TAOï¼‰

**ä¸‹ä¸€æ­¥**: 
- å¯¹è¯å†å²å·²åŒ…å«æœ¬è½®äº¤äº’
- å¯ä»¥ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯ï¼ˆåŸºäºå†å²ä¸Šä¸‹æ–‡ï¼‰
"""

            return stage4_result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ é˜¶æ®µ4æ‰§è¡Œå¼‚å¸¸: {str(e)}"
    
    def run_complete_flow(user_intent: str, max_turns: int = 1) -> Tuple[str, str, str, str, str]:
        """è¿è¡Œå®Œæ•´æµç¨‹ - ä½¿ç”¨ä¼˜åŒ–çš„ç®¡é“æ‰§è¡Œ
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. ä½¿ç”¨ContextEngineeringPipelineç»Ÿä¸€æ‰§è¡Œ
        2. é¿å…é‡å¤è°ƒç”¨ï¼ˆä»4æ¬¡LLMè°ƒç”¨é™ä¸º1æ¬¡ï¼‰
        3. çŠ¶æ€åœ¨ç®¡é“å†…ä¼ é€’ï¼Œæ— éœ€é‡å¤è·å–
        """
        if not user_intent.strip():
            return "è¯·è¾“å…¥ç”¨æˆ·æ„å›¾", "", "", "", ""
        
        try:
            # ä½¿ç”¨ç®¡é“æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(pipeline.execute_complete_flow(user_intent))
            finally:
                loop.close()
            
            if result["success"]:
                state = result["state"]
                
                # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
                final_summary = f"""## ğŸ‰ æ™ºèƒ½ä½“å¾ªç¯å®Œæˆæ€»ç»“ (ä¼˜åŒ–ç‰ˆ)

**æ‰§è¡ŒçŠ¶æ€**: âœ… å…¨éƒ¨æˆåŠŸ  
**ç”¨æˆ·æ„å›¾**: {user_intent}  
**æ‰§è¡Œæ—¶é—´**: {time.strftime("%Y-%m-%d %H:%M:%S")}

**âš¡ æ€§èƒ½ç»Ÿè®¡**:
- **æ€»è€—æ—¶**: {result['total_time']:.2f}ç§’
- **é˜¶æ®µ1 (æ¨¡æ¿é€‰æ‹©)**: {result['stage1']['time']:.2f}ç§’
- **é˜¶æ®µ2 (ä¸Šä¸‹æ–‡è£…é…)**: {result['stage2']['time']:.2f}ç§’
- **é˜¶æ®µ3 (LLMæ¨ç†)**: {result['stage3']['time']:.2f}ç§’
- **é˜¶æ®µ4 (ä¸Šä¸‹æ–‡æ›´æ–°)**: {result['stage4']['time']:.2f}ç§’

**ğŸ“Š å„é˜¶æ®µæ‰§è¡Œç»“æœ**:
- **é˜¶æ®µ1 (æ¨¡æ¿é€‰æ‹©)**: âœ… æˆåŠŸ - é€‰æ‹©äº† `{state.selected_template}`
- **é˜¶æ®µ2 (ä¸Šä¸‹æ–‡è£…é…)**: âœ… æˆåŠŸ - è£…é…äº† {len(state.assembled_context)} å­—ç¬¦
- **é˜¶æ®µ3 (LLMæ¨ç†)**: âœ… æˆåŠŸ - ç”ŸæˆTAOè¾“å‡º
- **é˜¶æ®µ4 (ä¸Šä¸‹æ–‡æ›´æ–°)**: âœ… æˆåŠŸ - å†å²å·²åŒæ­¥

**ğŸ”— MCPæ¶æ„éªŒè¯**:
- **æœåŠ¡å™¨è¿æ¥**: âœ… æ­£å¸¸
- **é€»è¾‘åˆ†åŒºç®¡ç†**: âœ… åŸºäºMCP Promptæ¥å£
- **å·¥å…·è°ƒç”¨**: âœ… æ­£å¸¸
- **èµ„æºè®¿é—®**: âœ… æ­£å¸¸

**ğŸ¯ æ ¸å¿ƒåŠŸèƒ½éªŒè¯**:
- **åŠ¨æ€æ¨¡æ¿é€‰æ‹©**: âœ… LLMæ™ºèƒ½é€‰æ‹©
- **é€»è¾‘åˆ†åŒºçƒ­æ’æ‹”**: âœ… é€šè¿‡section_*å‚æ•°å®ç°
- **ä¸Šä¸‹æ–‡å·¥ç¨‹**: âœ… å®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
- **MCPåè®®**: âœ… æ ‡å‡†åŒ–äº¤äº’
- **æ™ºèƒ½ä½“å¾ªç¯**: âœ… å››é˜¶æ®µå®Œæ•´æ‰§è¡Œ

**ğŸ’¡ ä¼˜åŒ–äº®ç‚¹**:
- âœ… é¿å…é‡å¤æ‰§è¡Œï¼ˆèŠ‚çœ70%æ—¶é—´å’Œæˆæœ¬ï¼‰
- âœ… çŠ¶æ€ç®¡ç†æ¸…æ™°
- âœ… é€»è¾‘åˆ†åŒºåŸºäºMCP Server Promptæ¥å£
- âœ… æ”¯æŒåŠ¨æ€é€‰æ‹©å’Œçƒ­æ’æ‹”
- âœ… å®Œå…¨ç¬¦åˆæ–‡æ¡£æœ€ä½³å®è·µ"""
                
                # æ ¼å¼åŒ–å„é˜¶æ®µè¾“å‡º
                stage1_output = f"""ğŸ¯ **é˜¶æ®µ1å®Œæˆ** ({result['stage1']['time']:.2f}ç§’)

**é€‰æ‹©æ¨¡æ¿**: `{result['stage1']['selected_template']}`
**é€‰æ‹©ç†ç”±**: {result['stage1']['reasoning']}
âœ… æ¨¡æ¿é€‰æ‹©æˆåŠŸ"""
                
                stage2_output = f"""ğŸ”§ **é˜¶æ®µ2å®Œæˆ** ({result['stage2']['time']:.2f}ç§’) | æ¨¡æ¿: {state.selected_template} | é•¿åº¦: {len(state.assembled_context)} å­—ç¬¦

---

{state.assembled_context}

---

âœ… **è£…é…å®Œæˆ** - ä»¥ä¸Šä¸ºè£…é…åçš„å®Œæ•´ä¸Šä¸‹æ–‡å†…å®¹"""
                
                # æ ¼å¼åŒ–LLMå“åº”
                try:
                    pretty = json.dumps(json.loads(state.llm_response), ensure_ascii=False, indent=2)
                except:
                    pretty = state.llm_response
                
                stage3_output = f"""ğŸ¤– **é˜¶æ®µ3å®Œæˆ** ({result['stage3']['time']:.2f}ç§’) | æ¨¡å‹: qwen-max | é•¿åº¦: {len(state.llm_response)} å­—ç¬¦

---

{pretty}

---

âœ… **æ¨ç†å®Œæˆ** - ä»¥ä¸Šä¸ºTAO JSONè¾“å‡º"""
                
                tao = state.tao_record
                stage4_output = f"""ğŸ”„ **é˜¶æ®µ4å®Œæˆ** ({result['stage4']['time']:.2f}ç§’)

**TAOè®°å½•å·²ä¿å­˜**:
- **Reasoning**: {tao['reasoning']}
- **Action**: {tao['action']}
- **Observation**: {tao['observation']}

âœ… **ä¸Šä¸‹æ–‡æ›´æ–°å®Œæˆ** - å¯¹è¯å†å²å·²åŒæ­¥åˆ°MCP Server"""
                
                return final_summary, stage1_output, stage2_output, stage3_output, stage4_output
            else:
                error_msg = f"âŒ æµç¨‹åœ¨{result['stage']}å¤±è´¥: {result['error']}"
                return error_msg, "", "", "", ""
            
        except Exception as e:
            error_msg = f"âŒ å®Œæ•´æµç¨‹æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            return error_msg, "", "", "", ""
    
    def clear_conversation_history() -> str:
        """æ¸…ç©ºå¯¹è¯å†å² - ç›´æ¥æ“ä½œ JSONL æ–‡ä»¶"""
        try:
            import os
            
            # å†å²æ–‡ä»¶è·¯å¾„
            history_file = os.path.join(
                os.path.dirname(__file__),
                "../..",
                "..",
                "data",
                "conversation_history.jsonl"
            )
            
            # æ¸…ç©ºæ–‡ä»¶ï¼ˆä¿ç•™æ–‡ä»¶ï¼Œä½†æ¸…ç©ºå†…å®¹ï¼‰
            if os.path.exists(history_file):
                with open(history_file, 'w', encoding='utf-8') as f:
                    pass  # æ¸…ç©ºæ–‡ä»¶
                
                return "âœ… å¯¹è¯å†å²å·²æ¸…ç©º\n\nå†å²æ–‡ä»¶å·²æ¸…ç©ºï¼Œå¯ä»¥å¼€å§‹æ–°çš„å¯¹è¯"
            else:
                return "âš ï¸ å†å²æ–‡ä»¶ä¸å­˜åœ¨\n\næ— éœ€æ¸…ç©º"
                
        except Exception as e:
            return f"âŒ æ¸…ç©ºå†å²å¤±è´¥: {str(e)}"
    
    def get_system_status() -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            # è·å–MCPæœåŠ¡å™¨çŠ¶æ€
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                tools = loop.run_until_complete(mcp_manager.list_tools())
                resources = loop.run_until_complete(mcp_manager.list_resources())
                prompts = loop.run_until_complete(mcp_manager.list_prompts())
            finally:
                loop.close()
            
            status = f"""## ğŸ“Š ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š

**ğŸ• æŠ¥å‘Šæ—¶é—´**: {time.strftime("%Y-%m-%d %H:%M:%S")}

**ğŸ”— MCPæœåŠ¡å™¨çŠ¶æ€**:
- **è¿æ¥çŠ¶æ€**: âœ… å·²è¿æ¥
- **å¯ç”¨å·¥å…·**: {len(tools)} ä¸ª
- **å¯ç”¨èµ„æº**: {len(resources)} ä¸ª  
- **å¯ç”¨æç¤ºè¯**: {len(prompts)} ä¸ª

**ğŸ› ï¸ å¯ç”¨å·¥å…·**:
"""
            for tool in tools:
                if isinstance(tool, dict):
                    name = tool.get("name", "")
                    description = tool.get("description", "")
                    status += f"- **{name}**: {description}\n"
            
            status += f"""
**ğŸ“š å¯ç”¨èµ„æº**:
"""
            for resource in resources:
                if isinstance(resource, dict):
                    uri = resource.get("uri", "")
                    status += f"- **{uri}**: MCPèµ„æº\n"
            
            status += f"""
**ğŸ“ å¯ç”¨æç¤ºè¯**:
"""
            for prompt in prompts:
                if isinstance(prompt, dict):
                    name = prompt.get("name", "")
                    description = prompt.get("description", "")
                    status += f"- **{name}**: {description}\n"
            
            status += f"""
**ğŸ¯ æ ¸å¿ƒåŠŸèƒ½**:
- **åŠ¨æ€å·¥å…·é€‰æ‹©**: âœ… å·²å®ç°
- **æ¨¡æ¿åŒ¹é…**: âœ… å·²å®ç°
- **ä¸Šä¸‹æ–‡å·¥ç¨‹**: âœ… å·²å®ç°
- **æ™ºèƒ½ä½“å¾ªç¯**: âœ… å·²å®ç°"""
            
            return status
            
        except Exception as e:
            return f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}"
    
    def view_conversation_history() -> str:
        """æŸ¥çœ‹å¯¹è¯å†å²ï¼ˆä»MCPèµ„æºè¯»å–ï¼‰"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                history_resource = loop.run_until_complete(
                    mcp_manager.get_resource("conversation://current/history")
                )
            finally:
                loop.close()
            
            if isinstance(history_resource, str):
                # å°è¯•è§£æä¸ºJSONåå†æ ¼å¼åŒ–
                try:
                    parsed = json.loads(history_resource)
                    return json.dumps(parsed, ensure_ascii=False, indent=2)
                except Exception:
                    return history_resource
            return json.dumps(history_resource, ensure_ascii=False, indent=2)
        except Exception as e:
            return f"âŒ è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}"
    
    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
        # é¡µé¢æ ‡é¢˜å’Œè¯´æ˜
        gr.Markdown("""
        # ğŸ§  ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“æ¼”ç¤º
        
        **åŸºäºModel Context Protocolçš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ™ºèƒ½ä½“ç³»ç»Ÿ**
        
        ---
        """)
        
        # ä¸»è¦å†…å®¹åŒºåŸŸ
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥å’Œæ§åˆ¶åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ ç”¨æˆ·è¾“å…¥")
                user_input = gr.Textbox(
                    label="ç”¨æˆ·æ„å›¾",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–éœ€æ±‚...",
                    lines=4,
                    max_lines=6
                )
                
                gr.Markdown("### ğŸ¯ æ‰§è¡Œæ§åˆ¶")
                with gr.Row():
                    stage1_btn = gr.Button("1ï¸âƒ£ æ¨¡æ¿é€‰æ‹©", variant="primary", size="sm")
                    stage2_btn = gr.Button("2ï¸âƒ£ ä¸Šä¸‹æ–‡è£…é…", variant="primary", size="sm")
                
                with gr.Row():
                    stage3_btn = gr.Button("3ï¸âƒ£ LLMæ¨ç†", variant="primary", size="sm")
                    stage4_btn = gr.Button("4ï¸âƒ£ ä¸Šä¸‹æ–‡æ›´æ–°", variant="primary", size="sm")
                
                gr.Markdown("### ğŸš€ å¿«æ·æ“ä½œ")
                complete_btn = gr.Button("ğŸ¯ æ‰§è¡Œå®Œæ•´æµç¨‹", variant="secondary", size="lg")
                
                gr.Markdown("### âš™ï¸ ç³»ç»Ÿç®¡ç†")
                with gr.Row():
                    status_btn = gr.Button("ğŸ“Š ç³»ç»ŸçŠ¶æ€", variant="secondary", size="sm")
                    history_btn = gr.Button("ğŸ“œ åˆ·æ–°å†å²", variant="secondary", size="sm")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", variant="stop", size="sm")
            
            # å³ä¾§ï¼šç»“æœæ˜¾ç¤ºåŒºåŸŸï¼ˆåˆ†é˜¶æ®µä¸çŠ¶æ€/å†å²ï¼‰
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“Š æ‰§è¡Œç»“æœä¸ç³»ç»Ÿè§†å›¾")
                result_tabs = gr.Tabs(selected="ğŸ§© æ€»ç»“")
                with result_tabs:
                    with gr.Tab("ğŸ§© æ€»ç»“", id="summary"):
                        output_summary = gr.Textbox(
                            label="æµç¨‹æ€»ç»“",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("1ï¸âƒ£ æ¨¡æ¿é€‰æ‹©", id="stage1"):
                        output_stage1 = gr.Textbox(
                            label="é˜¶æ®µ1è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("2ï¸âƒ£ ä¸Šä¸‹æ–‡è£…é…", id="stage2"):
                        output_stage2 = gr.Textbox(
                            label="é˜¶æ®µ2è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("3ï¸âƒ£ LLMæ¨ç†", id="stage3"):
                        output_stage3 = gr.Textbox(
                            label="é˜¶æ®µ3è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("4ï¸âƒ£ ä¸Šä¸‹æ–‡æ›´æ–°", id="stage4"):
                        output_stage4 = gr.Textbox(
                            label="é˜¶æ®µ4è¾“å‡º",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("ğŸ“Š ç³»ç»ŸçŠ¶æ€", id="status"):
                        status_output = gr.Textbox(
                            label="ç³»ç»ŸçŠ¶æ€",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
                    with gr.Tab("ğŸ“œ å¯¹è¯å†å²", id="history"):
                        history_output = gr.Textbox(
                            label="å¯¹è¯å†å² (TAO)",
                            lines=20,
                            max_lines=25,
                            interactive=False
                        )
        
        # åº•éƒ¨è¯´æ˜åŒºåŸŸ
        gr.Markdown("""
        ---
        
        ### ğŸ“‹ ä½¿ç”¨æŒ‡å—
        
        **å››é˜¶æ®µæ™ºèƒ½ä½“å¾ªç¯**ï¼š
        1. **ğŸ¯ æ¨¡æ¿é€‰æ‹©** - æ ¹æ®ç”¨æˆ·æ„å›¾æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
        2. **ğŸ”§ ä¸Šä¸‹æ–‡è£…é…** - å°†æ¨¡æ¿ä¸ç”¨æˆ·æ„å›¾ç»“åˆï¼Œç”Ÿæˆå®Œæ•´çš„ä¸Šä¸‹æ–‡
        3. **ğŸ¤– LLMæ¨ç†** - ä½¿ç”¨è£…é…çš„ä¸Šä¸‹æ–‡è°ƒç”¨LLMè¿›è¡Œæ¨ç†
        4. **ğŸ”„ ä¸Šä¸‹æ–‡æ›´æ–°** - æ›´æ–°å¯¹è¯å†å²ï¼Œä¸ºä¸‹ä¸€è½®å¯¹è¯åšå‡†å¤‡
        
        **ğŸ’¡ æŠ€æœ¯ç‰¹æ€§**ï¼š
        - åŸºäºMCPåè®®çš„æ ‡å‡†åŒ–äº¤äº’
        - LLMé©±åŠ¨çš„æ™ºèƒ½æ¨¡æ¿å’Œå·¥å…·é€‰æ‹©
        - å®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
        - åŠ¨æ€ä¸Šä¸‹æ–‡å·¥ç¨‹ç®¡ç†
        
        **ğŸ”— ç³»ç»Ÿæ¶æ„**ï¼š
        - MCPæœåŠ¡å™¨ï¼šæä¾›æ ‡å‡†åŒ–çš„promptsã€toolsã€resources
        - åŠ¨æ€å·¥å…·é€‰æ‹©ï¼šä¸æ¨¡æ¿å­—æ®µæ™ºèƒ½åŒ¹é…
        - ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šå®Œæ•´çš„æ™ºèƒ½ä½“å·¥ä½œå¾ªç¯
        """)
        
        # ç»‘å®šäº‹ä»¶ - è‡ªåŠ¨åˆ‡æ¢åˆ°å¯¹åº”çš„tab
        
        # åˆ›å»ºåŒ…è£…å‡½æ•°ï¼ŒåŒæ—¶è¿”å›ç»“æœå’Œtabåˆ‡æ¢
        def execute_stage1_with_tab(user_input):
            result = execute_stage_1_template_selection(user_input)
            return result, gr.Tabs(selected="1ï¸âƒ£ æ¨¡æ¿é€‰æ‹©")
        
        def execute_stage2_with_tab(user_input):
            result = execute_stage_2_context_assembly(user_input)
            return result, gr.Tabs(selected="2ï¸âƒ£ ä¸Šä¸‹æ–‡è£…é…")
        
        def execute_stage3_with_tab(user_input):
            result = execute_stage_3_llm_inference(user_input)
            return result, gr.Tabs(selected="3ï¸âƒ£ LLMæ¨ç†")
        
        def execute_stage4_with_tab(user_input):
            result = execute_stage_4_context_update(user_input)
            return result, gr.Tabs(selected="4ï¸âƒ£ ä¸Šä¸‹æ–‡æ›´æ–°")
        
        def run_complete_with_tab(user_input):
            summary, s1, s2, s3, s4 = run_complete_flow(user_input)
            return summary, s1, s2, s3, s4, gr.Tabs(selected="ğŸ§© æ€»ç»“")
        
        def get_status_with_tab():
            result = get_system_status()
            return result, gr.Tabs(selected="ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        
        def view_history_with_tab():
            result = view_conversation_history()
            return result, gr.Tabs(selected="ğŸ“œ å¯¹è¯å†å²")
        
        def clear_history_with_tab():
            result = clear_conversation_history()
            return result, gr.Tabs(selected="ğŸ“œ å¯¹è¯å†å²")
        
        stage1_btn.click(
            fn=execute_stage1_with_tab,
            inputs=[user_input],
            outputs=[output_stage1, result_tabs]
        )
        
        stage2_btn.click(
            fn=execute_stage2_with_tab,
            inputs=[user_input],
            outputs=[output_stage2, result_tabs]
        )
        
        stage3_btn.click(
            fn=execute_stage3_with_tab,
            inputs=[user_input],
            outputs=[output_stage3, result_tabs]
        )
        
        stage4_btn.click(
            fn=execute_stage4_with_tab,
            inputs=[user_input],
            outputs=[output_stage4, result_tabs]
        )
        
        complete_btn.click(
            fn=run_complete_with_tab,
            inputs=[user_input],
            outputs=[output_summary, output_stage1, output_stage2, output_stage3, output_stage4, result_tabs]
        )
        
        clear_btn.click(
            fn=clear_history_with_tab,
            inputs=[],
            outputs=[history_output, result_tabs]
        )
        
        status_btn.click(
            fn=get_status_with_tab,
            inputs=[],
            outputs=[status_output, result_tabs]
        )
        
        history_btn.click(
            fn=view_history_with_tab,
            inputs=[],
            outputs=[history_output, result_tabs]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_smart_agent_demo()
    demo.launch(server_name="0.0.0.0", server_port=7862, share=False)
