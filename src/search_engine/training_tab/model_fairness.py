#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å…¬å¹³æ€§åˆ†ææ¨¡å— - è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç¾¤ä½“ä¸Šçš„æ€§èƒ½å·®å¼‚
ç”¨äºæ•™å­¦ï¼šåˆæ­¥äº†è§£å¦‚ä½•è¯„ä¼°æ¨¡å‹çš„å…¬å¹³æ€§
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix
)
import warnings
warnings.filterwarnings('ignore')


class ModelFairnessAnalyzer:
    """æ¨¡å‹å…¬å¹³æ€§åˆ†æå™¨ - è¯„ä¼°ä¸åŒç¾¤ä½“çš„æ€§èƒ½å·®å¼‚"""
    
    def __init__(self):
        self.group_metrics = {}
    
    def define_groups(
        self,
        ctr_data: List[Dict[str, Any]],
        group_by: str = 'query'
    ) -> Dict[str, List[int]]:
        """
        å®šä¹‰ä¸åŒçš„ç¾¤ä½“
        
        Args:
            ctr_data: CTRæ•°æ®åˆ—è¡¨
            group_by: åˆ†ç»„ä¾æ®ï¼Œå¯é€‰: 'query', 'doc_id', 'position', 'custom'
        
        Returns:
            ç¾¤ä½“å­—å…¸ï¼Œé”®æ˜¯ç¾¤ä½“åç§°ï¼Œå€¼æ˜¯è¯¥ç¾¤ä½“çš„æ•°æ®ç´¢å¼•åˆ—è¡¨
        """
        df = pd.DataFrame(ctr_data)
        groups = {}
        
        if group_by == 'query':
            # æŒ‰æŸ¥è¯¢åˆ†ç»„
            for query in df['query'].unique():
                indices = df[df['query'] == query].index.tolist()
                if len(indices) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰ä½œä¸ºä¸€ä¸ªç¾¤ä½“
                    groups[f"æŸ¥è¯¢: {query[:20]}"] = indices
        
        elif group_by == 'doc_id':
            # æŒ‰æ–‡æ¡£IDåˆ†ç»„
            for doc_id in df['doc_id'].unique():
                indices = df[df['doc_id'] == doc_id].index.tolist()
                if len(indices) >= 3:
                    groups[f"æ–‡æ¡£: {doc_id[:20]}"] = indices
        
        elif group_by == 'position':
            # æŒ‰ä½ç½®åˆ†ç»„
            for position in sorted(df['position'].unique()):
                indices = df[df['position'] == position].index.tolist()
                if len(indices) >= 3:
                    groups[f"ä½ç½®: {position}"] = indices
        
        elif group_by == 'position_range':
            # æŒ‰ä½ç½®èŒƒå›´åˆ†ç»„
            df['position_range'] = pd.cut(
                df['position'],
                bins=[0, 3, 6, 10, float('inf')],
                labels=['é¡¶éƒ¨(1-3)', 'ä¸­éƒ¨(4-6)', 'ä¸‹éƒ¨(7-10)', 'åº•éƒ¨(>10)']
            )
            for range_name in df['position_range'].unique():
                if pd.notna(range_name):
                    indices = df[df['position_range'] == range_name].index.tolist()
                    if len(indices) >= 3:
                        groups[str(range_name)] = indices
        
        elif group_by == 'score_range':
            # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°èŒƒå›´åˆ†ç»„
            score_quantiles = df['score'].quantile([0, 0.25, 0.5, 0.75, 1.0])
            df['score_range'] = pd.cut(
                df['score'],
                bins=score_quantiles.values,
                labels=['ä½åˆ†', 'ä¸­ä½åˆ†', 'ä¸­é«˜åˆ†', 'é«˜åˆ†'],
                include_lowest=True
            )
            for range_name in df['score_range'].unique():
                if pd.notna(range_name):
                    indices = df[df['score_range'] == range_name].index.tolist()
                    if len(indices) >= 3:
                        groups[str(range_name)] = indices
        
        return groups
    
    def evaluate_group_performance(
        self,
        model_instance,
        ctr_data: List[Dict[str, Any]],
        group_indices: List[int],
        model_instance_extract_features
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°ç‰¹å®šç¾¤ä½“çš„æ¨¡å‹æ€§èƒ½
        
        Args:
            model_instance: è®­ç»ƒå¥½çš„æ¨¡å‹
            ctr_data: å®Œæ•´çš„CTRæ•°æ®
            group_indices: è¯¥ç¾¤ä½“çš„æ•°æ®ç´¢å¼•
            model_instance_extract_features: ç‰¹å¾æå–å‡½æ•°
        
        Returns:
            è¯¥ç¾¤ä½“çš„æ€§èƒ½æŒ‡æ ‡
        """
        try:
            # æå–è¯¥ç¾¤ä½“çš„æ•°æ®
            group_data = [ctr_data[i] for i in group_indices if i < len(ctr_data)]
            
            if len(group_data) < 2:
                return {'error': 'ç¾¤ä½“æ•°æ®é‡ä¸è¶³'}
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            features, labels = model_instance_extract_features(group_data)
            
            if len(features) == 0 or len(labels) == 0:
                return {'error': 'ç‰¹å¾æå–å¤±è´¥'}
            
            # æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆä½¿ç”¨æ¨¡å‹çš„scalerï¼‰
            if hasattr(model_instance, 'scaler') and model_instance.scaler:
                features_scaled = model_instance.scaler.transform(features)
            else:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                features_scaled = scaler.fit_transform(features)
            
            # é¢„æµ‹
            y_pred = model_instance.predict(features_scaled)
            
            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            accuracy = accuracy_score(labels, y_pred)
            precision = precision_score(labels, y_pred, zero_division=0)
            recall = recall_score(labels, y_pred, zero_division=0)
            f1 = f1_score(labels, y_pred, zero_division=0)
            
            # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰predict_probaï¼‰
            auc = None
            if hasattr(model_instance, 'predict_proba'):
                try:
                    y_pred_proba = model_instance.predict_proba(features_scaled)[:, 1]
                    if len(np.unique(labels)) == 2:
                        auc = roc_auc_score(labels, y_pred_proba)
                except:
                    pass
            
            # è®¡ç®—æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(labels, y_pred)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
            
            # è®¡ç®—ç‚¹å‡»ç‡
            click_rate = float(np.mean(labels)) if len(labels) > 0 else 0.0
            
            return {
                'n_samples': len(group_data),
                'click_rate': click_rate,
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'auc': float(auc) if auc is not None else None,
                'confusion_matrix': {
                    'tn': int(tn),
                    'fp': int(fp),
                    'fn': int(fn),
                    'tp': int(tp)
                }
            }
            
        except Exception as e:
            return {'error': f'è¯„ä¼°å¤±è´¥: {str(e)}'}
    
    def analyze_fairness(
        self,
        model_instance,
        ctr_data: List[Dict[str, Any]],
        group_by: str = 'position_range',
        model_instance_extract_features = None
    ) -> Dict[str, Any]:
        """
        åˆ†ææ¨¡å‹åœ¨ä¸åŒç¾¤ä½“ä¸Šçš„å…¬å¹³æ€§
        
        Args:
            model_instance: è®­ç»ƒå¥½çš„æ¨¡å‹
            ctr_data: CTRæ•°æ®åˆ—è¡¨
            group_by: åˆ†ç»„ä¾æ®
            model_instance_extract_features: ç‰¹å¾æå–å‡½æ•°ï¼ˆå¦‚æœæ¨¡å‹æœ‰extract_featuresæ–¹æ³•ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨ï¼‰
        
        Returns:
            å…¬å¹³æ€§åˆ†æç»“æœ
        """
        try:
            # è·å–ç‰¹å¾æå–å‡½æ•°
            if model_instance_extract_features is None:
                if hasattr(model_instance, 'extract_features'):
                    def extract_fn(data):
                        return model_instance.extract_features(data)
                else:
                    return {'error': 'éœ€è¦æä¾›ç‰¹å¾æå–å‡½æ•°'}
            else:
                extract_fn = model_instance_extract_features
            
            # å®šä¹‰ç¾¤ä½“
            groups = self.define_groups(ctr_data, group_by)
            
            if len(groups) < 2:
                return {
                    'error': f'æ— æ³•å®šä¹‰è¶³å¤Ÿçš„ç¾¤ä½“ï¼ˆè‡³å°‘éœ€è¦2ä¸ªï¼‰ï¼Œå½“å‰åªæœ‰{len(groups)}ä¸ª',
                    'groups': list(groups.keys())
                }
            
            # è¯„ä¼°æ¯ä¸ªç¾¤ä½“çš„æ€§èƒ½
            group_results = {}
            for group_name, group_indices in groups.items():
                result = self.evaluate_group_performance(
                    model_instance,
                    ctr_data,
                    group_indices,
                    extract_fn
                )
                if 'error' not in result:
                    group_results[group_name] = result
            
            if len(group_results) < 2:
                return {
                    'error': 'æˆåŠŸè¯„ä¼°çš„ç¾¤ä½“æ•°é‡ä¸è¶³',
                    'groups': list(groups.keys())
                }
            
            # è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡
            fairness_metrics = self._calculate_fairness_metrics(group_results)
            
            return {
                'groups': group_results,
                'fairness_metrics': fairness_metrics,
                'group_by': group_by,
                'n_groups': len(group_results)
            }
            
        except Exception as e:
            return {'error': f'å…¬å¹³æ€§åˆ†æå¤±è´¥: {str(e)}'}
    
    def _calculate_fairness_metrics(self, group_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡
        
        Args:
            group_results: å„ç¾¤ä½“çš„æ€§èƒ½ç»“æœ
        
        Returns:
            å…¬å¹³æ€§æŒ‡æ ‡å­—å…¸
        """
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        fairness = {}
        
        for metric in metrics:
            values = [r[metric] for r in group_results.values() if metric in r]
            if values:
                fairness[metric] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'range': float(np.max(values) - np.min(values)),
                    'cv': float(np.std(values) / np.mean(values)) if np.mean(values) > 0 else 0.0  # å˜å¼‚ç³»æ•°
                }
        
        # AUCå…¬å¹³æ€§ï¼ˆå¦‚æœæœ‰ï¼‰
        auc_values = [r['auc'] for r in group_results.values() if r.get('auc') is not None]
        if auc_values:
            fairness['auc'] = {
                'mean': float(np.mean(auc_values)),
                'std': float(np.std(auc_values)),
                'min': float(np.min(auc_values)),
                'max': float(np.max(auc_values)),
                'range': float(np.max(auc_values) - np.min(auc_values)),
                'cv': float(np.std(auc_values) / np.mean(auc_values)) if np.mean(auc_values) > 0 else 0.0
            }
        
        # è®¡ç®—æ€§èƒ½å·®å¼‚ï¼ˆæœ€å¤§å·®å¼‚ï¼‰
        if 'accuracy' in fairness:
            fairness['max_accuracy_gap'] = fairness['accuracy']['range']
        if 'f1' in fairness:
            fairness['max_f1_gap'] = fairness['f1']['range']
        
        # è®¡ç®—ç‚¹å‡»ç‡å·®å¼‚
        click_rates = [r['click_rate'] for r in group_results.values() if 'click_rate' in r]
        if click_rates:
            fairness['click_rate'] = {
                'mean': float(np.mean(click_rates)),
                'std': float(np.std(click_rates)),
                'min': float(np.min(click_rates)),
                'max': float(np.max(click_rates)),
                'range': float(np.max(click_rates) - np.min(click_rates))
            }
        
        return fairness
    
    def generate_fairness_report(self, analysis_result: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå…¬å¹³æ€§åˆ†ææŠ¥å‘Šï¼ˆHTMLæ ¼å¼ï¼‰
        
        Args:
            analysis_result: å…¬å¹³æ€§åˆ†æç»“æœ
        
        Returns:
            HTMLæ ¼å¼çš„æŠ¥å‘Š
        """
        if 'error' in analysis_result:
            return f"<h4>âŒ å…¬å¹³æ€§åˆ†æå¤±è´¥</h4><p>{analysis_result['error']}</p>"
        
        report = "<h4>ğŸ“Š æ¨¡å‹å…¬å¹³æ€§åˆ†ææŠ¥å‘Š</h4>"
        report += f"<p><strong>åˆ†ç»„ä¾æ®:</strong> {analysis_result.get('group_by', 'N/A')}</p>"
        report += f"<p><strong>åˆ†æç¾¤ä½“æ•°:</strong> {analysis_result.get('n_groups', 0)}</p>"
        
        # å„ç¾¤ä½“æ€§èƒ½
        report += "<h5>å„ç¾¤ä½“æ€§èƒ½</h5>"
        report += "<table border='1' style='border-collapse: collapse; width: 100%; margin: 10px 0;'>"
        report += "<thead><tr style='background-color: #e9ecef;'>"
        report += "<th style='padding: 8px;'>ç¾¤ä½“</th>"
        report += "<th style='padding: 8px;'>æ ·æœ¬æ•°</th>"
        report += "<th style='padding: 8px;'>ç‚¹å‡»ç‡</th>"
        report += "<th style='padding: 8px;'>å‡†ç¡®ç‡</th>"
        report += "<th style='padding: 8px;'>ç²¾ç¡®ç‡</th>"
        report += "<th style='padding: 8px;'>å¬å›ç‡</th>"
        report += "<th style='padding: 8px;'>F1</th>"
        report += "<th style='padding: 8px;'>AUC</th>"
        report += "</tr></thead><tbody>"
        
        for group_name, metrics in analysis_result.get('groups', {}).items():
            report += f"<tr><td style='padding: 8px;'>{group_name}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('n_samples', 0)}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('click_rate', 0):.3f}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('accuracy', 0):.3f}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('precision', 0):.3f}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('recall', 0):.3f}</td>"
            report += f"<td style='padding: 8px; text-align: center;'>{metrics.get('f1', 0):.3f}</td>"
            auc = metrics.get('auc', 'N/A')
            if isinstance(auc, (int, float)):
                report += f"<td style='padding: 8px; text-align: center;'>{auc:.3f}</td>"
            else:
                report += f"<td style='padding: 8px; text-align: center;'>{auc}</td>"
            report += "</tr>"
        
        report += "</tbody></table>"
        
        # å…¬å¹³æ€§æŒ‡æ ‡
        fairness = analysis_result.get('fairness_metrics', {})
        if fairness:
            report += "<h5>å…¬å¹³æ€§æŒ‡æ ‡</h5>"
            report += "<div style='margin: 10px 0;'>"
            
            for metric_name, metric_stats in fairness.items():
                if isinstance(metric_stats, dict) and 'mean' in metric_stats:
                    report += f"<div style='margin-bottom: 15px; padding: 10px; background-color: #f8f9fa; border-radius: 5px;'>"
                    report += f"<strong>{metric_name}</strong>"
                    report += "<ul style='margin: 5px 0;'>"
                    report += f"<li>å¹³å‡å€¼: {metric_stats['mean']:.3f}</li>"
                    report += f"<li>æ ‡å‡†å·®: {metric_stats['std']:.3f}</li>"
                    report += f"<li>èŒƒå›´: [{metric_stats['min']:.3f}, {metric_stats['max']:.3f}]</li>"
                    report += f"<li>æœ€å¤§å·®å¼‚: {metric_stats.get('range', 0):.3f}</li>"
                    if 'cv' in metric_stats:
                        report += f"<li>å˜å¼‚ç³»æ•°: {metric_stats['cv']:.3f}</li>"
                    report += "</ul></div>"
            
            report += "</div>"
        
        # å…¬å¹³æ€§ç»“è®º
        report += "<h5>å…¬å¹³æ€§è¯„ä¼°</h5><div>"
        
        if 'accuracy' in fairness:
            acc_range = fairness['accuracy'].get('range', 0)
            if acc_range < 0.05:
                report += "<p style='color: green;'>âœ… <strong>å‡†ç¡®ç‡å·®å¼‚è¾ƒå°</strong>ï¼Œæ¨¡å‹åœ¨ä¸åŒç¾¤ä½“ä¸Šè¡¨ç°ç›¸å¯¹å…¬å¹³</p>"
            elif acc_range < 0.15:
                report += "<p style='color: orange;'>âš ï¸ <strong>å‡†ç¡®ç‡å­˜åœ¨ä¸€å®šå·®å¼‚</strong>ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æåŸå› </p>"
            else:
                report += "<p style='color: red;'>âŒ <strong>å‡†ç¡®ç‡å·®å¼‚è¾ƒå¤§</strong>ï¼Œæ¨¡å‹å¯èƒ½å­˜åœ¨å…¬å¹³æ€§é—®é¢˜</p>"
        
        if 'f1' in fairness:
            f1_range = fairness['f1'].get('range', 0)
            if f1_range < 0.1:
                report += "<p style='color: green;'>âœ… <strong>F1åˆ†æ•°å·®å¼‚è¾ƒå°</strong></p>"
            elif f1_range < 0.2:
                report += "<p style='color: orange;'>âš ï¸ <strong>F1åˆ†æ•°å­˜åœ¨ä¸€å®šå·®å¼‚</strong></p>"
            else:
                report += "<p style='color: red;'>âŒ <strong>F1åˆ†æ•°å·®å¼‚è¾ƒå¤§</strong></p>"
        
        report += "</div>"
        
        return report

