#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AutoMLæ¨¡å— - è‡ªåŠ¨æ¨¡å‹æœç´¢å’Œè¶…å‚æ•°ä¼˜åŒ–
ç”¨äºæ•™å­¦ï¼šä½¿ç”¨AutoMLå·¥å…·è¿›è¡Œæ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°ä¼˜åŒ–
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥TPOT
try:
    from tpot import TPOTClassifier
    TPOT_AVAILABLE = True
except ImportError:
    TPOT_AVAILABLE = False
    print("âš ï¸ TPOTæœªå®‰è£…ï¼ŒAutoMLåŠŸèƒ½å°†å—é™ã€‚å®‰è£…: pip install tpot")

# å°è¯•å¯¼å…¥Optunaï¼ˆç”¨äºè¶…å‚æ•°ä¼˜åŒ–ï¼‰
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optunaæœªå®‰è£…ï¼Œè¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½å°†å—é™ã€‚å®‰è£…: pip install optuna")


class AutoMLOptimizer:
    """AutoMLä¼˜åŒ–å™¨ - è‡ªåŠ¨æ¨¡å‹æœç´¢å’Œè¶…å‚æ•°ä¼˜åŒ–"""
    
    def __init__(self):
        self.tpot_pipeline = None
        self.best_model = None
        self.optimization_history = []
    
    def optimize_with_tpot(
        self,
        X: np.ndarray,
        y: np.ndarray,
        generations: int = 5,
        population_size: int = 20,
        cv: int = 3,
        scoring: str = 'roc_auc',
        max_time_mins: Optional[int] = None,
        verbosity: int = 2
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨TPOTè¿›è¡Œè‡ªåŠ¨æ¨¡å‹æœç´¢å’Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾å‘é‡
            generations: è¿›åŒ–ä»£æ•°
            population_size: ç§ç¾¤å¤§å°
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            scoring: è¯„ä¼°æŒ‡æ ‡
            max_time_mins: æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            verbosity: è¯¦ç»†ç¨‹åº¦
        
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        if not TPOT_AVAILABLE:
            return {
                'error': 'TPOTæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tpot',
                'available': False
            }
        
        try:
            # æ£€æŸ¥æ•°æ®
            if len(X) < cv * 2:
                return {
                    'error': f'æ•°æ®é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{cv * 2}æ¡è®°å½•ï¼Œå½“å‰åªæœ‰{len(X)}æ¡',
                    'available': True
                }
            
            # åˆ›å»ºTPOTåˆ†ç±»å™¨
            tpot = TPOTClassifier(
                generations=generations,
                population_size=population_size,
                cv=cv,
                scoring=scoring,
                verbosity=verbosity,
                random_state=42,
                n_jobs=1,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
                max_time_mins=max_time_mins
            )
            
            # æ‰§è¡Œä¼˜åŒ–
            print(f"ğŸš€ å¼€å§‹TPOTä¼˜åŒ–ï¼ˆä»£æ•°: {generations}, ç§ç¾¤: {population_size}ï¼‰...")
            tpot.fit(X, y)
            
            # è·å–æœ€ä½³æ¨¡å‹
            self.tpot_pipeline = tpot.fitted_pipeline_
            self.best_model = tpot
            
            # è¯„ä¼°æœ€ä½³æ¨¡å‹
            best_score = tpot.score(X, y)
            
            # è·å–æœ€ä½³ç®¡é“ä»£ç 
            pipeline_code = tpot.export()
            
            return {
                'success': True,
                'best_score': float(best_score),
                'best_pipeline': str(self.tpot_pipeline),
                'pipeline_code': pipeline_code,
                'generations': generations,
                'population_size': population_size,
                'scoring': scoring
            }
            
        except Exception as e:
            return {
                'error': f'TPOTä¼˜åŒ–å¤±è´¥: {str(e)}',
                'available': True
            }
    
    def optimize_hyperparameters_with_optuna(
        self,
        model_class,
        X: np.ndarray,
        y: np.ndarray,
        param_space: Dict[str, Any],
        n_trials: int = 20,
        cv: int = 3,
        scoring: str = 'roc_auc'
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            model_class: æ¨¡å‹ç±»ï¼ˆå¦‚LogisticRegressionï¼‰
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾å‘é‡
            param_space: å‚æ•°ç©ºé—´å®šä¹‰
            n_trials: è¯•éªŒæ¬¡æ•°
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            scoring: è¯„ä¼°æŒ‡æ ‡
        
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        if not OPTUNA_AVAILABLE:
            return {
                'error': 'Optunaæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install optuna',
                'available': False
            }
        
        try:
            from sklearn.model_selection import cross_val_score
            from sklearn.preprocessing import StandardScaler
            
            # æ ‡å‡†åŒ–æ•°æ®
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            def objective(trial):
                # ä»å‚æ•°ç©ºé—´ä¸­é‡‡æ ·
                params = {}
                for param_name, param_config in param_space.items():
                    if param_config['type'] == 'float':
                        params[param_name] = trial.suggest_float(
                            param_name,
                            param_config['low'],
                            param_config['high'],
                            log=param_config.get('log', False)
                        )
                    elif param_config['type'] == 'int':
                        params[param_name] = trial.suggest_int(
                            param_name,
                            param_config['low'],
                            param_config['high'],
                            log=param_config.get('log', False)
                        )
                    elif param_config['type'] == 'categorical':
                        params[param_name] = trial.suggest_categorical(
                            param_name,
                            param_config['choices']
                        )
                
                # åˆ›å»ºæ¨¡å‹
                model = model_class(**params, random_state=42)
                
                # äº¤å‰éªŒè¯
                scores = cross_val_score(
                    model, X_scaled, y,
                    cv=cv,
                    scoring=scoring,
                    n_jobs=1
                )
                
                return scores.mean()
            
            # åˆ›å»ºç ”ç©¶å¹¶ä¼˜åŒ–
            study = optuna.create_study(direction='maximize', study_name='hyperparameter_optimization')
            study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
            
            # è·å–æœ€ä½³å‚æ•°
            best_params = study.best_params
            best_score = study.best_value
            
            # è®­ç»ƒæœ€ä½³æ¨¡å‹
            best_model = model_class(**best_params, random_state=42)
            best_model.fit(X_scaled, y)
            
            self.best_model = best_model
            
            return {
                'success': True,
                'best_params': best_params,
                'best_score': float(best_score),
                'n_trials': n_trials,
                'study_summary': str(study.trials_dataframe())
            }
            
        except Exception as e:
            return {
                'error': f'Optunaä¼˜åŒ–å¤±è´¥: {str(e)}',
                'available': True
            }
    
    def simple_grid_search(
        self,
        model_class,
        X: np.ndarray,
        y: np.ndarray,
        param_grid: Dict[str, List[Any]],
        cv: int = 3,
        scoring: str = 'roc_auc'
    ) -> Dict[str, Any]:
        """
        ç®€å•çš„ç½‘æ ¼æœç´¢ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
        
        Args:
            model_class: æ¨¡å‹ç±»
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾å‘é‡
            param_grid: å‚æ•°ç½‘æ ¼
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            scoring: è¯„ä¼°æŒ‡æ ‡
        
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        try:
            from sklearn.model_selection import GridSearchCV
            from sklearn.preprocessing import StandardScaler
            
            # æ ‡å‡†åŒ–æ•°æ®
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # åˆ›å»ºæ¨¡å‹
            base_model = model_class(random_state=42)
            
            # ç½‘æ ¼æœç´¢
            grid_search = GridSearchCV(
                base_model,
                param_grid,
                cv=cv,
                scoring=scoring,
                n_jobs=1,
                verbose=1
            )
            
            grid_search.fit(X_scaled, y)
            
            self.best_model = grid_search.best_estimator_
            
            return {
                'success': True,
                'best_params': grid_search.best_params_,
                'best_score': float(grid_search.best_score_),
                'cv_results': {
                    'mean_test_score': grid_search.cv_results_['mean_test_score'].tolist(),
                    'std_test_score': grid_search.cv_results_['std_test_score'].tolist(),
                    'params': grid_search.cv_results_['params']
                }
            }
            
        except Exception as e:
            return {
                'error': f'ç½‘æ ¼æœç´¢å¤±è´¥: {str(e)}',
                'available': True
            }
    
    def get_best_model(self):
        """è·å–ä¼˜åŒ–åçš„æœ€ä½³æ¨¡å‹"""
        return self.best_model
    
    def predict_with_best_model(self, X: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if self.best_model is None:
            raise ValueError("å°šæœªè¿›è¡Œä¼˜åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ä¼˜åŒ–æ–¹æ³•")
        
        from sklearn.preprocessing import StandardScaler
        
        # å¦‚æœæ•°æ®éœ€è¦æ ‡å‡†åŒ–
        if hasattr(self.best_model, 'scaler'):
            X_scaled = self.best_model.scaler.transform(X)
        else:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        
        return self.best_model.predict(X_scaled)
    
    def predict_proba_with_best_model(self, X: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ¦‚ç‡é¢„æµ‹"""
        if self.best_model is None:
            raise ValueError("å°šæœªè¿›è¡Œä¼˜åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ä¼˜åŒ–æ–¹æ³•")
        
        if not hasattr(self.best_model, 'predict_proba'):
            raise ValueError("æœ€ä½³æ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹")
        
        from sklearn.preprocessing import StandardScaler
        
        # å¦‚æœæ•°æ®éœ€è¦æ ‡å‡†åŒ–
        if hasattr(self.best_model, 'scaler'):
            X_scaled = self.best_model.scaler.transform(X)
        else:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        
        return self.best_model.predict_proba(X_scaled)

