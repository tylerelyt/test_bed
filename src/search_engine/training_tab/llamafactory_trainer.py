"""
LLaMA-Factory è®­ç»ƒæœåŠ¡
å‚è€ƒ LLaMA-Factory WebUI çš„å®ç°ï¼Œä½¿ç”¨ subprocess å¯åŠ¨ç‹¬ç«‹è®­ç»ƒè¿›ç¨‹
è¿™æ ·å¯ä»¥é¿å… "signal only works in main thread" çš„é—®é¢˜
"""
import os
import json
import subprocess
import tempfile
import shutil
from typing import Dict, Any, Optional
from datetime import datetime


class LLaMAFactoryTrainer:
    """LLaMA-Factory è®­ç»ƒæœåŠ¡ç±»ï¼ˆä½¿ç”¨ subprocess å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹ï¼‰"""
    
    def __init__(self):
        self.current_process: Optional[subprocess.Popen] = None
        self.training_status = {
            'running': False,
            'stage': None,
            'output_dir': None
        }
        self.config_file = None  # ä¸´æ—¶é…ç½®æ–‡ä»¶è·¯å¾„
    
    def _build_train_args(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè®­ç»ƒå‚æ•°ï¼Œè½¬æ¢ä¸º LLaMA-Factory éœ€è¦çš„æ ¼å¼
        
        å‚è€ƒ LLaMA-Factory WebUI çš„ runner._parse_train_args æ–¹æ³•
        """
        args = {}
        
        # åŸºç¡€é…ç½®
        args['stage'] = config.get('stage', 'sft')  # pt, sft, dpo, etc.
        args['model_name_or_path'] = config.get('model_name_or_path', '')
        
        # SFT/DPOé˜¶æ®µï¼šä»ä¹‹å‰çš„checkpointç»§ç»­è®­ç»ƒ
        adapter_path = config.get('adapter_name_or_path', '')
        if adapter_path:
            args['adapter_name_or_path'] = adapter_path
        
        args['dataset'] = config.get('dataset', '')
        args['dataset_dir'] = config.get('dataset_dir', 'data/llmops')
        args['template'] = config.get('template', 'default')
        args['finetuning_type'] = config.get('finetuning_type', 'lora')
        
        # è®­ç»ƒå‚æ•°
        args['output_dir'] = config.get('output_dir', '')
        args['overwrite_output_dir'] = config.get('overwrite_output_dir', True)
        args['do_train'] = True
        args['num_train_epochs'] = config.get('num_train_epochs', 3.0)
        args['learning_rate'] = config.get('learning_rate', 5e-5)
        args['per_device_train_batch_size'] = config.get('per_device_train_batch_size', 2)
        args['gradient_accumulation_steps'] = config.get('gradient_accumulation_steps', 8)
        args['cutoff_len'] = config.get('cutoff_len', 512)  # æ”¹ä¸º512ï¼Œé¿å…å°æ•°æ®é›†è¢«è¿‡æ»¤
        args['max_grad_norm'] = config.get('max_grad_norm', 1.0)
        args['lr_scheduler_type'] = config.get('lr_scheduler_type', 'cosine')
        args['warmup_steps'] = config.get('warmup_steps', 0)
        args['logging_steps'] = config.get('logging_steps', 5)
        args['save_steps'] = config.get('save_steps', 100)
        args['save_strategy'] = 'steps'
        args['logging_strategy'] = 'steps'
        
        # è®¡ç®—ç±»å‹ (MacOS MPS ä¸æ”¯æŒ bf16/fp16ï¼Œé»˜è®¤ä½¿ç”¨ fp32)
        compute_type = config.get('compute_type', 'fp32')
        if compute_type == 'bf16':
            args['bf16'] = True
            args['fp16'] = False
        elif compute_type == 'fp16':
            args['fp16'] = True
            args['bf16'] = False
        else:  # fp32 æˆ–å…¶ä»–
            args['fp16'] = False
            args['bf16'] = False
        
        # LoRA é…ç½®
        if args['finetuning_type'] == 'lora':
            args['lora_rank'] = config.get('lora_rank', 8)
            args['lora_alpha'] = config.get('lora_alpha', 16)
            args['lora_dropout'] = config.get('lora_dropout', 0.05)
            args['lora_target'] = config.get('lora_target', 'all')
        
        # DPO é…ç½®
        if args['stage'] == 'dpo':
            args['pref_beta'] = config.get('pref_beta', 0.1)
            args['pref_ftx'] = config.get('pref_ftx', 0.0)
            args['pref_loss'] = config.get('pref_loss', 'sigmoid')
        
        # å…¶ä»–é…ç½®
        args['max_samples'] = config.get('max_samples', 100000)
        args['val_size'] = config.get('val_size', 0.0)
        args['plot_loss'] = True
        args['trust_remote_code'] = True
        args['overwrite_cache'] = True  # æ€»æ˜¯é‡æ–°å¤„ç†æ•°æ®ï¼Œé¿å…ç¼“å­˜é—®é¢˜
        args['preprocessing_num_workers'] = 1  # å•è¿›ç¨‹å¤„ç†ï¼Œé¿å…å¹¶å‘é—®é¢˜
        
        if args['val_size'] > 1e-6:
            args['eval_strategy'] = 'steps'
            args['eval_steps'] = args['save_steps']
            args['per_device_eval_batch_size'] = args['per_device_train_batch_size']
        
        return args
    
    def _save_config_to_file(self, args: Dict[str, Any]) -> str:
        """å°†é…ç½®ä¿å­˜ä¸ºä¸´æ—¶ YAML æ–‡ä»¶
        
        å‚è€ƒ LLaMA-Factory WebUI çš„ save_cmd æ–¹æ³•
        """
        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        config_dir = tempfile.mkdtemp(prefix='llamafactory_')
        config_file = os.path.join(config_dir, 'train_config.yaml')
        
        # è½¬æ¢å‚æ•°æ ¼å¼ï¼ˆå°† Python ç±»å‹è½¬ä¸º YAML å‹å¥½æ ¼å¼ï¼‰
        yaml_args = {}
        for key, value in args.items():
            if isinstance(value, bool):
                yaml_args[key] = value
            elif isinstance(value, (int, float)):
                yaml_args[key] = value
            elif isinstance(value, str):
                yaml_args[key] = value
            elif value is None:
                continue  # è·³è¿‡ None å€¼
            else:
                yaml_args[key] = str(value)
        
        # å†™å…¥ YAML æ–‡ä»¶
        import yaml
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_args, f, default_flow_style=False, allow_unicode=True)
        
        self.config_file = config_file
        return config_file
    
    def start_training(self, config: Dict[str, Any], log_callback: Optional[callable] = None) -> bool:
        """å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼ˆä½¿ç”¨ subprocess å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹ï¼‰
        
        å‚è€ƒ LLaMA-Factory WebUI çš„ _launch æ–¹æ³•
        
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°ï¼ˆæš‚ä¸ä½¿ç”¨ï¼Œä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸå¯åŠ¨è®­ç»ƒè¿›ç¨‹
        """
        if self.training_status['running']:
            return False
        
        # æŸ¥æ‰¾ llamafactory-cli å‘½ä»¤
        # ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿ PATHï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™å°è¯• Python ç¯å¢ƒçš„ bin ç›®å½•
        llamafactory_cmd = shutil.which('llamafactory-cli')
        if not llamafactory_cmd:
            # å°è¯•ä» Python ç¯å¢ƒçš„ bin ç›®å½•æŸ¥æ‰¾
            import sys
            python_bin_dir = os.path.dirname(sys.executable)
            llamafactory_cmd = os.path.join(python_bin_dir, 'llamafactory-cli')
            if not os.path.exists(llamafactory_cmd):
                if log_callback:
                    log_callback("âŒ llamafactory-cli å‘½ä»¤ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£… LLaMA-Factory")
                print(f"llamafactory-cli not found in PATH or {python_bin_dir}")
                return False
        
        # æ„å»ºè®­ç»ƒå‚æ•°
        train_args = self._build_train_args(config)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = train_args.get('output_dir')
        if not output_dir:
            if log_callback:
                log_callback("âŒ è¾“å‡ºç›®å½•æœªæŒ‡å®š")
            return False
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®åˆ°ä¸´æ—¶æ–‡ä»¶
        try:
            config_file = self._save_config_to_file(train_args)
            print(f"ğŸ“„ é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {config_file}")
        except Exception as e:
            if log_callback:
                log_callback(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            print(f"é…ç½®æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå‚è€ƒ LLaMA-Factory WebUIï¼‰
        env = os.environ.copy()
        env['LLAMABOARD_ENABLED'] = '1'
        env['LLAMABOARD_WORKDIR'] = output_dir
        
        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹ï¼ˆä½¿ç”¨ subprocess.Popenï¼‰
        # å‚è€ƒ LLaMA-Factory WebUI: self.trainer = Popen(["llamafactory-cli", "train", save_cmd(args)], env=env, stderr=PIPE, text=True)
        try:
            print(f"ğŸš€ å¯åŠ¨è®­ç»ƒè¿›ç¨‹: {llamafactory_cmd} train {config_file}")
            # ä¸æ•è· stdout/stderrï¼Œè®©å®ƒç›´æ¥è¾“å‡ºåˆ°ç»ˆç«¯
            # è¿™æ ·ç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°è®­ç»ƒè¿›åº¦
            self.current_process = subprocess.Popen(
                [llamafactory_cmd, 'train', config_file],
                env=env,
                # æ³¨é‡Šæ‰ç®¡é“ï¼Œè®©è¾“å‡ºç›´æ¥æ˜¾ç¤º
                # stdout=subprocess.PIPE,
                # stderr=subprocess.PIPE,
                text=True
            )
            
            # æ›´æ–°çŠ¶æ€
            self.training_status = {
                'running': True,
                'stage': train_args.get('stage'),
                'output_dir': output_dir
            }
            
            return True
            
        except Exception as e:
            if log_callback:
                log_callback(f"âŒ å¯åŠ¨è®­ç»ƒè¿›ç¨‹å¤±è´¥: {str(e)}")
            print(f"è®­ç»ƒå¯åŠ¨å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
            if self.config_file and os.path.exists(self.config_file):
                try:
                    config_dir = os.path.dirname(self.config_file)
                    shutil.rmtree(config_dir)
                except:
                    pass
            return False
    
    def get_training_logs(self, max_lines: int = 100) -> str:
        """è·å–è®­ç»ƒæ—¥å¿—ï¼ˆä» trainer_log.jsonl æ–‡ä»¶ï¼‰
        
        å‚è€ƒ LLaMA-Factory WebUI çš„ get_trainer_info æ–¹æ³•
        """
        output_dir = self.training_status.get('output_dir')
        if not output_dir or not os.path.exists(output_dir):
            return "æš‚æ— è®­ç»ƒæ—¥å¿—"
        
        trainer_log_path = os.path.join(output_dir, "trainer_log.jsonl")
        if not os.path.exists(trainer_log_path):
            return "è®­ç»ƒå°šæœªå¼€å§‹æˆ–æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨"
        
        try:
            logs = []
            with open(trainer_log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line)
                        logs.append(log_entry)
                    except json.JSONDecodeError:
                        continue
            
            if not logs:
                return "æš‚æ— è®­ç»ƒæ—¥å¿—"
            
            # æ ¼å¼åŒ–æ—¥å¿—è¾“å‡ºï¼ˆåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼‰
            latest_log = logs[-1]
            
            current_steps = latest_log.get('current_steps', 0)
            total_steps = latest_log.get('total_steps', 0)
            elapsed = latest_log.get('elapsed_time', '0:00:00')
            remaining = latest_log.get('remaining_time', '0:00:00')
            percentage = latest_log.get('percentage', 0)
            
            log_text = f"""
<div style="padding: 10px; background-color: #f8f9fa; border-radius: 5px;">
    <p><strong>ğŸ“Š è®­ç»ƒè¿›åº¦:</strong> {current_steps}/{total_steps} æ­¥ ({percentage:.1f}%)</p>
    <p><strong>â±ï¸ å·²ç”¨æ—¶é—´:</strong> {elapsed}</p>
    <p><strong>â³ å‰©ä½™æ—¶é—´:</strong> {remaining}</p>
"""
            
            if 'loss' in latest_log:
                log_text += f'    <p><strong>ğŸ“‰ å½“å‰æŸå¤±:</strong> {latest_log["loss"]:.4f}</p>\n'
            
            if 'learning_rate' in latest_log:
                log_text += f'    <p><strong>ğŸ“ˆ å­¦ä¹ ç‡:</strong> {latest_log["learning_rate"]:.2e}</p>\n'
            
            log_text += "</div>"
            
            return log_text
        except Exception as e:
            return f"è¯»å–æ—¥å¿—å¤±è´¥: {str(e)}"
    
    def get_training_progress(self) -> tuple[float, str]:
        """è·å–è®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”å’ŒçŠ¶æ€ä¿¡æ¯
        
        Returns:
            (è¿›åº¦ç™¾åˆ†æ¯” 0-100, çŠ¶æ€æ–‡æœ¬)
        """
        output_dir = self.training_status.get('output_dir')
        if not output_dir or not os.path.exists(output_dir):
            return 0.0, "æœªå¼€å§‹"
        
        trainer_log_path = os.path.join(output_dir, "trainer_log.jsonl")
        if not os.path.exists(trainer_log_path):
            return 0.0, "è®­ç»ƒåˆå§‹åŒ–ä¸­..."
        
        try:
            logs = []
            with open(trainer_log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line)
                        logs.append(log_entry)
                    except json.JSONDecodeError:
                        continue
            
            if not logs:
                return 0.0, "è®­ç»ƒåˆå§‹åŒ–ä¸­..."
            
            # è·å–æœ€æ–°æ—¥å¿—
            latest_log = logs[-1]
            current_steps = latest_log.get('current_steps', 0)
            total_steps = latest_log.get('total_steps', 1)
            
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            progress = (current_steps / total_steps * 100) if total_steps > 0 else 0.0
            
            # æ„å»ºçŠ¶æ€æ–‡æœ¬
            status = f"è®­ç»ƒä¸­: {current_steps}/{total_steps} æ­¥"
            if 'loss' in latest_log:
                status += f" | æŸå¤±: {latest_log['loss']:.4f}"
            
            return progress, status
            
        except Exception as e:
            return 0.0, f"è¯»å–è¿›åº¦å¤±è´¥: {str(e)}"
    
    def check_process_status(self) -> Optional[int]:
        """æ£€æŸ¥è®­ç»ƒè¿›ç¨‹çŠ¶æ€
        
        Returns:
            è¿›ç¨‹è¿”å›ç ï¼ˆNone è¡¨ç¤ºä»åœ¨è¿è¡Œï¼Œ0 è¡¨ç¤ºæˆåŠŸå®Œæˆï¼Œå…¶ä»–å€¼è¡¨ç¤ºé”™è¯¯ï¼‰
        """
        if self.current_process is None:
            return None
        
        # éé˜»å¡æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
        return_code = self.current_process.poll()
        
        if return_code is not None:
            # è¿›ç¨‹å·²ç»“æŸï¼Œè¯»å–è¾“å‡º
            try:
                stdout, stderr = self.current_process.communicate(timeout=1)
                if stderr:
                    print(f"âš ï¸  è®­ç»ƒè¿›ç¨‹stderrè¾“å‡º:\n{stderr}")
                if stdout:
                    print(f"ğŸ“„ è®­ç»ƒè¿›ç¨‹stdoutè¾“å‡º:\n{stdout}")
            except:
                pass
            
            # è¿›ç¨‹å·²ç»“æŸ
            self.training_status['running'] = False
            
            # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
            if self.config_file and os.path.exists(self.config_file):
                try:
                    config_dir = os.path.dirname(self.config_file)
                    shutil.rmtree(config_dir)
                except:
                    pass
                self.config_file = None
        
        return return_code
    
    def stop_training(self) -> bool:
        """åœæ­¢è®­ç»ƒä»»åŠ¡ï¼ˆç»ˆæ­¢è¿›ç¨‹ï¼‰"""
        if not self.training_status['running'] or self.current_process is None:
            return False
        
        try:
            # ä¼˜é›…åœ°ç»ˆæ­¢è¿›ç¨‹
            self.current_process.terminate()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼ˆæœ€å¤š 5 ç§’ï¼‰
            try:
                self.current_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                # å¦‚æœè¿›ç¨‹æ²¡æœ‰å“åº”ï¼Œå¼ºåˆ¶æ€æ­»
                self.current_process.kill()
                self.current_process.wait()
            
            # æ›´æ–°çŠ¶æ€
            self.training_status['running'] = False
            self.current_process = None
            
            # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
            if self.config_file and os.path.exists(self.config_file):
                try:
                    config_dir = os.path.dirname(self.config_file)
                    shutil.rmtree(config_dir)
                except:
                    pass
                self.config_file = None
            
            return True
            
        except Exception as e:
            print(f"åœæ­¢è®­ç»ƒè¿›ç¨‹å¤±è´¥: {str(e)}")
            return False
    
    def get_training_status(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒçŠ¶æ€"""
        return self.training_status.copy()
    
    def is_training(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è®­ç»ƒ"""
        # å¦‚æœæ ‡è®°ä¸ºè¿è¡Œä¸­ï¼Œå†æ£€æŸ¥è¿›ç¨‹æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
        if self.training_status['running']:
            return_code = self.check_process_status()
            return return_code is None
        return False


# å…¨å±€è®­ç»ƒå™¨å®ä¾‹
_trainer_instance = None

def get_trainer() -> LLaMAFactoryTrainer:
    """è·å–è®­ç»ƒå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _trainer_instance
    if _trainer_instance is None:
        _trainer_instance = LLaMAFactoryTrainer()
    return _trainer_instance
