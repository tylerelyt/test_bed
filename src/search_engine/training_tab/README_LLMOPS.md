# LLMOps 闭环系统 - 快速入门

## ⚠️ 重要概念：离线 vs 在线

### 🏗️ 离线训练（大版本开发）
- **用途**：从头构建或大版本更新（v1.0 → v2.0）
- **步骤**：
  1. CPT → **Completion Model** 发布（用于补全）
  2. SFT → **Chat Model** 发布（用于对话）
- **频率**：数月一次
- **成本**：高（需要大量计算资源）

### 🔄 在线优化（持续迭代）⭐ 核心
- **用途**：发布后持续优化（v1.0 → v1.1 → v1.2）
- **步骤**：收集偏好 → DPO 训练 → 部署新版本
- **频率**：每周或每月
- **成本**：低（只训练 LoRA）

**本指南重点介绍在线优化流程！**

---

## 🚀 快速开始

### 1. 启动系统

```bash
cd /Users/tyler/courseware-1/projects/Testbed
python start_system.py
```

访问 Web 界面，进入 **训练 & 实验** → **LLMOps 闭环** 标签页。

---

## 🔄 在线优化流程（推荐）⭐

假设你已经有一个 SFT 训练好的 Chat 模型（v1.0），现在要通过用户反馈持续优化：

### 步骤 1: 收集偏好数据（持续进行）

1. 进入 **A/B 测试与反馈闭环** 标签页
2. 输入问题，生成对比回答
3. 用户选择更好的回答
4. 系统自动记录到 `data/llmops/prefs.jsonl`

**目标**：每周收集 100-200 条真实偏好数据

### 步骤 2: 定期 DPO 训练（每周/月）

1. 进入 **训练配置** → **DPO** 标签页
2. 设置模型路径（当前版本）
3. 点击"生成 DPO 配置"
4. 复制训练命令并执行

```bash
llamafactory-cli train configs/llmops/dpo_config_*.yaml
```

**时间**：几小时完成（比 CPT/SFT 快得多）

### 步骤 3: 部署新版本

1. 训练完成后得到新的 LoRA（v1.1）
2. 更新推理服务配置
3. 继续收集反馈，进入下一轮迭代

**这就是完整的在线闭环！** 🎉

---

## 🏗️ 离线训练流程（大版本开发）

仅在以下情况需要：
- 首次构建模型
- 大版本升级（如 Qwen2 → Qwen3）
- 需要新的领域能力

**产出两个模型**：
- **Completion Model**（CPT 后）- 用于文本补全、代码生成
- **Chat Model**（SFT 后）- 用于对话、问答

### 2. 数据生成（5分钟）

#### 步骤 1: 领域语料
1. 点击"加载预置文档"
2. 点击"处理语料"
3. 点击"保存语料"

✅ 生成文件：`data/llmops/domain_corpus_*.jsonl`

#### 步骤 2: 指令数据
1. 设置生成数量（建议 50-100）
2. 点击"生成指令数据"
3. 点击"保存指令数据"

✅ 生成文件：`data/llmops/sft_data_*.json`

#### 步骤 3: 偏好数据
1. 点击"创建模拟偏好数据"（演示用）
2. 或通过 A/B 测试收集真实偏好

✅ 生成文件：`data/llmops/prefs.jsonl`

### 3. 配置训练（3分钟）

#### CPT 配置
1. 进入"训练配置" → "CPT" 标签
2. 设置基础模型：`Qwen/Qwen2-1.5B`
3. 点击"生成 CPT 配置"
4. 复制训练命令

#### SFT 配置
1. 进入"训练配置" → "SFT" 标签
2. 设置模型路径：`checkpoints/domain-cpt`（或使用原始模型）
3. 点击"生成 SFT 配置"
4. 复制训练命令

#### DPO 配置
1. 进入"训练配置" → "DPO" 标签
2. 设置模型路径：`checkpoints/sft-lora`
3. 点击"生成 DPO 配置"
4. 复制训练命令

### 4. 执行训练（需要 GPU）

```bash
# CPT 训练（可选）
llamafactory-cli train configs/llmops/cpt_config_*.yaml

# SFT 训练（必需）
llamafactory-cli train configs/llmops/sft_config_*.yaml

# DPO 训练（推荐）
llamafactory-cli train configs/llmops/dpo_config_*.yaml
```

### 5. A/B 测试与反馈（持续）

1. 进入"A/B 测试与反馈闭环"标签
2. 输入问题，点击"生成对比回答"
3. 查看不同模型的回答
4. 选择更好的回答（A 或 B）
5. 偏好数据自动写入 `prefs.jsonl`

### 6. 在线闭环迭代 ⭐ 核心

```
第1周：v1.0 上线 → 收集 150 条偏好
第2周：DPO 训练 → v1.1 上线 → 收集 200 条偏好  
第3周：DPO 训练 → v1.2 上线 → 收集 180 条偏好
...持续迭代
```

**关键**：
- ✅ 每周/月收集足够偏好数据（100-500 条）
- ✅ 定期 DPO 训练（几小时完成）
- ✅ 快速部署新版本
- ✅ 无需重新训练 CPT/SFT

## 📊 演示模式

系统内置演示模式，无需实际训练即可体验：

1. 模拟数据生成
2. 配置文件生成  
3. A/B 测试对比（使用内置响应）

这允许你在没有 GPU 的情况下了解整个流程。

## 🎯 实战建议

### 如果你是第一次使用

1. **先体验在线优化**（演示模式）
   - 进入 A/B 测试标签
   - 创建模拟偏好数据
   - 体验 DPO 配置生成

2. **再考虑离线训练**（需要 GPU）
   - 有真实领域数据时
   - 需要构建专属 Chat 模型
   - 进行 CPT/SFT 训练

### 生产环境最佳实践

1. **第一阶段**：离线训练（数周-数月）
   - CPT 训练 → **Completion Model v1.0** 发布
   - SFT 训练 → **Chat Model v1.0** 发布
   
2. **第二阶段**：在线优化 Chat 模型（持续）
   - 每天收集用户反馈
   - 每周执行一次 DPO
   - 快速迭代：v1.1 → v1.2 → v1.3...
   
3. **第三阶段**：大版本更新（当优化到瓶颈）
   - 考虑 v2.0 重构
   - 重新做 CPT/SFT
   - 发布新的 Completion 和 Chat 模型
   - 开启新一轮在线优化

**关键**：Completion Model 在 CPT 后就可以发布，无需等 Chat Model 完成！

## 🔍 监控数据

查看数据统计：
```python
# 在"数据生成"标签页点击"刷新统计"

# 或查看文件
ls -lh data/llmops/
cat data/llmops/prefs.jsonl | wc -l  # 偏好数据条数
```

## 🐛 常见问题

### Q: 没有 GPU 怎么办？
A: 使用演示模式，或使用 CPU 训练小模型（训练会很慢）。

### Q: 训练需要多久？
A: 
- **CPT**: 2-8 小时（离线，数月一次）
- **SFT**: 1-4 小时（离线，数月一次）
- **DPO**: 30分钟-2小时（在线，每周/月一次）⭐

### Q: 偏好数据需要多少条？
A: 
- **最少**：100 条（每次 DPO）
- **建议**：300-500 条（每周/月收集）
- **理想**：1000+ 条（累积后效果更好）
- **关键**：真实用户反馈 > 人工构造

### Q: 如何评估效果？
A: 
1. A/B 测试投票比例
2. 人工评估生成质量
3. 对比模型在测试集上的表现

## 📚 完整文档

详细文档请查看：`docs/LLMOPS_GUIDE.md`

## 🎯 下一步

1. 收集更多真实用户反馈
2. 定期执行 DPO 训练
3. 监控模型性能指标
4. 调整训练超参数
5. 扩展到更大模型

---

**提示**: 这是一个教学演示系统，实际生产部署需要更完善的监控、安全和性能优化。

