"""
LLMOps é—­ç¯ç³»ç»Ÿä¸»ç•Œé¢
æŒ‰ç…§ä¸šåŠ¡æµç¨‹ç»„ç»‡ï¼šCPT â†’ SFT â†’ DPOï¼Œæ¯ä¸ªTabå†…èšæ•°æ®+è®­ç»ƒ
"""
# å¿…é¡»åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å… transformers å¯¼å…¥ TensorFlow
import os
os.environ.setdefault('TRANSFORMERS_NO_TF', '1')

import gradio as gr
import json
from datetime import datetime
from typing import Dict, Any, List, Tuple

from .self_instruct_generator import SelfInstructGenerator
from .domain_corpus_processor import DomainCorpusProcessor
from .preference_collector import PreferenceCollector
from .llama_factory_config import LLaMAFactoryConfig
from .llamafactory_trainer import get_trainer
from .llmops_engine import LLMOpsEngine
from .llmops_models import LLaMAFactoryModels
from .inference_model import InferenceModel


def get_trained_models(stage: str = "cpt") -> List[str]:
    """æ‰«æå·²è®­ç»ƒçš„æ¨¡å‹ç›®å½•ï¼ˆåŸºäºè®­ç»ƒé˜¶æ®µçš„è¾“å‡ºç›®å½•ï¼‰
    
    Args:
        stage: è®­ç»ƒé˜¶æ®µ "cpt", "sft", "dpo"
        - "cpt": æ‰«æ checkpoints/cpt/ ç›®å½•
        - "sft": æ‰«æ checkpoints/sft/ ç›®å½•
        - "dpo": æ‰«æ checkpoints/dpo/ ç›®å½•
    
    Returns:
        å·²è®­ç»ƒæ¨¡å‹è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰ï¼‰
    """
    models = []
    
    # åªæ‰«æå¯¹åº”é˜¶æ®µçš„è¾“å‡ºç›®å½•
    checkpoint_dir = os.path.join("checkpoints", stage)
    
    if not os.path.exists(checkpoint_dir):
        return models
    
    # æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    try:
        for item in os.listdir(checkpoint_dir):
            item_path = os.path.join(checkpoint_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å« adapter_config.jsonï¼ˆLoRAæ¨¡å‹æ ‡å¿—ï¼‰
                adapter_config = os.path.join(item_path, "adapter_config.json")
                if os.path.exists(adapter_config):
                    models.append(item_path)
    except Exception as e:
        print(f"æ‰«æ {checkpoint_dir} ç›®å½•å¤±è´¥: {e}")
        return models
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    if models:
        models.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    return models


def get_available_datasets(stage: str = None) -> List[str]:
    """è·å–å·²æ³¨å†Œçš„æ•°æ®é›†åˆ—è¡¨
    
    Args:
        stage: è®­ç»ƒé˜¶æ®µ "cpt", "sft", "dpo"ã€‚å¦‚æœä¸ºNoneï¼Œè¿”å›æ‰€æœ‰æ•°æ®é›†
    
    Returns:
        ç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†åç§°åˆ—è¡¨
    """
    dataset_info_path = "data/llmops/dataset_info.json"
    if not os.path.exists(dataset_info_path):
        return []
    
    try:
        with open(dataset_info_path, 'r', encoding='utf-8') as f:
            dataset_info = json.load(f)
        
        if stage is None:
            return list(dataset_info.keys())
        
        # æ ¹æ®é˜¶æ®µè¿‡æ»¤æ•°æ®é›†
        filtered_datasets = []
        for name, config in dataset_info.items():
            if stage == "cpt":
                # CPT: åªè¦ prompt->text çš„æ•°æ®é›†ï¼ˆæ²¡æœ‰ formatting æˆ– formatting ä¸æ˜¯ sharegptï¼‰
                if config.get("columns", {}).get("prompt") == "text" and not config.get("formatting"):
                    filtered_datasets.append(name)
            elif stage == "sft":
                # SFT: ShareGPT æ ¼å¼ä¸”æœ‰ messages å­—æ®µï¼Œæ²¡æœ‰ ranking
                if config.get("formatting") == "sharegpt" and "messages" in config.get("columns", {}) and not config.get("ranking"):
                    filtered_datasets.append(name)
            elif stage == "dpo":
                # DPO: ShareGPT æ ¼å¼ä¸”æœ‰ ranking=True
                if config.get("formatting") == "sharegpt" and config.get("ranking"):
                    filtered_datasets.append(name)
        
        return filtered_datasets
    except Exception as e:
        print(f"è¯»å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {e}")
        return []


class LLMOpsSystem:
    """LLMOps ç³»ç»Ÿç®¡ç†å™¨"""
    
    def __init__(self):
        self.self_instruct = SelfInstructGenerator()
        self.corpus_processor = DomainCorpusProcessor()
        self.pref_collector = PreferenceCollector()
        self.config_manager = LLaMAFactoryConfig()
        self.inference_model = InferenceModel()  # æ¨ç†æ¨¡å‹ï¼ˆå€Ÿé‰´ LLaMA-Factoryï¼‰
        
        # æ¨¡å‹è·¯å¾„çŠ¶æ€ï¼ˆç¡®ä¿æµç¨‹ä¾èµ–ï¼‰
        self.cpt_output_path = ""  # CPT è¾“å‡ºçš„ Completion Model
        self.sft_output_path = ""  # SFT è¾“å‡ºçš„ Chat Model
        
        # å½“å‰å¯¹æ¯”æµ‹è¯•çš„æŸ¥è¯¢å’Œå“åº”
        self.current_query = ""
        self.current_model = ""
        self.current_responses = {}


def build_llmops_content():
    """æ„å»º LLMOps å†…å®¹ï¼ˆä¸åˆ›å»º Blocksï¼Œç›´æ¥æ¸²æŸ“ç»„ä»¶ï¼‰
    
    Returns:
        train_engine: è®­ç»ƒå¼•æ“å®ä¾‹ï¼Œå¯ç”¨äºè°ƒç”¨ resume() ç­‰æ–¹æ³•
    """
    
    system = LLMOpsSystem()
    
    gr.Markdown("""
    # ğŸ”„ LLMOps æŒç»­è¿›åŒ–é—­ç¯ç³»ç»Ÿ
    
    **å®Œæ•´æµç¨‹**: Base Model â†’ CPT â†’ Completion Model â†’ SFT â†’ Chat Model â†’ DPO â†’ Optimized Chat Model
    
    æ¯ä¸ªé˜¶æ®µå†…èšï¼šæ•°æ®å‡†å¤‡ + è®­ç»ƒé…ç½® + æ¨¡å‹è¾“å‡º
    """)
    
    # ç”¨äºå­˜å‚¨è®­ç»ƒå¼•æ“
    train_engines = {}
    
    # è·å–æ¨¡å‹åˆ—è¡¨
    model_choices = LLaMAFactoryModels.get_flat_choices()
    print(f"âœ… åŠ è½½äº† {len(model_choices)} ä¸ªæ”¯æŒçš„æ¨¡å‹")
    
    with gr.Tabs():
        # ==================== Tab 1: CPT (Continued Pre-Training) ====================
        with gr.Tab("ğŸ“š é˜¶æ®µ1: CPT - ç»§ç»­é¢„è®­ç»ƒ"):
            gr.Markdown("""
            ### é¢†åŸŸé€‚é… - æ³¨å…¥è¡Œä¸šçŸ¥è¯†
            **è¾“å…¥**: Base Modelï¼ˆå¦‚ Llama-3-8Bï¼‰
            **æ•°æ®**: é¢†åŸŸä¸“ä¸šè¯­æ–™ï¼ˆæ— ç›‘ç£æ–‡æœ¬ï¼‰
            **è¾“å‡º**: Completion Modelï¼ˆå¯ç”¨äºæ–‡æœ¬è¡¥å…¨ã€ä»£ç ç”Ÿæˆï¼‰
            """)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ğŸ“Š ç¬¬1æ­¥ï¼šå‡†å¤‡é¢†åŸŸè¯­æ–™")
                    load_corpus_btn = gr.Button("ğŸ“¥ åŠ è½½é¢„ç½®æ–‡æ¡£", variant="secondary")
                    corpus_limit = gr.Slider(100, 2000, value=500, step=100, label="åŠ è½½æ–‡æ¡£æ•°é‡")
                    process_corpus_btn = gr.Button("ğŸ”§ å¤„ç†è¯­æ–™", variant="primary")
                    save_corpus_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¸ºCPTæ•°æ®é›†", variant="secondary")
                    corpus_output = gr.HTML(value="<p>ç‚¹å‡»åŠ è½½é¢„ç½®æ–‡æ¡£å¼€å§‹...</p>")
                    corpus_stats = gr.HTML(value="<p>å¤„ç†åæ˜¾ç¤ºç»Ÿè®¡...</p>")
                
                with gr.Column():
                    gr.Markdown("#### âš™ï¸ ç¬¬2æ­¥ï¼šé…ç½®CPTè®­ç»ƒ")
                    
                    # åŸºç¡€é…ç½®
                    with gr.Row():
                        cpt_model = gr.Dropdown(
                            choices=model_choices,
                            value="Qwen/Qwen2-0.5B",
                            label="Base Model",
                            info="æ”¯æŒ100+ä¸»æµå¼€æºæ¨¡å‹",
                            allow_custom_value=True,
                            filterable=True,
                            interactive=True
                        )
                    
                    with gr.Row():
                        cpt_dataset = gr.Dropdown(
                            choices=get_available_datasets("cpt"),  # åªæ˜¾ç¤ºCPTæ•°æ®é›†
                            value="test_corpus_large",  # ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
                            label="æ•°æ®é›†åç§°",
                            info="é€‰æ‹©å·²ä¿å­˜çš„CPTæ•°æ®é›†ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰",
                            allow_custom_value=True,
                            interactive=True
                        )
                        cpt_output = gr.Textbox(value="checkpoints/cpt/qwen-0.5b-cpt", label="è¾“å‡ºè·¯å¾„")
                    
                    # è®­ç»ƒå‚æ•°
                    with gr.Accordion("ğŸ”§ è®­ç»ƒå‚æ•°", open=False):
                        with gr.Row():
                            cpt_epochs = gr.Slider(1, 10, value=1, step=1, label="è®­ç»ƒè½®æ•°")
                            cpt_lr = gr.Slider(1e-5, 1e-3, value=5e-5, step=1e-5, label="å­¦ä¹ ç‡")
                        with gr.Row():
                            cpt_batch_size = gr.Slider(1, 16, value=1, step=1, label="æ‰¹æ¬¡å¤§å°")
                            cpt_grad_acc = gr.Slider(1, 16, value=2, step=1, label="æ¢¯åº¦ç´¯ç§¯")
                        with gr.Row():
                            cpt_max_len = gr.Slider(512, 4096, value=512, step=128, label="æœ€å¤§åºåˆ—é•¿åº¦")
                            cpt_save_steps = gr.Slider(10, 2000, value=50, step=10, label="ä¿å­˜æ­¥æ•°")
                    
                    # LoRA é…ç½®
                    with gr.Accordion("ğŸ¯ LoRA é…ç½®", open=False):
                        with gr.Row():
                            cpt_lora_rank = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank")
                            cpt_lora_alpha = gr.Slider(8, 128, value=16, step=8, label="LoRA Alpha")
                        cpt_lora_dropout = gr.Slider(0, 0.5, value=0.05, step=0.05, label="LoRA Dropout")
                    
                    # æ“ä½œæŒ‰é’®
                    with gr.Row():
                        cpt_start_btn = gr.Button("ğŸš€ å¼€å§‹CPTè®­ç»ƒ", variant="primary")
                        cpt_stop_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="secondary")
                    
                    # çŠ¶æ€æ˜¾ç¤º
                    cpt_progress = gr.Slider(0, 100, value=0, label="è®­ç»ƒè¿›åº¦", visible=False, interactive=False)
                    cpt_status = gr.HTML(value="<p>æœªå¼€å§‹è®­ç»ƒ</p>")
                    
                    # åˆ›å»º CPT è®­ç»ƒå¼•æ“
                    cpt_engine = LLMOpsEngine()
                    train_engines['cpt'] = cpt_engine
        
        # ==================== Tab 2: SFT (Supervised Fine-Tuning) ====================
        with gr.Tab("ğŸ“ é˜¶æ®µ2: SFT - æŒ‡ä»¤å¾®è°ƒ"):
            gr.Markdown("""
            ### æŒ‡ä»¤å¯¹é½ - æ•™ä¼šæ¨¡å‹å¯¹è¯
            **è¾“å…¥**: CPT è¾“å‡ºçš„ Completion Model
            **æ•°æ®**: æŒ‡ä»¤-å›ç­”å¯¹ï¼ˆinstruction-response pairsï¼‰
            **è¾“å‡º**: Chat Modelï¼ˆå¯ç”¨äºå¯¹è¯ã€é—®ç­”ï¼‰
            """)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ğŸ“Š ç¬¬1æ­¥ï¼šç”ŸæˆæŒ‡ä»¤æ•°æ®")
                    instruct_count = gr.Slider(10, 200, value=50, step=10, label="ç”ŸæˆæŒ‡ä»¤æ•°é‡")
                    generate_instruct_btn = gr.Button("ğŸš€ ç”ŸæˆæŒ‡ä»¤æ•°æ®", variant="primary")
                    save_instruct_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¸ºSFTæ•°æ®é›†", variant="secondary")
                    instruct_output = gr.HTML(value="<p>ç‚¹å‡»ç”ŸæˆæŒ‡ä»¤æ•°æ®å¼€å§‹...</p>")
                    instruct_stats = gr.HTML(value="<p>ç”Ÿæˆåæ˜¾ç¤ºç»Ÿè®¡...</p>")
                
                with gr.Column():
                    gr.Markdown("#### âš™ï¸ ç¬¬2æ­¥ï¼šé…ç½®SFTè®­ç»ƒ")
                    gr.Markdown("âš ï¸ **æ³¨æ„**: æ¨¡å‹è·¯å¾„å¿…é¡»ä½¿ç”¨ CPT çš„è¾“å‡º")
                    
                    # åŸºç¡€é…ç½®
                    with gr.Row():
                        sft_base_model = gr.Dropdown(
                            choices=model_choices,
                            value="Qwen/Qwen2-0.5B",
                            label="Base Model",
                            info="åŸºç¡€æ¨¡å‹ï¼ˆä¸CPTç›¸åŒï¼‰",
                            allow_custom_value=True,
                            filterable=True,
                            interactive=True
                        )
                    
                    with gr.Row():
                        # åˆå§‹åŒ–æ—¶åŠ è½½å¯ç”¨çš„CPTæ¨¡å‹
                        initial_cpt_models = get_trained_models("cpt")
                        
                        sft_cpt_model = gr.Dropdown(
                            choices=initial_cpt_models,
                            value=initial_cpt_models[0] if initial_cpt_models else None,
                            label="CPT Checkpoint",
                            info="é€‰æ‹©CPTé˜¶æ®µçš„è¾“å‡ºæ¨¡å‹ï¼ˆå¿…å¡«ï¼Œè¯·å…ˆå®ŒæˆCPTè®­ç»ƒï¼‰",
                            allow_custom_value=True,
                            interactive=True
                        )
                        sft_refresh_models = gr.Button("ğŸ”„", scale=0, min_width=50)
                    
                    with gr.Row():
                        sft_dataset = gr.Dropdown(
                            choices=get_available_datasets("sft"),  # åªæ˜¾ç¤ºSFTæ•°æ®é›†
                            value="test_sft_data",  # ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
                            label="æ•°æ®é›†åç§°",
                            info="é€‰æ‹©å·²ä¿å­˜çš„SFTæ•°æ®é›†ï¼ˆShareGPTå¯¹è¯æ ¼å¼ï¼‰",
                            allow_custom_value=True,
                            interactive=True
                        )
                        sft_output = gr.Textbox(value="checkpoints/sft/qwen-0.5b-sft", label="è¾“å‡ºè·¯å¾„")
                    
                    with gr.Row():
                        sft_template = gr.Dropdown(
                            choices=["llama3", "qwen", "chatglm3", "mistral"],
                            value="llama3",
                            label="å¯¹è¯æ¨¡æ¿"
                        )
                    
                    # è®­ç»ƒå‚æ•°
                    with gr.Accordion("ğŸ”§ è®­ç»ƒå‚æ•°", open=False):
                        with gr.Row():
                            sft_epochs = gr.Slider(1, 10, value=1, step=1, label="è®­ç»ƒè½®æ•°")
                            sft_lr = gr.Slider(1e-5, 1e-3, value=5e-5, step=1e-5, label="å­¦ä¹ ç‡")
                        with gr.Row():
                            sft_batch_size = gr.Slider(1, 16, value=1, step=1, label="æ‰¹æ¬¡å¤§å°")
                            sft_grad_acc = gr.Slider(1, 16, value=2, step=1, label="æ¢¯åº¦ç´¯ç§¯")
                        with gr.Row():
                            sft_max_len = gr.Slider(512, 4096, value=512, step=128, label="æœ€å¤§åºåˆ—é•¿åº¦")
                            sft_save_steps = gr.Slider(10, 2000, value=50, step=10, label="ä¿å­˜æ­¥æ•°")
                    
                    # LoRA é…ç½®
                    with gr.Accordion("ğŸ¯ LoRA é…ç½®", open=False):
                        with gr.Row():
                            sft_lora_rank = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank")
                            sft_lora_alpha = gr.Slider(8, 128, value=16, step=8, label="LoRA Alpha")
                        sft_lora_dropout = gr.Slider(0, 0.5, value=0.05, step=0.05, label="LoRA Dropout")
                    
                    # æ“ä½œæŒ‰é’®
                    with gr.Row():
                        sft_start_btn = gr.Button("ğŸš€ å¼€å§‹SFTè®­ç»ƒ", variant="primary")
                        sft_stop_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="secondary")
                    
                    # çŠ¶æ€æ˜¾ç¤º
                    sft_progress = gr.Slider(0, 100, value=0, label="è®­ç»ƒè¿›åº¦", visible=False, interactive=False)
                    sft_status = gr.HTML(value="<p>æœªå¼€å§‹è®­ç»ƒ</p>")
                    
                    # åˆ›å»º SFT è®­ç»ƒå¼•æ“
                    sft_engine = LLMOpsEngine()
                    train_engines['sft'] = sft_engine
        
        # ==================== Tab 3: DPO/RLHF - åœ¨çº¿ä¼˜åŒ–é—­ç¯ ====================
        with gr.Tab("ğŸ”¬ é˜¶æ®µ3: DPO/RLHF - åå¥½å¯¹é½"):
            gr.Markdown("""
            ### åå¥½å¯¹é½ - æŒç»­ä¼˜åŒ–
            **è¾“å…¥**: SFT è¾“å‡ºçš„ Chat Model
            **æ•°æ®**: ç”¨æˆ·åå¥½æ•°æ®ï¼ˆé€šè¿‡ABæµ‹è¯•æ”¶é›†ï¼‰
            **è¾“å‡º**: Optimized Chat Modelï¼ˆv1.0 â†’ v1.1 â†’ v1.2...ï¼‰
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("#### ğŸš€ ç¬¬1æ­¥ï¼šåŠ è½½æ¨ç†æ¨¡å‹")
                    gr.Markdown("*ä½¿ç”¨ LLaMA-Factory å†…ç½®æ¨ç†å¼•æ“ï¼Œç›´æ¥åŠ è½½æ¨¡å‹åˆ°å†…å­˜*")
                    
                    # åŠ è½½å¯ç”¨çš„SFTå’ŒDPOæ¨¡å‹
                    inference_sft_models = get_trained_models("sft")
                    inference_dpo_models = get_trained_models("dpo")
                    inference_models = inference_sft_models + inference_dpo_models
                    
                    infer_model = gr.Dropdown(
                        choices=inference_models,
                        value=inference_models[0] if inference_models else None,
                        label="Chat Model (SFT/DPO)",
                        info="é€‰æ‹©SFTæˆ–DPOæ¨¡å‹",
                        allow_custom_value=True,
                        interactive=True
                    )
                    infer_refresh = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨", variant="secondary")
                    
                    with gr.Row():
                        load_model_btn = gr.Button("â–¶ï¸ åŠ è½½æ¨¡å‹", variant="primary")
                        unload_model_btn = gr.Button("â¹ï¸ å¸è½½æ¨¡å‹", variant="secondary")
                    
                    infer_status = gr.Textbox(
                        label="æ¨¡å‹çŠ¶æ€",
                        value="æœªåŠ è½½æ¨¡å‹",
                        interactive=False,
                        lines=3
                    )
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("#### ğŸ”¬ ç¬¬2æ­¥ï¼šABæµ‹è¯•æ”¶é›†åå¥½")
                    gr.Markdown("åŒä¸€æ¨¡å‹ç”Ÿæˆä¸¤ä¸ªä¸åŒå›ç­”ï¼ˆé€šè¿‡è°ƒæ•´é‡‡æ ·å‚æ•°ï¼‰ï¼Œç”¨æˆ·æŠ•ç¥¨é€‰æ‹©æ›´å¥½çš„å›ç­”")
                    
                    ab_query = gr.Textbox(
                        label="è¾“å…¥æµ‹è¯•é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
                        lines=2
                    )
                    
                    with gr.Row():
                        ab_model = gr.Dropdown(
                            choices=inference_models,
                            value=inference_models[0] if inference_models else None,
                            label="é€‰æ‹©æ¨¡å‹",
                            info="ç”¨äºç”ŸæˆABå¯¹æ¯”çš„æ¨¡å‹",
                            allow_custom_value=True,
                            interactive=True
                        )
                        ab_refresh_model = gr.Button("ğŸ”„", scale=0, min_width=50)
                    
                    with gr.Row():
                        ab_temperature_a = gr.Slider(0.1, 2.0, value=0.7, step=0.1, label="Temperature Aï¼ˆæ›´ä¿å®ˆï¼‰")
                        ab_temperature_b = gr.Slider(0.1, 2.0, value=1.2, step=0.1, label="Temperature Bï¼ˆæ›´åˆ›é€ æ€§ï¼‰")
                    
                    ab_generate_btn = gr.Button("ğŸ”„ ç”ŸæˆABå¯¹æ¯”", variant="primary")
                    
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("##### ğŸ…°ï¸ å›ç­” A")
                            response_a_label = gr.Textbox(value="", label="æ¨¡å‹ç‰ˆæœ¬", interactive=False)
                            response_a = gr.Textbox(label="å›ç­”å†…å®¹", lines=6, interactive=False)
                            vote_a_btn = gr.Button("ğŸ‘ é€‰æ‹© A æ›´å¥½", variant="secondary", size="lg")
                        
                        with gr.Column():
                            gr.Markdown("##### ğŸ…±ï¸ å›ç­” B")
                            response_b_label = gr.Textbox(value="", label="æ¨¡å‹ç‰ˆæœ¬", interactive=False)
                            response_b = gr.Textbox(label="å›ç­”å†…å®¹", lines=6, interactive=False)
                            vote_b_btn = gr.Button("ğŸ‘ é€‰æ‹© B æ›´å¥½", variant="secondary", size="lg")
                    
                    ab_result = gr.HTML(value="<p>ç”Ÿæˆå¯¹æ¯”åæŠ•ç¥¨ï¼Œåå¥½æ•°æ®è‡ªåŠ¨ä¿å­˜åˆ° prefs.jsonl</p>")
                    
                    with gr.Row():
                        view_prefs_btn = gr.Button("ğŸ“Š æŸ¥çœ‹åå¥½ç»Ÿè®¡", variant="secondary")
                        export_prefs_btn = gr.Button("ğŸ“¤ å¯¼å‡ºDPOæ•°æ®é›†", variant="primary")
                    
                    prefs_stats = gr.HTML(value="<p>ç‚¹å‡»æŸ¥çœ‹åå¥½ç»Ÿè®¡...</p>")
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### âš™ï¸ ç¬¬3æ­¥ï¼šé…ç½®DPOè®­ç»ƒ")
                    gr.Markdown("âš ï¸ **æ³¨æ„**: æ¨¡å‹è·¯å¾„å¿…é¡»ä½¿ç”¨ SFT çš„è¾“å‡º")
                    
                    # åŸºç¡€é…ç½®
                    with gr.Row():
                        dpo_base_model = gr.Dropdown(
                            choices=model_choices,
                            value="Qwen/Qwen2-0.5B",
                            label="Base Model",
                            info="åŸºç¡€æ¨¡å‹ï¼ˆä¸SFTç›¸åŒï¼‰",
                            allow_custom_value=True,
                            filterable=True,
                            interactive=True
                        )
                    
                    with gr.Row():
                        # åˆå§‹åŒ–æ—¶åŠ è½½å¯ç”¨çš„SFTå’ŒDPOæ¨¡å‹
                        initial_sft_models = get_trained_models("sft")
                        initial_dpo_models = get_trained_models("dpo")
                        # åˆå¹¶SFTå’ŒDPOæ¨¡å‹ï¼ˆDPOå¯ä»¥åœ¨ä¹‹å‰çš„DPOåŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼‰
                        initial_models = initial_sft_models + initial_dpo_models
                        
                        dpo_sft_model = gr.Dropdown(
                            choices=initial_models,
                            value=initial_models[0] if initial_models else None,
                            label="SFT/DPO Checkpoint",
                            info="é€‰æ‹©SFTæˆ–DPOæ¨¡å‹ï¼ˆå¿…å¡«ï¼Œè¯·å…ˆå®ŒæˆSFTè®­ç»ƒï¼‰",
                            allow_custom_value=True,
                            interactive=True
                        )
                        dpo_refresh_models = gr.Button("ğŸ”„", scale=0, min_width=50)
                    
                    with gr.Row():
                        dpo_dataset = gr.Dropdown(
                            choices=get_available_datasets("dpo"),  # åªæ˜¾ç¤ºDPOæ•°æ®é›†
                            value="test_dpo_data",  # ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
                            label="åå¥½æ•°æ®é›†",
                            info="é€‰æ‹©å·²ä¿å­˜çš„DPOæ•°æ®é›†ï¼ˆShareGPT Rankingæ ¼å¼ï¼‰",
                            allow_custom_value=True,
                            interactive=True
                        )
                        dpo_output = gr.Textbox(value="checkpoints/dpo/qwen-0.5b-dpo", label="è¾“å‡ºè·¯å¾„")
                    
                    # DPO ç‰¹æœ‰å‚æ•°
                    with gr.Row():
                        dpo_beta = gr.Slider(0.01, 0.5, value=0.1, step=0.01, label="DPO Beta", info="åå¥½å¼ºåº¦")
                        dpo_ftx = gr.Slider(0, 1, value=0, step=0.1, label="FTXæƒé‡", info="SFTæŸå¤±æƒé‡")
                    
                    # è®­ç»ƒå‚æ•°
                    with gr.Accordion("ğŸ”§ è®­ç»ƒå‚æ•°", open=False):
                        with gr.Row():
                            dpo_epochs = gr.Slider(1, 10, value=1, step=1, label="è®­ç»ƒè½®æ•°")
                            dpo_lr = gr.Slider(1e-6, 1e-4, value=5e-6, step=1e-6, label="å­¦ä¹ ç‡ï¼ˆDPOé€šå¸¸æ›´å°ï¼‰")
                        with gr.Row():
                            dpo_batch_size = gr.Slider(1, 16, value=1, step=1, label="æ‰¹æ¬¡å¤§å°")
                            dpo_grad_acc = gr.Slider(1, 16, value=2, step=1, label="æ¢¯åº¦ç´¯ç§¯")
                        with gr.Row():
                            dpo_max_len = gr.Slider(512, 4096, value=512, step=128, label="æœ€å¤§åºåˆ—é•¿åº¦")
                            dpo_save_steps = gr.Slider(10, 2000, value=50, step=10, label="ä¿å­˜æ­¥æ•°")
                    
                    # LoRA é…ç½®
                    with gr.Accordion("ğŸ¯ LoRA é…ç½®", open=False):
                        with gr.Row():
                            dpo_lora_rank = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank")
                            dpo_lora_alpha = gr.Slider(8, 128, value=16, step=8, label="LoRA Alpha")
                        dpo_lora_dropout = gr.Slider(0, 0.5, value=0.05, step=0.05, label="LoRA Dropout")
                    
                    # æ“ä½œæŒ‰é’®
                    with gr.Row():
                        dpo_start_btn = gr.Button("ğŸš€ å¼€å§‹DPOè®­ç»ƒ", variant="primary")
                        dpo_stop_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="secondary")
                    
                    # çŠ¶æ€æ˜¾ç¤º
                    dpo_progress = gr.Slider(0, 100, value=0, label="è®­ç»ƒè¿›åº¦", visible=False, interactive=False)
                    dpo_status = gr.HTML(value="<p>æœªå¼€å§‹è®­ç»ƒ</p>")
                    
                    # åˆ›å»º DPO è®­ç»ƒå¼•æ“
                    dpo_engine = LLMOpsEngine()
                    train_engines['dpo'] = dpo_engine
    
    # ==================== äº‹ä»¶ç»‘å®š ====================
    
    # === CPT Tab äº‹ä»¶ ===
    def load_corpus(limit):
        count = system.corpus_processor.load_from_preloaded(int(limit))
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… å·²åŠ è½½ {count} ç¯‡æ–‡æ¡£</p>
        </div>
        """
    
    def process_corpus():
        count = system.corpus_processor.process()
        stats = system.corpus_processor.get_statistics()
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <h4>âœ… è¯­æ–™å¤„ç†å®Œæˆ</h4>
            <ul>
                <li><strong>æ–‡æœ¬æ•°:</strong> {stats['processed_count']}</li>
                <li><strong>æ€»å­—ç¬¦:</strong> {stats['total_chars']:,}</li>
                <li><strong>ä¼°è®¡tokens:</strong> {stats['estimated_tokens']:,}</li>
            </ul>
        </div>
        """, f"""
        <div style="background-color: #f8f9fa; padding: 10px; border-radius: 5px;">
            <p><strong>å¹³å‡é•¿åº¦:</strong> {stats['avg_length']}</p>
            <p><strong>æ•°æ®è´¨é‡:</strong> å·²å»é‡ã€æ¸…æ´—</p>
        </div>
        """
    
    def save_corpus():
        filepath = system.corpus_processor.save_corpus()
        # è·å–æ›´æ–°åçš„CPTæ•°æ®é›†åˆ—è¡¨
        datasets = get_available_datasets("cpt")
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… CPTæ•°æ®é›†å·²ä¿å­˜: <code>{filepath}</code></p>
            <p>ğŸ’¡ æ•°æ®é›†å·²è‡ªåŠ¨æ³¨å†Œï¼Œå¯åœ¨è®­ç»ƒé…ç½®ä¸­é€‰æ‹©</p>
        </div>
        """, gr.update(choices=datasets, value="domain_corpus")
    
    # === SFT Tab äº‹ä»¶ ===
    def generate_instructions(count):
        instructions = system.self_instruct.generate_instructions(int(count), use_mock=True)
        stats = system.self_instruct.get_statistics()
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <h4>âœ… æŒ‡ä»¤æ•°æ®ç”Ÿæˆå®Œæˆ</h4>
            <ul>
                <li><strong>æœ¬æ¬¡ç”Ÿæˆ:</strong> {len(instructions)}</li>
                <li><strong>ç´¯è®¡æ€»æ•°:</strong> {stats['total']}</li>
            </ul>
        </div>
        """, f"""
        <div style="background-color: #f8f9fa; padding: 10px; border-radius: 5px;">
            <p><strong>ä»»åŠ¡ç±»å‹åˆ†å¸ƒ:</strong></p>
            {''.join(f"<li>{k}: {v}</li>" for k, v in stats.get('task_types', {}).items())}
        </div>
        """
    
    def save_instructions():
        filepath = system.self_instruct.save_dataset()
        # è·å–æ›´æ–°åçš„SFTæ•°æ®é›†åˆ—è¡¨
        datasets = get_available_datasets("sft")
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… SFTæ•°æ®é›†å·²ä¿å­˜: <code>{filepath}</code></p>
            <p>ğŸ’¡ æ•°æ®é›†å·²è‡ªåŠ¨æ³¨å†Œï¼Œå¯åœ¨è®­ç»ƒé…ç½®ä¸­é€‰æ‹©</p>
        </div>
        """, gr.update(choices=datasets, value="sft_data")
    
    # === è®­ç»ƒç›¸å…³å‡½æ•° ===
    def start_cpt_training(model, dataset, output, epochs, lr, batch_size, grad_acc, max_len, save_steps,
                          lora_rank, lora_alpha, lora_dropout):
        """å¯åŠ¨CPTè®­ç»ƒï¼ˆä½¿ç”¨ generator æŒç»­ç›‘æ§è¿›åº¦ï¼‰"""
        import time
        trainer = get_trainer()
        
        if trainer.is_training():
            yield gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>âš ï¸ å·²æœ‰è®­ç»ƒä»»åŠ¡åœ¨è¿è¡Œä¸­</p>
            </div>
            """
            return
        
        # æ„å»ºè®­ç»ƒé…ç½®
        config = {
            'stage': 'pt',
            'model_name_or_path': model,
            'dataset': dataset,
            'dataset_dir': 'data/llmops',
            'output_dir': output,
            'num_train_epochs': int(epochs),
            'learning_rate': float(lr),
            'per_device_train_batch_size': int(batch_size),
            'gradient_accumulation_steps': int(grad_acc),
            'cutoff_len': int(max_len),
            'save_steps': int(save_steps),
            'finetuning_type': 'lora',
            'lora_rank': int(lora_rank),
            'lora_alpha': int(lora_alpha),
            'lora_dropout': float(lora_dropout),
            'logging_steps': 10,
        }
        
        print(f"ğŸš€ å‡†å¤‡å¯åŠ¨CPTè®­ç»ƒ: {config}")
        success = trainer.start_training(config)
        print(f"è®­ç»ƒå¯åŠ¨ç»“æœ: {success}")
        
        if not success:
            yield gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥</h4>
                <p>è¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ</p>
            </div>
            """
            return
        
        # æ˜¾ç¤ºå¯åŠ¨æˆåŠŸ
        yield gr.update(value=0, visible=True), f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <h4>âœ… CPTè®­ç»ƒå·²å¯åŠ¨</h4>
            <p><strong>æ¨¡å‹:</strong> {model}</p>
            <p><strong>æ•°æ®é›†:</strong> {dataset}</p>
            <p><strong>è¾“å‡º:</strong> {output}</p>
            <p>è®­ç»ƒè¿›è¡Œä¸­...</p>
        </div>
        """
        
        # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        time.sleep(2)
        
        # æŒç»­ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆå‚è€ƒ LLaMA-Factory çš„ monitorï¼‰
        while trainer.is_training():
            # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
            return_code = trainer.check_process_status()
            if return_code is not None:
                # è¿›ç¨‹å·²ç»“æŸ
                break
            
            # è·å–è®­ç»ƒè¿›åº¦
            progress, status_msg = trainer.get_training_progress()
            log_text = trainer.get_training_logs(max_lines=10)
            
            # å¦‚æœæ—¥å¿—æ–‡ä»¶è¿˜æœªç”Ÿæˆï¼Œæ˜¾ç¤ºå‹å¥½æç¤º
            if "è®­ç»ƒå°šæœªå¼€å§‹æˆ–æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨" in log_text or "æš‚æ— è®­ç»ƒæ—¥å¿—" in log_text:
                yield gr.update(value=0, visible=True), f"""
                <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                    <h4>â³ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œæ•°æ®...</h4>
                    <p><strong>æ¨¡å‹:</strong> {model}</p>
                    <p><strong>æ•°æ®é›†:</strong> {dataset}</p>
                    <p><strong>çŠ¶æ€:</strong> è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–</p>
                    <p>ğŸ’¡ <strong>æç¤º:</strong> è¯¦ç»†æ—¥å¿—æ­£åœ¨ç»ˆç«¯çª—å£å®æ—¶è¾“å‡º</p>
                    <p>ğŸ“Š é¦–æ¬¡è®­ç»ƒéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...</p>
                </div>
                """
            else:
                # æ˜¾ç¤ºå®é™…è®­ç»ƒè¿›åº¦
                yield gr.update(value=progress, visible=True), f"""
                <div style="background-color: #d1ecf1; padding: 10px; border-radius: 5px;">
                    <h4>â³ {status_msg}</h4>
                    {log_text}
                </div>
                """
            
            time.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
        
        # è®­ç»ƒå®Œæˆ
        return_code = trainer.check_process_status()
        if return_code == 0:
            final_log = trainer.get_training_logs(max_lines=20)
            yield gr.update(value=100, visible=True), f"""
            <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
                <h4>âœ… CPTè®­ç»ƒå®Œæˆ</h4>
                {final_log}
            </div>
            """
        else:
            yield gr.update(visible=False), f"""
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¤±è´¥</h4>
                <p>é€€å‡ºç : {return_code}</p>
            </div>
            """
    
    def stop_cpt_training():
        """åœæ­¢CPTè®­ç»ƒ"""
        trainer = get_trainer()
        success = trainer.stop_training()
        
        if success:
            return gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>â¹ï¸ è®­ç»ƒå·²åœæ­¢</p>
            </div>
            """
        else:
            return gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <p>âŒ æ²¡æœ‰è¿è¡Œä¸­çš„è®­ç»ƒä»»åŠ¡</p>
            </div>
            """
    
    def start_sft_training(base_model, cpt_model, dataset, output, template, epochs, lr, batch_size, grad_acc, max_len, save_steps,
                          lora_rank, lora_alpha, lora_dropout):
        """å¯åŠ¨SFTè®­ç»ƒï¼ˆä½¿ç”¨ generator æŒç»­ç›‘æ§è¿›åº¦ï¼‰"""
        import time
        trainer = get_trainer()
        
        if trainer.is_training():
            yield gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>âš ï¸ å·²æœ‰è®­ç»ƒä»»åŠ¡åœ¨è¿è¡Œä¸­</p>
            </div>
            """
            return
        
        # éªŒè¯å¿…å¡«é¡¹
        if not cpt_model:
            yield gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ é…ç½®é”™è¯¯</h4>
                <p>å¿…é¡»é€‰æ‹©CPT Checkpoint</p>
            </div>
            """
            return
        
        config = {
            'stage': 'sft',
            'model_name_or_path': base_model,  # åŸºç¡€æ¨¡å‹
            'adapter_name_or_path': cpt_model,  # CPT checkpoint
            'dataset': dataset,
            'dataset_dir': 'data/llmops',
            'template': template,
            'output_dir': output,
            'num_train_epochs': int(epochs),
            'learning_rate': float(lr),
            'per_device_train_batch_size': int(batch_size),
            'gradient_accumulation_steps': int(grad_acc),
            'cutoff_len': int(max_len),
            'save_steps': int(save_steps),
            'finetuning_type': 'lora',
            'lora_rank': int(lora_rank),
            'lora_alpha': int(lora_alpha),
            'lora_dropout': float(lora_dropout),
            'logging_steps': 10,
        }
        
        print(f"ğŸš€ å‡†å¤‡å¯åŠ¨SFTè®­ç»ƒ: {config}")
        success = trainer.start_training(config)
        print(f"è®­ç»ƒå¯åŠ¨ç»“æœ: {success}")
        
        if not success:
            yield gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥</h4>
                <p>è¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ</p>
            </div>
            """
            return
        
        # æ˜¾ç¤ºå¯åŠ¨æˆåŠŸ
        yield gr.update(value=0, visible=True), f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <h4>âœ… SFTè®­ç»ƒå·²å¯åŠ¨</h4>
            <p><strong>åŸºç¡€æ¨¡å‹:</strong> {base_model}</p>
            <p><strong>CPT Checkpoint:</strong> {cpt_model}</p>
            <p><strong>æ¨¡æ¿:</strong> {template}</p>
            <p><strong>æ•°æ®é›†:</strong> {dataset}</p>
            <p><strong>è¾“å‡º:</strong> {output}</p>
            <p>è®­ç»ƒè¿›è¡Œä¸­...</p>
        </div>
        """
        
        time.sleep(2)  # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        
        # æŒç»­ç›‘æ§è®­ç»ƒè¿›åº¦
        while trainer.is_training():
            return_code = trainer.check_process_status()
            if return_code is not None:
                break
            
            progress, status_msg = trainer.get_training_progress()
            log_text = trainer.get_training_logs(max_lines=10)
            
            # å¦‚æœæ—¥å¿—æ–‡ä»¶è¿˜æœªç”Ÿæˆï¼Œæ˜¾ç¤ºå‹å¥½æç¤º
            if "è®­ç»ƒå°šæœªå¼€å§‹æˆ–æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨" in log_text or "æš‚æ— è®­ç»ƒæ—¥å¿—" in log_text:
                yield gr.update(value=0, visible=True), f"""
                <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                    <h4>â³ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œæ•°æ®...</h4>
                    <p><strong>åŸºç¡€æ¨¡å‹:</strong> {base_model}</p>
                    <p><strong>CPT Checkpoint:</strong> {cpt_model}</p>
                    <p><strong>æ•°æ®é›†:</strong> {dataset}</p>
                    <p><strong>çŠ¶æ€:</strong> è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–</p>
                    <p>ğŸ’¡ <strong>æç¤º:</strong> è¯¦ç»†æ—¥å¿—æ­£åœ¨ç»ˆç«¯çª—å£å®æ—¶è¾“å‡º</p>
                </div>
                """
            else:
                yield gr.update(value=progress, visible=True), f"""
                <div style="background-color: #d1ecf1; padding: 10px; border-radius: 5px;">
                    <h4>â³ {status_msg}</h4>
                    {log_text}
                </div>
                """
            
            time.sleep(2)
        
        # è®­ç»ƒå®Œæˆ
        return_code = trainer.check_process_status()
        if return_code == 0:
            final_log = trainer.get_training_logs(max_lines=20)
            yield gr.update(value=100, visible=True), f"""
            <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
                <h4>âœ… SFTè®­ç»ƒå®Œæˆ</h4>
                {final_log}
            </div>
            """
        else:
            yield gr.update(visible=False), f"""
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¤±è´¥</h4>
                <p>é€€å‡ºç : {return_code}</p>
            </div>
            """
    
    def stop_sft_training():
        """åœæ­¢SFTè®­ç»ƒ"""
        trainer = get_trainer()
        success = trainer.stop_training()
        
        if success:
            return gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>â¹ï¸ è®­ç»ƒå·²åœæ­¢</p>
            </div>
            """
        else:
            return gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <p>âŒ æ²¡æœ‰è¿è¡Œä¸­çš„è®­ç»ƒä»»åŠ¡</p>
            </div>
            """
    
    def start_dpo_training(base_model, sft_model, dataset, output, beta, ftx, epochs, lr, batch_size, grad_acc, max_len, save_steps,
                          lora_rank, lora_alpha, lora_dropout):
        """å¯åŠ¨DPOè®­ç»ƒï¼ˆä½¿ç”¨ generator æŒç»­ç›‘æ§è¿›åº¦ï¼‰"""
        import time
        trainer = get_trainer()
        
        if trainer.is_training():
            yield gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>âš ï¸ å·²æœ‰è®­ç»ƒä»»åŠ¡åœ¨è¿è¡Œä¸­</p>
            </div>
            """
            return
        
        # éªŒè¯å¿…å¡«é¡¹
        if not sft_model:
            yield gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ é…ç½®é”™è¯¯</h4>
                <p>å¿…é¡»é€‰æ‹©SFT Checkpoint</p>
            </div>
            """
            return
        
        config = {
            'stage': 'dpo',
            'model_name_or_path': base_model,  # åŸºç¡€æ¨¡å‹
            'adapter_name_or_path': sft_model,  # SFT checkpoint
            'dataset': dataset,
            'dataset_dir': 'data/llmops',
            'template': 'qwen',  # ä¸SFTä¿æŒä¸€è‡´
            'output_dir': output,
            'num_train_epochs': int(epochs),
            'learning_rate': float(lr),
            'per_device_train_batch_size': int(batch_size),
            'gradient_accumulation_steps': int(grad_acc),
            'cutoff_len': int(max_len),
            'save_steps': int(save_steps),
            'finetuning_type': 'lora',
            'lora_rank': int(lora_rank),
            'lora_alpha': int(lora_alpha),
            'lora_dropout': float(lora_dropout),
            'pref_beta': float(beta),
            'pref_ftx': float(ftx),
            'logging_steps': 10,
        }
        
        print(f"ğŸš€ å‡†å¤‡å¯åŠ¨DPOè®­ç»ƒ: {config}")
        success = trainer.start_training(config)
        print(f"è®­ç»ƒå¯åŠ¨ç»“æœ: {success}")
        
        if not success:
            yield gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥</h4>
                <p>è¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ</p>
            </div>
            """
            return
        
        # æ˜¾ç¤ºå¯åŠ¨æˆåŠŸ
        yield gr.update(value=0, visible=True), f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <h4>âœ… DPOè®­ç»ƒå·²å¯åŠ¨</h4>
            <p><strong>åŸºç¡€æ¨¡å‹:</strong> {base_model}</p>
            <p><strong>SFT Checkpoint:</strong> {sft_model}</p>
            <p><strong>Beta:</strong> {beta}</p>
            <p><strong>æ•°æ®é›†:</strong> {dataset}</p>
            <p><strong>è¾“å‡º:</strong> {output}</p>
            <p>è®­ç»ƒè¿›è¡Œä¸­...</p>
        </div>
        """
        
        time.sleep(1)  # ç­‰å¾…è¿›ç¨‹å¯åŠ¨
        
        # æŒç»­ç›‘æ§è®­ç»ƒè¿›åº¦
        while trainer.is_training():
            return_code = trainer.check_process_status()
            if return_code is not None:
                break
            
            progress, status_msg = trainer.get_training_progress()
            log_text = trainer.get_training_logs(max_lines=10)
            
            yield gr.update(value=progress, visible=True), f"""
            <div style="background-color: #d1ecf1; padding: 10px; border-radius: 5px;">
                <h4>â³ {status_msg}</h4>
                {log_text}
            </div>
            """
            
            time.sleep(2)
        
        # è®­ç»ƒå®Œæˆ
        return_code = trainer.check_process_status()
        if return_code == 0:
            final_log = trainer.get_training_logs(max_lines=20)
            yield gr.update(value=100, visible=True), f"""
            <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
                <h4>âœ… DPOè®­ç»ƒå®Œæˆ</h4>
                {final_log}
            </div>
            """
        else:
            yield gr.update(visible=False), f"""
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <h4>âŒ è®­ç»ƒå¤±è´¥</h4>
                <p>é€€å‡ºç : {return_code}</p>
            </div>
            """
    
    def stop_dpo_training():
        """åœæ­¢DPOè®­ç»ƒ"""
        trainer = get_trainer()
        success = trainer.stop_training()
        
        if success:
            return gr.update(visible=False), """
            <div style="background-color: #fff3cd; padding: 10px; border-radius: 5px;">
                <p>â¹ï¸ è®­ç»ƒå·²åœæ­¢</p>
            </div>
            """
        else:
            return gr.update(visible=False), """
            <div style="background-color: #f8d7da; padding: 10px; border-radius: 5px;">
                <p>âŒ æ²¡æœ‰è¿è¡Œä¸­çš„è®­ç»ƒä»»åŠ¡</p>
            </div>
            """
    
    # === DPO Tab äº‹ä»¶ ===
    def load_infer_model(model_path):
        """åŠ è½½æ¨ç†æ¨¡å‹ï¼ˆå€Ÿé‰´ LLaMA-Factory WebUIï¼‰"""
        if not model_path:
            yield "âŒ è¯·é€‰æ‹©æ¨¡å‹"
            return
        
        # ä»adapterè·¯å¾„ä¸­æå–base modelï¼ˆå‡è®¾éƒ½æ˜¯ç”¨Qwen2-0.5Bè®­ç»ƒçš„ï¼‰
        base_model = "Qwen/Qwen2-0.5B"
        
        # è°ƒç”¨InferenceModelçš„load_modelï¼ˆgeneratorï¼‰
        for msg in system.inference_model.load_model(
            base_model=base_model,
            adapter_path=model_path,
            template="qwen"
        ):
            yield msg
    
    def unload_infer_model():
        """å¸è½½æ¨ç†æ¨¡å‹"""
        for msg in system.inference_model.unload_model():
            yield msg
    
    def refresh_inference_models():
        """åˆ·æ–°å¯ç”¨çš„æ¨ç†æ¨¡å‹åˆ—è¡¨"""
        sft_models = get_trained_models("sft")
        dpo_models = get_trained_models("dpo")
        all_models = sft_models + dpo_models
        return gr.update(choices=all_models)
    
    def generate_ab_responses(query, model, temp_a, temp_b):
        """ç”ŸæˆABå¯¹æ¯”å›ç­”ï¼ˆä½¿ç”¨å®é™…æ¨¡å‹æ¨ç†ï¼‰"""
        if not query:
            return "", "", "", "", "<p style='color: red;'>è¯·è¾“å…¥é—®é¢˜</p>"
        
        if not system.inference_model.loaded:
            return "", "", "", "", "<p style='color: red;'>è¯·å…ˆåŠ è½½æ¨¡å‹</p>"
        
        try:
            # ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ï¼Œä½†ä¸åŒçš„temperatureç”Ÿæˆä¸¤ä¸ªå›ç­”
            print(f"ç”ŸæˆA (temp={temp_a})...")
            response_a = system.inference_model.generate_once(
                prompt=query,
                temperature=temp_a,
                max_new_tokens=150
            )
            
            print(f"ç”ŸæˆB (temp={temp_b})...")
            response_b = system.inference_model.generate_once(
                prompt=query,
                temperature=temp_b,
                max_new_tokens=150
            )
            
            # éšæœºæ‰“ä¹±A/Bä½ç½®ï¼Œé¿å…ä½ç½®åè§
            import random
            responses = [
                (f"Temperature {temp_a:.1f}", response_a),
                (f"Temperature {temp_b:.1f}", response_b)
            ]
            random.shuffle(responses)
            
            # ä¿å­˜å½“å‰é—®é¢˜å’Œå›ç­”ï¼Œç”¨äºæŠ•ç¥¨æ—¶è®°å½•
            system.current_query = query
            system.current_model = model
            system.current_responses = {
                "A": {"label": responses[0][0], "response": responses[0][1]},
                "B": {"label": responses[1][0], "response": responses[1][1]}
            }
            
            return (
                responses[0][0], responses[0][1],
                responses[1][0], responses[1][1],
                f"<p>âœ… å·²ç”Ÿæˆå¯¹æ¯”ï¼ˆæ¨¡å‹: {model}ï¼‰ã€‚è¯·é€‰æ‹©ä½ è®¤ä¸ºæ›´å¥½çš„å›ç­”</p>"
            )
        except Exception as e:
            return "", "", "", "", f"<p style='color: red;'>âŒ ç”Ÿæˆå¤±è´¥: {str(e)}</p>"
    
    def vote_for_a():
        if not system.current_query:
            return "<p style='color: red;'>è¯·å…ˆç”Ÿæˆå¯¹æ¯”</p>"
        
        system.pref_collector.add_preference(
            prompt=system.current_query,
            chosen=system.current_responses["A"]["response"],
            rejected=system.current_responses["B"]["response"],
            metadata={
                "chosen_model": system.current_responses["A"]["label"],
                "rejected_model": system.current_responses["B"]["label"],
                "vote_time": datetime.now().isoformat()
            }
        )
        
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… åå¥½å·²è®°å½•ï¼šé€‰æ‹©äº† <strong>{system.current_responses["A"]["label"]}</strong></p>
            <p>ğŸ’¾ æ•°æ®å·²å†™å…¥ prefs.jsonl</p>
        </div>
        """
    
    def vote_for_b():
        if not system.current_query:
            return "<p style='color: red;'>è¯·å…ˆç”Ÿæˆå¯¹æ¯”</p>"
        
        system.pref_collector.add_preference(
            prompt=system.current_query,
            chosen=system.current_responses["B"]["response"],
            rejected=system.current_responses["A"]["response"],
            metadata={
                "chosen_model": system.current_responses["B"]["label"],
                "rejected_model": system.current_responses["A"]["label"],
                "vote_time": datetime.now().isoformat()
            }
        )
        
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… åå¥½å·²è®°å½•ï¼šé€‰æ‹©äº† <strong>{system.current_responses["B"]["label"]}</strong></p>
            <p>ğŸ’¾ æ•°æ®å·²å†™å…¥ prefs.jsonl</p>
        </div>
        """
    
    def view_preferences():
        stats = system.pref_collector.get_statistics()
        prefs = system.pref_collector.get_all_preferences()
        
        html = f"""
        <div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px;">
            <h4>ğŸ“Š åå¥½æ•°æ®ç»Ÿè®¡</h4>
            <ul>
                <li><strong>æ€»æ•°é‡:</strong> {stats['total_preferences']}</li>
                <li><strong>æ•°æ®æ–‡ä»¶:</strong> <code>{stats['data_file']}</code></li>
            </ul>
            
            <h5>æœ€è¿‘ 3 æ¡:</h5>
        """
        
        for pref in prefs[-3:][::-1]:
            html += f"""
            <div style="border: 1px solid #ddd; margin: 5px 0; padding: 8px; border-radius: 5px;">
                <p><strong>é—®é¢˜:</strong> {pref['prompt'][:50]}...</p>
                <p style="color: green;"><strong>âœ“ åå¥½:</strong> {pref['chosen'][:60]}...</p>
            </div>
            """
        
        html += "</div>"
        return html
    
    def export_preferences():
        filepath = system.pref_collector.export_for_dpo()
        stats = system.pref_collector.get_statistics()
        # è·å–æ›´æ–°åçš„DPOæ•°æ®é›†åˆ—è¡¨
        datasets = get_available_datasets("dpo")
        return f"""
        <div style="background-color: #d4edda; padding: 10px; border-radius: 5px;">
            <p>âœ… DPOæ•°æ®é›†å·²å¯¼å‡º: <code>{filepath}</code></p>
            <p><strong>æ€»æ•°é‡:</strong> {stats['total_preferences']}</p>
            <p>ğŸ’¡ æ•°æ®é›†å·²è‡ªåŠ¨æ³¨å†Œï¼Œå¯åœ¨è®­ç»ƒé…ç½®ä¸­é€‰æ‹©</p>
        </div>
        """, gr.update(choices=datasets, value="prefs_data")
    
    # ç»‘å®šæ‰€æœ‰äº‹ä»¶
    load_corpus_btn.click(load_corpus, inputs=[corpus_limit], outputs=[corpus_output])
    process_corpus_btn.click(process_corpus, outputs=[corpus_output, corpus_stats])
    save_corpus_btn.click(save_corpus, outputs=[corpus_output, cpt_dataset])
    
    generate_instruct_btn.click(generate_instructions, inputs=[instruct_count], outputs=[instruct_output, instruct_stats])
    save_instruct_btn.click(save_instructions, outputs=[instruct_output, sft_dataset])
    
    # CPT è®­ç»ƒäº‹ä»¶ç»‘å®š
    cpt_start_btn.click(
        start_cpt_training,
        inputs=[cpt_model, cpt_dataset, cpt_output, cpt_epochs, cpt_lr, cpt_batch_size, cpt_grad_acc, 
                cpt_max_len, cpt_save_steps, cpt_lora_rank, cpt_lora_alpha, cpt_lora_dropout],
        outputs=[cpt_progress, cpt_status]
    )
    cpt_stop_btn.click(stop_cpt_training, outputs=[cpt_progress, cpt_status])
    
    # SFT è®­ç»ƒäº‹ä»¶ç»‘å®š
    def refresh_cpt_models():
        """åˆ·æ–°CPTæ¨¡å‹åˆ—è¡¨ï¼ˆä¾›SFTé˜¶æ®µä½¿ç”¨ï¼‰"""
        models = get_trained_models("cpt")
        return gr.update(choices=models, value=models[0] if models else None)
    
    def refresh_dpo_models():
        """åˆ·æ–°SFTå’ŒDPOæ¨¡å‹åˆ—è¡¨ï¼ˆä¾›DPOé˜¶æ®µä½¿ç”¨ï¼‰"""
        sft_models = get_trained_models("sft")
        dpo_models = get_trained_models("dpo")
        # åˆå¹¶SFTå’ŒDPOæ¨¡å‹
        models = sft_models + dpo_models
        return gr.update(choices=models, value=models[0] if models else None)
    
    sft_refresh_models.click(refresh_cpt_models, outputs=[sft_cpt_model])
    
    sft_start_btn.click(
        start_sft_training,
        inputs=[sft_base_model, sft_cpt_model, sft_dataset, sft_output, sft_template, sft_epochs, sft_lr, sft_batch_size, sft_grad_acc,
                sft_max_len, sft_save_steps, sft_lora_rank, sft_lora_alpha, sft_lora_dropout],
        outputs=[sft_progress, sft_status]
    )
    sft_stop_btn.click(stop_sft_training, outputs=[sft_progress, sft_status])
    
    # DPO è®­ç»ƒäº‹ä»¶ç»‘å®š
    dpo_refresh_models.click(refresh_dpo_models, outputs=[dpo_sft_model])
    
    dpo_start_btn.click(
        start_dpo_training,
        inputs=[dpo_base_model, dpo_sft_model, dpo_dataset, dpo_output, dpo_beta, dpo_ftx, dpo_epochs, dpo_lr, dpo_batch_size, 
                dpo_grad_acc, dpo_max_len, dpo_save_steps, dpo_lora_rank, dpo_lora_alpha, dpo_lora_dropout],
        outputs=[dpo_progress, dpo_status]
    )
    dpo_stop_btn.click(stop_dpo_training, outputs=[dpo_progress, dpo_status])
    
    # æ¨ç†æœåŠ¡äº‹ä»¶ç»‘å®š
    def refresh_inference_models():
        """åˆ·æ–°å¯ç”¨äºæ¨ç†çš„æ¨¡å‹åˆ—è¡¨ï¼ˆSFTå’ŒDPOï¼‰"""
        sft_models = get_trained_models("sft")
        dpo_models = get_trained_models("dpo")
        models = sft_models + dpo_models
        return gr.update(choices=models, value=models[0] if models else None)
    
    # æ¨ç†æ¨¡å‹åŠ è½½/å¸è½½äº‹ä»¶ï¼ˆå€Ÿé‰´ LLaMA-Factory WebUIï¼‰
    infer_refresh.click(refresh_inference_models, outputs=[infer_model, ab_model])
    load_model_btn.click(load_infer_model, inputs=[infer_model], outputs=[infer_status])
    unload_model_btn.click(unload_infer_model, outputs=[infer_status])
    
    # ABæµ‹è¯•äº‹ä»¶
    ab_refresh_model.click(refresh_inference_models, outputs=[ab_model])
    ab_generate_btn.click(
        generate_ab_responses,
        inputs=[ab_query, ab_model, ab_temperature_a, ab_temperature_b],
        outputs=[response_a_label, response_a, response_b_label, response_b, ab_result]
    )
    
    vote_a_btn.click(vote_for_a, outputs=[ab_result])
    vote_b_btn.click(vote_for_b, outputs=[ab_result])
    
    view_prefs_btn.click(view_preferences, outputs=[prefs_stats])
    export_prefs_btn.click(export_preferences, outputs=[prefs_stats, dpo_dataset])
    
    # è¿”å›è®­ç»ƒå¼•æ“ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ resumeï¼‰
    # ä¼˜å…ˆè¿”å› CPT å¼•æ“ï¼Œå› ä¸ºå®ƒæ˜¯ç¬¬ä¸€ä¸ªé˜¶æ®µ
    return train_engines.get('cpt')
