#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GUI-Agent æœåŠ¡ - åŸºäº OSWorld æ¶æ„çš„æ¡Œé¢è‡ªåŠ¨åŒ–ä»£ç†
å‚è€ƒï¼šhttps://github.com/xlang-ai/OSWorld
"""

import os
import base64
import json
import time
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from io import BytesIO
from PIL import Image, ImageGrab, ImageDraw, ImageFont
import subprocess
import logging

logger = logging.getLogger(__name__)

# å¯¼å…¥ accessibility tree æ”¯æŒ
try:
    from .accessibility_tree import get_accessibility_tree, is_accessibility_available
    ACCESSIBILITY_AVAILABLE = is_accessibility_available()
except ImportError:
    ACCESSIBILITY_AVAILABLE = False
    get_accessibility_tree = None


def annotate_screenshot_with_coordinates(screenshot_bytes: bytes, screen_width: int, screen_height: int, enable_grid: bool = True) -> bytes:
    """
    åœ¨æˆªå›¾ä¸Šæ ‡æ³¨åæ ‡ä¿¡æ¯ï¼Œå¸®åŠ© VLM æ›´å¥½åœ°ç†è§£åæ ‡ç³»ç»Ÿ
    
    ç­–ç•¥ï¼šå…ˆå°†æˆªå›¾ç¼©æ”¾åˆ° PyAutoGUI çš„é€»è¾‘åˆ†è¾¨ç‡ï¼Œå†è¿›è¡Œæ ‡æ³¨
    è¿™æ · VLM çœ‹åˆ°çš„å›¾åƒåˆ†è¾¨ç‡ä¸åæ ‡ç³»ç»Ÿå®Œå…¨ä¸€è‡´ï¼ˆ1:1 å¯¹åº”ï¼‰
    
    æ ‡æ³¨å†…å®¹ï¼š
    - å››ä¸ªè§’çš„åæ ‡
    - ä¸­å¿ƒç‚¹åæ ‡
    - å¯é€‰ï¼šç½‘æ ¼è¾…åŠ©çº¿å’Œäº¤ç‚¹åæ ‡ï¼ˆå¢å¼ºæ¨¡å¼ï¼‰
    - é†’ç›®ä½†ä¸å¹²æ‰°çš„æ ·å¼
    
    Args:
        screenshot_bytes: åŸå§‹æˆªå›¾çš„å­—èŠ‚æ•°æ®
        screen_width: å±å¹•é€»è¾‘å®½åº¦ï¼ˆPyAutoGUI å¯æ§åŒºåŸŸå®½åº¦ï¼‰
        screen_height: å±å¹•é€»è¾‘é«˜åº¦ï¼ˆPyAutoGUI å¯æ§åŒºåŸŸé«˜åº¦ï¼‰
        enable_grid: æ˜¯å¦å¯ç”¨ç½‘æ ¼è¾…åŠ©çº¿ï¼ˆé»˜è®¤ Trueï¼‰
    
    Returns:
        æ ‡æ³¨åçš„æˆªå›¾å­—èŠ‚æ•°æ®ï¼ˆå·²ç¼©æ”¾åˆ°é€»è¾‘åˆ†è¾¨ç‡ï¼‰
    """
    try:
        # 1. æ‰“å¼€åŸå§‹æˆªå›¾
        img = Image.open(BytesIO(screenshot_bytes))
        original_width, original_height = img.size
        
        # 2. å°†æˆªå›¾ç¼©æ”¾åˆ° PyAutoGUI çš„é€»è¾‘åˆ†è¾¨ç‡
        # è¿™æ · VLM çœ‹åˆ°çš„å›¾åƒå°ºå¯¸ = åæ ‡ç³»ç»Ÿå°ºå¯¸
        if (original_width, original_height) != (screen_width, screen_height):
            print(f"ğŸ”„ ç¼©æ”¾æˆªå›¾: {original_width}x{original_height} -> {screen_width}x{screen_height}")
            img = img.resize((screen_width, screen_height), Image.LANCZOS)
        else:
            print(f"âœ“ æˆªå›¾åˆ†è¾¨ç‡å·²åŒ¹é…é€»è¾‘åˆ†è¾¨ç‡: {screen_width}x{screen_height}")
        
        # 3. åˆ›å»ºç»˜å›¾å¯¹è±¡
        draw = ImageDraw.Draw(img, 'RGBA')
        
        # 4. å­—ä½“è®¾ç½®ï¼ˆå›ºå®šå¤§å°ï¼Œå› ä¸ºå›¾åƒå·²ç»æ˜¯é€»è¾‘åˆ†è¾¨ç‡ï¼‰
        try:
            # macOS ç³»ç»Ÿå­—ä½“
            font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 24)
            font_small = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 18)
        except:
            try:
                # Linux å­—ä½“
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 24)
                font_small = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 18)
            except:
                # é™çº§åˆ°é»˜è®¤å­—ä½“
                font = ImageFont.load_default()
                font_small = font
        
        # 5. æ ‡æ³¨ç‚¹é…ç½®ï¼ˆç›´æ¥ä½¿ç”¨é€»è¾‘åæ ‡ï¼Œæ— éœ€è½¬æ¢ï¼‰
        annotations = [
            # (xåæ ‡, yåæ ‡, æ ‡ç­¾, ä½ç½®)
            (0, 0, "(0, 0)", "topleft"),
            (screen_width - 1, 0, f"({screen_width-1}, 0)", "topright"),
            (0, screen_height - 1, f"(0, {screen_height-1})", "bottomleft"),
            (screen_width - 1, screen_height - 1, f"({screen_width-1}, {screen_height-1})", "bottomright"),
            (screen_width // 2, screen_height // 2, f"({screen_width//2}, {screen_height//2})", "center"),
        ]
        
        # 6. ç»˜åˆ¶æ ‡æ³¨
        for coord_x, coord_y, label, position in annotations:
            # å›¾åƒå·²ç¼©æ”¾åˆ°é€»è¾‘åˆ†è¾¨ç‡ï¼Œç›´æ¥ä½¿ç”¨åæ ‡
            x, y = coord_x, coord_y
            
            # ç»˜åˆ¶åœ†ç‚¹ï¼ˆé†’ç›®çš„çº¢è‰²ï¼‰
            point_radius = 8
            draw.ellipse(
                [x - point_radius, y - point_radius,
                 x + point_radius, y + point_radius],
                fill=(255, 0, 0, 200),  # åŠé€æ˜çº¢è‰²
                outline=(255, 255, 255, 255),  # ç™½è‰²è¾¹æ¡†
                width=2
            )
            
            # è®¡ç®—æ–‡æœ¬ä½ç½®å’ŒèƒŒæ™¯
            # ä½¿ç”¨ textbbox è·å–æ–‡æœ¬è¾¹ç•Œï¼ˆPIL 10.0.0+ï¼‰
            try:
                bbox = draw.textbbox((0, 0), label, font=font_small)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
            except:
                # é™çº§æ–¹æ¡ˆ
                text_width = len(label) * 10
                text_height = 16
            
            # æ ¹æ®ä½ç½®è°ƒæ•´æ–‡æœ¬åæ ‡
            padding = 5
            if position == "topleft":
                text_x = x + point_radius + padding
                text_y = y + point_radius + padding
            elif position == "topright":
                text_x = x - point_radius - padding - text_width
                text_y = y + point_radius + padding
            elif position == "bottomleft":
                text_x = x + point_radius + padding
                text_y = y - point_radius - padding - text_height
            elif position == "bottomright":
                text_x = x - point_radius - padding - text_width
                text_y = y - point_radius - padding - text_height
            elif position == "center":
                text_x = x + point_radius + padding
                text_y = y - text_height // 2
            
            # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
            bg_padding = 4
            draw.rectangle(
                [text_x - bg_padding, text_y - bg_padding,
                 text_x + text_width + bg_padding, text_y + text_height + bg_padding],
                fill=(0, 0, 0, 180)  # åŠé€æ˜é»‘è‰²èƒŒæ™¯
            )
            
            # ç»˜åˆ¶æ–‡å­—ï¼ˆç™½è‰²ï¼‰
            draw.text((text_x, text_y), label, fill=(255, 255, 255, 255), font=font_small)
        
        # 7. ç»˜åˆ¶ç½‘æ ¼è¾…åŠ©çº¿ï¼ˆå¯é€‰ï¼‰
        if enable_grid:
            # æ ¹æ®å±å¹•å°ºå¯¸è‡ªé€‚åº”ç½‘æ ¼é—´è·
            if max(screen_width, screen_height) < 1000:
                grid_spacing = 100
            elif max(screen_width, screen_height) < 2000:
                grid_spacing = 200
            else:
                grid_spacing = 300
            
            print(f"ğŸ“ ç»˜åˆ¶ç½‘æ ¼è¾…åŠ©çº¿: é—´è· {grid_spacing}px")
            
            # å­—ä½“è®¾ç½®ï¼ˆå°å·å­—ä½“ç”¨äºç½‘æ ¼åæ ‡ï¼‰
            try:
                font_tiny = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 12)
            except:
                try:
                    font_tiny = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
                except:
                    font_tiny = font_small
            
            # ç»˜åˆ¶å‚ç›´ç½‘æ ¼çº¿
            for x in range(0, screen_width + 1, grid_spacing):
                if x == 0 or x >= screen_width:  # è·³è¿‡è¾¹ç•Œï¼Œå·²æœ‰æ ‡æ³¨
                    continue
                # åŠé€æ˜ç°è‰²è™šçº¿
                for y in range(0, screen_height, 10):
                    draw.line([(x, y), (x, min(y + 5, screen_height))], 
                             fill=(128, 128, 128, 80), width=1)
            
            # ç»˜åˆ¶æ°´å¹³ç½‘æ ¼çº¿
            for y in range(0, screen_height + 1, grid_spacing):
                if y == 0 or y >= screen_height:  # è·³è¿‡è¾¹ç•Œï¼Œå·²æœ‰æ ‡æ³¨
                    continue
                # åŠé€æ˜ç°è‰²è™šçº¿
                for x in range(0, screen_width, 10):
                    draw.line([(x, y), (min(x + 5, screen_width), y)], 
                             fill=(128, 128, 128, 80), width=1)
            
            # åœ¨ç½‘æ ¼äº¤ç‚¹æ ‡æ³¨åæ ‡ï¼ˆåªæ ‡æ³¨å…³é”®äº¤ç‚¹ï¼Œé¿å…è¿‡äºå¯†é›†ï¼‰
            for x in range(grid_spacing, screen_width, grid_spacing):
                for y in range(grid_spacing, screen_height, grid_spacing):
                    # è·³è¿‡å·²ç»æ ‡æ³¨çš„ä¸­å¿ƒç‚¹é™„è¿‘
                    if abs(x - screen_width // 2) < grid_spacing // 2 and abs(y - screen_height // 2) < grid_spacing // 2:
                        continue
                    
                    coord_label = f"({x},{y})"
                    
                    try:
                        bbox = draw.textbbox((0, 0), coord_label, font=font_tiny)
                        label_w = bbox[2] - bbox[0]
                        label_h = bbox[3] - bbox[1]
                    except:
                        label_w = len(coord_label) * 6
                        label_h = 12
                    
                    # åŠé€æ˜èƒŒæ™¯
                    bg_pad = 2
                    draw.rectangle(
                        [x - label_w // 2 - bg_pad, y - label_h // 2 - bg_pad,
                         x + label_w // 2 + bg_pad, y + label_h // 2 + bg_pad],
                        fill=(0, 0, 0, 120)
                    )
                    
                    # ç»˜åˆ¶åæ ‡æ–‡å­—
                    draw.text((x - label_w // 2, y - label_h // 2), 
                             coord_label, fill=(200, 200, 200, 200), font=font_tiny)
        
        # 8. åœ¨é¡¶éƒ¨ä¸­å¤®æ·»åŠ åˆ†è¾¨ç‡ä¿¡æ¯
        resolution_label = f"Screen: {screen_width}Ã—{screen_height}"
        if enable_grid:
            resolution_label += " [Grid Enabled]"
        
        try:
            bbox = draw.textbbox((0, 0), resolution_label, font=font)
            res_text_width = bbox[2] - bbox[0]
            res_text_height = bbox[3] - bbox[1]
        except:
            res_text_width = len(resolution_label) * 12
            res_text_height = 20
        
        res_x = (img.width - res_text_width) // 2
        res_y = 10
        
        # èƒŒæ™¯
        bg_padding = 8
        draw.rectangle(
            [res_x - bg_padding, res_y - bg_padding,
             res_x + res_text_width + bg_padding, res_y + res_text_height + bg_padding],
            fill=(0, 0, 0, 200)
        )
        
        # æ–‡å­—
        draw.text((res_x, res_y), resolution_label, fill=(0, 255, 0, 255), font=font)
        
        # 9. è½¬æ¢å›å­—èŠ‚
        output_buffer = BytesIO()
        img.save(output_buffer, format='PNG')
        return output_buffer.getvalue()
        
    except Exception as e:
        print(f"âš ï¸  æ ‡æ³¨æˆªå›¾å¤±è´¥: {e}")
        # å¦‚æœæ ‡æ³¨å¤±è´¥ï¼Œè¿”å›åŸå§‹æˆªå›¾
        return screenshot_bytes


class SimpleDesktopEnv:
    """
    ç®€åŒ–ç‰ˆæ¡Œé¢ç¯å¢ƒ - åŸºäº OSWorld DesktopEnv çš„æ ¸å¿ƒæ€æƒ³
    æ”¯æŒæœ¬åœ° macOS/Ubuntu ç¯å¢ƒçš„å±å¹•æ•è·å’ŒåŠ¨ä½œæ‰§è¡Œ
    """
    
    def __init__(
        self,
        provider_name: str = "local",
        os_type: str = "macOS",
        action_space: str = "pyautogui",
        screen_size: Tuple[int, int] = (1920, 1080),
        headless: bool = False,
        require_a11y_tree: bool = False,
        require_terminal: bool = False
    ):
        self.provider_name = provider_name
        self.os_type = os_type
        self.action_space = action_space
        self.screen_size = screen_size
        self.headless = headless
        self.require_a11y_tree = require_a11y_tree
        self.require_terminal = require_terminal
        
        self.current_task = None
        self.step_count = 0
        self.max_steps = 50
        self.history = []
        
        # åˆå§‹åŒ– PyAutoGUIï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._init_controller()
    
    def _init_controller(self):
        """åˆå§‹åŒ–æ§åˆ¶å™¨"""
        try:
            import pyautogui
            self.controller = pyautogui
            # è®¾ç½®å®‰å…¨æš‚åœæ—¶é—´
            self.controller.PAUSE = 0.5
            # ç¦ç”¨ fail-safeï¼ˆåœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥ä¿ç•™ï¼‰
            # self.controller.FAILSAFE = True
            print(f"âœ… PyAutoGUI æ§åˆ¶å™¨åˆå§‹åŒ–æˆåŠŸ - å±å¹•å°ºå¯¸: {self.controller.size()}")
        except ImportError:
            print("âš ï¸  PyAutoGUI æœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")
            self.controller = None
    
    def reset(self, task_config: Optional[Dict] = None) -> Dict:
        """
        é‡ç½®ç¯å¢ƒå¹¶åŠ è½½ä»»åŠ¡é…ç½®
        
        Args:
            task_config: ä»»åŠ¡é…ç½®å­—å…¸ï¼ŒåŒ…å« instruction, evaluator, config ç­‰
        
        Returns:
            åˆå§‹è§‚å¯Ÿå­—å…¸
        """
        self.current_task = task_config or {}
        self.step_count = 0
        self.history = []
        
        # æ•è·åˆå§‹å±å¹•æˆªå›¾
        obs = self._get_observation()
        obs['instruction'] = self.current_task.get('instruction', 'æœªæŒ‡å®šä»»åŠ¡')
        
        print(f"ğŸ”„ ç¯å¢ƒå·²é‡ç½® - ä»»åŠ¡: {obs['instruction']}")
        return obs
    
    def _get_observation(self) -> Dict:
        """
        è·å–å½“å‰è§‚å¯Ÿ
        å®Œå…¨åŸºäº OSWorld çš„ _get_obs() å®ç°
        
        Returns:
            åŒ…å«æˆªå›¾ã€accessibility tree å’Œå…¶ä»–ä¿¡æ¯çš„è§‚å¯Ÿå­—å…¸
        """
        obs = {
            'screenshot': None,
            'screenshot_path': None,
            'accessibility_tree': None,
            'terminal': None,
            'timestamp': datetime.now().isoformat()
        }
        
        # æ•è·æˆªå›¾ï¼ˆä¸ OSWorld ä¸€è‡´ï¼‰
        try:
            screenshot = ImageGrab.grab()
            
            # ä¿å­˜ä¸º bytes
            buffer = BytesIO()
            screenshot.save(buffer, format='PNG')
            obs['screenshot'] = buffer.getvalue()
            
            # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
            screenshot_dir = Path('data/gui_agent/screenshots')
            screenshot_dir.mkdir(parents=True, exist_ok=True)
            screenshot_path = screenshot_dir / f'step_{self.step_count}_{int(time.time())}.png'
            screenshot.save(screenshot_path)
            obs['screenshot_path'] = str(screenshot_path)
            
        except Exception as e:
            print(f"âš ï¸  æˆªå›¾å¤±è´¥: {e}")
        
        # è·å– Accessibility Treeï¼ˆå¦‚æœå¯ç”¨ï¼Œä¸ OSWorld ä¸€è‡´ï¼‰
        # OSWorld: accessibility_tree = self.controller.get_accessibility_tree() if self.require_a11y_tree else None
        if self.require_a11y_tree and ACCESSIBILITY_AVAILABLE and get_accessibility_tree:
            try:
                # ä½¿ç”¨ OSWorld æ ‡å‡†æ·±åº¦ï¼ˆMAX_DEPTH=50ï¼Œåœ¨ accessibility_tree.py ä¸­å®šä¹‰ï¼‰
                obs['accessibility_tree'] = get_accessibility_tree(include_dock=False)
            except Exception as e:
                logger.debug(f"è·å– accessibility tree å¤±è´¥: {e}")
                obs['accessibility_tree'] = None
        else:
            obs['accessibility_tree'] = None
        
        # ç»ˆç«¯è¾“å‡ºï¼ˆå¯é€‰ï¼Œå½“å‰æœªå®ç°ï¼‰
        # obs['terminal'] = self.controller.get_terminal_output() if self.require_terminal else None
        
        return obs
    
    def step(self, action: str, pause: float = 2.0) -> Tuple[Dict, float, bool, Dict]:
        """
        æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œå¹¶è¿”å›æ–°çš„è§‚å¯Ÿ
        å®Œå…¨åŸºäº OSWorld çš„ step() å®ç°
        
        Args:
            action: è¦æ‰§è¡Œçš„åŠ¨ä½œå­—ç¬¦ä¸²ï¼ˆå¦‚ "pyautogui.click(100, 100)"ï¼‰
            pause: æ‰§è¡Œåç­‰å¾…æ—¶é—´ï¼ˆé»˜è®¤ 2.0 ç§’ï¼Œä¸ OSWorld ä¸€è‡´ï¼‰
        
        Returns:
            (observation, reward, done, info) å…ƒç»„
        """
        self.step_count += 1
        done = False
        reward = 0.0
        info = {'action': action, 'step': self.step_count}
        
        # å¤„ç†æ§åˆ¶ç¬¦
        if action in ["DONE", "FAIL", "WAIT"]:
            if action == "DONE":
                done = True
                reward = 1.0
                print(f"âœ… ä»»åŠ¡å®Œæˆ - æ­¥éª¤: {self.step_count}")
            elif action == "FAIL":
                done = True
                reward = 0.0
                print(f"âŒ ä»»åŠ¡å¤±è´¥ - æ­¥éª¤: {self.step_count}")
            elif action == "WAIT":
                print(f"â¸ï¸  ç­‰å¾… - æ­¥éª¤: {self.step_count}")
            
            obs = self._get_observation()
            return obs, reward, done, info
        
        # æ‰§è¡Œ PyAutoGUI åŠ¨ä½œ
        try:
            execution_result = self._execute_action(action)
            info['execution_result'] = execution_result
            
            # ç­‰å¾…ç•Œé¢å“åº”
            time.sleep(pause)
            
        except Exception as e:
            print(f"âš ï¸  åŠ¨ä½œæ‰§è¡Œå¤±è´¥: {e}")
            info['error'] = str(e)
        
        # è·å–æ–°è§‚å¯Ÿ
        obs = self._get_observation()
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
        if self.step_count >= self.max_steps:
            done = True
            print(f"â±ï¸  è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}")
        
        # è®°å½•å†å²
        self.history.append({
            'step': self.step_count,
            'action': action,
            'screenshot_path': obs.get('screenshot_path'),
            'info': info
        })
        
        return obs, reward, done, info
    
    def _execute_action(self, action: str) -> Dict:
        """
        æ‰§è¡Œå•ä¸ª PyAutoGUI åŠ¨ä½œ
        
        Args:
            action: PyAutoGUI å‘½ä»¤å­—ç¬¦ä¸²
        
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        if not self.controller:
            return {'status': 'error', 'message': 'PyAutoGUI ä¸å¯ç”¨'}
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ pyautogui å‘½ä»¤
        if not action.strip().startswith('pyautogui.'):
            return {'status': 'error', 'message': f'ä¸å®‰å…¨çš„å‘½ä»¤: {action}'}
        
        try:
            # åœ¨å®‰å…¨çš„å‘½åç©ºé—´ä¸­æ‰§è¡Œ
            namespace = {'pyautogui': self.controller}
            exec(action, namespace)
            
            return {'status': 'success', 'action': action}
            
        except Exception as e:
            return {'status': 'error', 'message': str(e), 'action': action}
    
    def get_history(self) -> List[Dict]:
        """è·å–æ‰§è¡Œå†å²"""
        return self.history
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        print(f"ğŸ”’ ç¯å¢ƒå·²å…³é—­ - æ€»æ­¥æ•°: {self.step_count}")


class SimplePromptAgent:
    """
    ç®€åŒ–ç‰ˆ Prompt Agent - åŸºäº OSWorld PromptAgent çš„æ ¸å¿ƒæ€æƒ³
    ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä»»åŠ¡æ¨ç†å’ŒåŠ¨ä½œè§„åˆ’
    """
    
    def __init__(
        self,
        model: str = "gpt-4o",
        action_space: str = "pyautogui",
        observation_type: str = "screenshot",
        max_trajectory_length: int = 3,
        temperature: float = 0.1,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        enable_thinking: bool = False,
        use_trajectory: bool = True
    ):
        self.model = model
        self.action_space = action_space
        self.observation_type = observation_type
        self.max_trajectory_length = max_trajectory_length
        self.temperature = temperature
        self.enable_thinking = enable_thinking
        self.use_trajectory = use_trajectory
        
        # API é…ç½®
        # å¦‚æœæ˜¯ Qwen æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨ DASHSCOPE_API_KEY
        if model.startswith("qwen") or model.startswith("qvq"):
            self.api_key = api_key or os.getenv('DASHSCOPE_API_KEY') or os.getenv('OPENAI_API_KEY')
            self.base_url = base_url or os.getenv('OPENAI_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
        else:
            self.api_key = api_key or os.getenv('OPENAI_API_KEY')
            self.base_url = base_url or os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        
        # å†å²è½¨è¿¹
        self.trajectory = []
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            print(f"âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {self.model}")
        except ImportError:
            print("âš ï¸  OpenAI åº“æœªå®‰è£…")
            self.client = None
        except Exception as e:
            print(f"âš ï¸  OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
    
    def predict(self, instruction: str, observation: Dict) -> Tuple[str, List[str]]:
        """
        åŸºäºå½“å‰è§‚å¯Ÿå’Œä»»åŠ¡æŒ‡ä»¤ï¼Œé¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œ
        
        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤
            observation: å½“å‰è§‚å¯Ÿï¼ˆåŒ…å«æˆªå›¾ï¼‰
        
        Returns:
            (æ€è€ƒè¿‡ç¨‹, åŠ¨ä½œåˆ—è¡¨) å…ƒç»„
        """
        if not self.client:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›ç¤ºä¾‹åŠ¨ä½œ
            return self._mock_prediction(instruction, observation)
        
        try:
            # æå–å±å¹•åˆ†è¾¨ç‡ - ä½¿ç”¨ PyAutoGUI é€»è¾‘åˆ†è¾¨ç‡è€Œä¸æ˜¯æˆªå›¾ç‰©ç†åˆ†è¾¨ç‡
            # é‡è¦ï¼šåœ¨ Retina/HiDPI æ˜¾ç¤ºå™¨ä¸Šï¼Œæˆªå›¾çš„ç‰©ç†åƒç´ æ˜¯é€»è¾‘åƒç´ çš„ 2 å€
            # ä½† PyAutoGUI çš„åæ ‡ç³»ç»Ÿä½¿ç”¨çš„æ˜¯é€»è¾‘åƒç´ ï¼Œæ‰€ä»¥å¿…é¡»ä½¿ç”¨ pyautogui.size()
            screen_size = None
            try:
                import pyautogui
                logical_size = pyautogui.size()
                screen_size = (logical_size.width, logical_size.height)
                print(f"ğŸ“ ä½¿ç”¨ PyAutoGUI é€»è¾‘å±å¹•å°ºå¯¸: {screen_size[0]}x{screen_size[1]}")
            except Exception as e:
                print(f"âš ï¸  æ— æ³•è·å– PyAutoGUI å±å¹•å°ºå¯¸: {e}")
                # é™çº§æ–¹æ¡ˆï¼šä»æˆªå›¾ä¸­æå–ï¼ˆå¯èƒ½åœ¨ HiDPI å±å¹•ä¸Šä¸å‡†ç¡®ï¼‰
                if 'screenshot' in observation:
                    try:
                        img = Image.open(BytesIO(observation['screenshot']))
                        screen_size = (img.width, img.height)
                        print(f"âš ï¸  é™çº§ä½¿ç”¨æˆªå›¾åˆ†è¾¨ç‡: {screen_size[0]}x{screen_size[1]} (å¯èƒ½åœ¨ Retina å±å¹•ä¸Šä¸å‡†ç¡®)")
                    except Exception as e2:
                        print(f"âš ï¸  ä¹Ÿæ— æ³•æå–æˆªå›¾åˆ†è¾¨ç‡: {e2}")
            
            # åœ¨æˆªå›¾ä¸Šæ ‡æ³¨åæ ‡ä¿¡æ¯ï¼ˆå¸®åŠ© VLM ç†è§£åæ ‡ç³»ç»Ÿï¼‰
            annotated_screenshot = observation['screenshot']
            if screen_size:
                print(f"ğŸ¯ åœ¨æˆªå›¾ä¸Šæ ‡æ³¨åæ ‡åŸºå‡†ç‚¹...")
                annotated_screenshot = annotate_screenshot_with_coordinates(
                    observation['screenshot'],
                    screen_size[0],
                    screen_size[1]
                )
                
                # ä¿å­˜æ ‡æ³¨åçš„æˆªå›¾åˆ°æ–‡ä»¶ï¼ˆè¦†ç›–åŸå§‹æˆªå›¾ï¼‰
                if observation.get('screenshot_path'):
                    try:
                        annotated_img = Image.open(BytesIO(annotated_screenshot))
                        annotated_img.save(observation['screenshot_path'])
                        print(f"ğŸ’¾ æ ‡æ³¨åçš„æˆªå›¾å·²ä¿å­˜: {observation['screenshot_path']}")
                    except Exception as e:
                        print(f"âš ï¸  ä¿å­˜æ ‡æ³¨æˆªå›¾å¤±è´¥: {e}")
            
            # ç¼–ç æˆªå›¾ä¸º base64ï¼ˆä½¿ç”¨æ ‡æ³¨åçš„æˆªå›¾ï¼‰
            screenshot_b64 = self._encode_screenshot(annotated_screenshot)
            
            # ä¿å­˜å½“å‰æˆªå›¾çš„ base64ï¼ˆç”¨äºåç»­ä½œä¸ºå†å²ï¼‰
            observation['screenshot_b64'] = screenshot_b64
            
            # è·å– Accessibility Treeï¼ˆå¦‚æœ observation ä¸­æ²¡æœ‰ï¼Œåˆ™è·å–æ–°çš„ï¼‰
            # ä¸ OSWorld ä¸€è‡´ï¼šå¦‚æœ step() å·²ç»è·å–äº†æ–°çš„ observationï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
            accessibility_tree = observation.get('accessibility_tree')
            
            # å¦‚æœ observation ä¸­æ²¡æœ‰ accessibility_treeï¼Œä¸”ç³»ç»Ÿæ”¯æŒï¼Œåˆ™å°è¯•è·å–
            if accessibility_tree is None:
                if ACCESSIBILITY_AVAILABLE and get_accessibility_tree:
                    try:
                        print(f"ğŸŒ² è·å– Accessibility Tree (ä½¿ç”¨ OSWorld æ ‡å‡†æ·±åº¦ MAX_DEPTH=50)...")
                        accessibility_tree = get_accessibility_tree(include_dock=False)
                        if accessibility_tree:
                            print(f"âœ… Accessibility Tree å·²è·å– ({len(accessibility_tree)} å­—ç¬¦)")
                        else:
                            print(f"âš ï¸  Accessibility Tree ä¸ºç©º")
                    except Exception as e:
                        print(f"âš ï¸  è·å– Accessibility Tree å¤±è´¥: {e}")
                        accessibility_tree = None
                else:
                    # ç³»ç»Ÿä¸æ”¯æŒæˆ–æ¨¡å—æœªå¯¼å…¥
                    if not ACCESSIBILITY_AVAILABLE:
                        print(f"â„¹ï¸  Accessibility Tree ä¸å¯ç”¨ (å½“å‰å¹³å°ä¸æ”¯æŒï¼Œä»…ä½¿ç”¨æˆªå›¾æ¨¡å¼)")
                    else:
                        print(f"â„¹ï¸  Accessibility Tree ä¸å¯ç”¨ (æ¨¡å—æœªå¯¼å…¥ï¼Œä»…ä½¿ç”¨æˆªå›¾æ¨¡å¼)")
            elif accessibility_tree:
                # observation ä¸­å·²æœ‰ accessibility_tree
                print(f"â„¹ï¸  ä½¿ç”¨ observation ä¸­çš„ Accessibility Tree ({len(accessibility_tree)} å­—ç¬¦)")
            
            # æ„é€ æ¶ˆæ¯
            messages = self._build_messages(instruction, screenshot_b64, screen_size, accessibility_tree)
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆQwen æ€è€ƒæ¨¡å¼æ¨èä½¿ç”¨æµå¼ï¼‰
            use_stream = (self.model.startswith("qwen") or self.model.startswith("qvq")) and self.enable_thinking
            
            if use_stream:
                # æµå¼è¾“å‡ºæ¨¡å¼ï¼ˆç”¨äºè·å–æ€è€ƒè¿‡ç¨‹ï¼‰
                reasoning_content = ""
                answer_content = ""
                is_answering = False
                
                # å‡†å¤‡å‚æ•°
                params = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature,
                    "max_tokens": 2000,
                    "stream": True
                }
                
                # å¦‚æœæ˜¯ Qwen æ¨¡å‹ä¸”å¯ç”¨æ€è€ƒï¼Œæ·»åŠ  enable_thinking å‚æ•°
                if self.model.startswith("qwen") and self.enable_thinking:
                    # æ³¨æ„ï¼šOpenAI å…¼å®¹ API å¯èƒ½ä¸æ”¯æŒç›´æ¥ä¼ é€’é¢å¤–å‚æ•°
                    # è¿™é‡Œæˆ‘ä»¬é€šè¿‡ extra_headers æˆ–å…¶ä»–æ–¹å¼ä¼ é€’
                    # å¦‚æœ API ä¸æ”¯æŒï¼Œåˆ™ä½¿ç”¨éæµå¼æ¨¡å¼
                    pass
                
                # è°ƒç”¨æµå¼ API
                stream = self.client.chat.completions.create(**params)
                
                print("\n" + "=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20 + "\n")
                
                for chunk in stream:
                    if not chunk.choices:
                        continue
                    
                    delta = chunk.choices[0].delta
                    
                    # å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆQwen ç‰¹æœ‰ï¼‰
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        print(delta.reasoning_content, end='', flush=True)
                        reasoning_content += delta.reasoning_content
                    else:
                        # å¼€å§‹å›å¤
                        if delta.content and not is_answering:
                            print("\n" + "=" * 20 + "å®Œæ•´å›å¤" + "=" * 20 + "\n")
                            is_answering = True
                        
                        # æ‰“å°å›å¤è¿‡ç¨‹
                        if delta.content:
                            print(delta.content, end='', flush=True)
                            answer_content += delta.content
                
                # åˆå¹¶æ€è€ƒè¿‡ç¨‹å’Œå›å¤
                if reasoning_content:
                    response_text = f"ã€æ€è€ƒè¿‡ç¨‹ã€‘\n{reasoning_content}\n\nã€æœ€ç»ˆå›å¤ã€‘\n{answer_content}"
                else:
                    response_text = answer_content
                
                # æ‰“å°æ¨¡å‹çš„å®Œæ•´å“åº”ï¼ˆæµå¼æ¨¡å¼ä¸‹ï¼‰
                print("\n" + "=" * 60)
                print("ğŸ“¥ æ¨¡å‹è¿”å›çš„å®Œæ•´ Response")
                print("=" * 60)
                print(response_text)
                print("=" * 60 + "\n")
                
            else:
                # éæµå¼è¾“å‡ºæ¨¡å¼
                params = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature,
                    "max_tokens": 2000
                }
                
                # è°ƒç”¨ VL æ¨¡å‹
                response = self.client.chat.completions.create(**params)
                
                # æå–å“åº”
                response_text = response.choices[0].message.content
                
                # æ‰“å°æ¨¡å‹çš„å®Œæ•´å“åº”
                print("\n" + "=" * 60)
                print("ğŸ“¥ æ¨¡å‹è¿”å›çš„ Response")
                print("=" * 60)
                print(response_text)
                print("=" * 60 + "\n")
            
            # è§£æåŠ¨ä½œ
            actions = self._parse_actions(response_text)
            
            # æ›´æ–°è½¨è¿¹
            self._update_trajectory(instruction, observation, response_text, actions)
            
            return response_text, actions
            
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"é”™è¯¯: {str(e)}", ["FAIL"]
    
    def _encode_screenshot(self, screenshot_bytes: bytes) -> str:
        """å°†æˆªå›¾ç¼–ç ä¸º base64"""
        if screenshot_bytes is None:
            return ""
        return base64.b64encode(screenshot_bytes).decode('utf-8')
    
    def _build_messages(self, instruction: str, screenshot_b64: str, screen_size: tuple = None, accessibility_tree: Optional[str] = None) -> List[Dict]:
        """æ„é€ å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯ - åŸºäº OSWorld å®˜æ–¹ Prompt"""
        # OSWorld å®˜æ–¹ System Prompt
        # æ ¹æ®æ˜¯å¦æœ‰ accessibility tree é€‰æ‹©ä¸åŒçš„æ¨¡å¼ï¼š
        # - æœ‰ a11y tree: SYS_PROMPT_IN_BOTH_OUT_CODE (screenshot + a11y tree)
        # - ä»…æˆªå›¾: SYS_PROMPT_IN_SCREENSHOT_OUT_CODE
        # å‚è€ƒ: https://github.com/xlang-ai/OSWorld/blob/main/mm_agents/prompts.py
        
        # åŠ¨æ€ç”Ÿæˆåæ ‡ç³»ç»Ÿå¢å¼ºè¯´æ˜
        coordinate_enhancement = ""
        if screen_size:
            width, height = screen_size
            coordinate_enhancement = f"""
IMPORTANT - Coordinate System Enhancement:
The screenshot has been annotated with 5 red coordinate reference points to help you locate elements accurately:
- Top-left corner: (0, 0)
- Top-right corner: ({width-1}, 0)
- Bottom-left corner: (0, {height-1})
- Bottom-right corner: ({width-1}, {height-1})
- Center point: ({width//2}, {height//2})
The screen resolution {width}x{height} is displayed at the top center of the screenshot. Please use these reference points to accurately estimate the coordinates of target elements.
"""
        
        # æ ¹æ®æ˜¯å¦æœ‰ accessibility tree æ„é€ ä¸åŒçš„è§‚å¯Ÿæè¿°
        if accessibility_tree:
            observation_description = """For each step, you will get an observation of the desktop by 1) a screenshot; and 2) accessibility tree, which is based on AT-SPI library (Linux) or AX API (macOS). And you will predict the action of the computer based on the screenshot and accessibility tree.

**CRITICAL: When accessibility tree is provided, you MUST:**
1. First locate the target UI element in the accessibility tree by its name, role, or description
2. Extract the EXACT coordinates from the element's cp:screencoord attribute (format: "(x, y)")
3. Use these EXACT coordinates in your pyautogui commands
4. DO NOT estimate coordinates from the screenshot - always use accessibility tree coordinates for precision

Example: If you see <button name="Later" cp:screencoord="(1408, 752)" cp:size="(60, 20)">, you MUST use pyautogui.click(1408, 752) - not an estimated position."""
        else:
            observation_description = "For each step, you will get an observation of an image, which is the screenshot of the computer screen and you will predict the action of the computer based on the image."
        
        system_prompt = f"""You are an agent which follow my instruction and perform desktop computer tasks as instructed.
You have good knowledge of computer and good internet connection and assume your code will run on a computer for controlling the mouse and keyboard.
{observation_description}

You are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.
Return one line or multiple lines of python code to perform the action each time, be time efficient. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history
You need to to specify the coordinates of by yourself based on your observation of current observation, but you should be careful to ensure that the coordinates are correct.
You ONLY need to return the code inside a code block, like this:
```python
# your code here
```
Specially, it is also allowed to return the following special code:
When you think you have to wait for some time, return ```WAIT```;
When you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;
When you think the task is done, return ```DONE```.
{coordinate_enhancement}
First give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.""".strip()
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹ - OSWorld é£æ ¼
        user_content = []
        
        # OSWorld é£æ ¼ï¼šç®€æ´çš„ä»»åŠ¡æè¿°
        task_text = f"Task: {instruction}\n\n"
        
        # å±å¹•åˆ†è¾¨ç‡ä¿¡æ¯ï¼ˆç®€æ´æ ¼å¼ï¼‰
        if screen_size:
            width, height = screen_size
            task_text += f"Screen resolution: {width}x{height}\n"
            task_text += f"Coordinate range: x=[0, {width-1}], y=[0, {height-1}]\n\n"
        
        # æ·»åŠ å†å²è½¨è¿¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_trajectory and self.trajectory:
            recent_history = self.trajectory[-self.max_trajectory_length:]
            task_text += f"Previous actions (last {len(recent_history)} steps):\n"
            
            for i, t in enumerate(recent_history):
                task_text += f"Step {i+1}: {t['actions']}\n"
                
                # æ·»åŠ å†å²æˆªå›¾
                if t.get('screenshot_b64'):
                    user_content.append({
                        "type": "text",
                        "text": f"--- Screenshot of Step {i+1} ---"
                    })
                    user_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{t['screenshot_b64']}"}
                    })
            
            task_text += "\n--- Current Screenshot ---\n"
        
        # æ·»åŠ ä»»åŠ¡æ–‡æœ¬
        user_content.insert(0, {"type": "text", "text": task_text})
        
        # æ·»åŠ å½“å‰æˆªå›¾
        user_content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{screenshot_b64}"}
        })
        
        # æ·»åŠ  Accessibility Treeï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if accessibility_tree:
            a11y_text = "\n--- Accessibility Tree ---\n"
            a11y_text += "The following is the accessibility tree of current desktop, which contains UI elements with their properties (role, coordinates, states, etc.):\n\n"
            a11y_text += """âš ï¸ CRITICAL INSTRUCTION - How to Use Accessibility Tree Coordinates:
1. ALWAYS search for your target element in the accessibility tree by matching name, role, or description
2. When you find the element, extract its cp:screencoord="(x, y)" attribute
3. Use these EXACT numbers in pyautogui.click(x, y) - DO NOT modify or estimate them
4. The accessibility tree provides precise coordinates - screenshot coordinates are LESS accurate

âš ï¸ WINDOW OCCLUSION WARNING:
- The accessibility tree includes ALL foreground windows, even if they are visually occluded by other windows
- Check st:AXFocused attribute: elements with st:AXFocused="True" are in the top-most window
- If multiple windows overlap, prioritize elements from the focused window or compare with screenshot
- An element may be in the tree but not clickable if covered by another window

Example workflow:
- Task: "Click the Later button"
- Find in tree: <button name="Later" cp:screencoord="(1408, 752)" st:AXFocused="False">
- Check screenshot: Is this button visible or covered?
- Your code: pyautogui.click(1408, 752)  # Use exact coordinates from tree

If element not found in tree, explain why and use screenshot estimation as fallback.

"""
            # OSWorld æ–¹å¼ï¼šä¼ é€’å®Œæ•´çš„ accessibility treeï¼Œä¸åšé•¿åº¦æˆªæ–­
            # åªé€šè¿‡æ·±åº¦é™åˆ¶ï¼ˆMAX_DEPTH=50ï¼‰æ§åˆ¶æ ‘çš„å¤§å°
            # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šæ¶ˆè€—è¾ƒå¤š tokensï¼Œä½†èƒ½ä¿è¯å®Œæ•´æ€§
            a11y_text += accessibility_tree
            a11y_text += f"\n\n(Total tree size: {len(accessibility_tree)} characters)"
            user_content.append({
                "type": "text",
                "text": a11y_text
            })
        
        messages.append({
            "role": "user",
            "content": user_content
        })
        
        # æ‰“å°å‘é€ç»™æ¨¡å‹çš„ promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("\n" + "=" * 60)
        print("ğŸ“¤ å‘é€ç»™æ¨¡å‹çš„ Prompt")
        print("=" * 60)
        print(f"System Prompt:\n{system_prompt}\n")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å« accessibility tree
        has_a11y_tree = accessibility_tree is not None and len(accessibility_tree) > 0
        print(f"ğŸŒ² Accessibility Tree çŠ¶æ€: {'âœ… å·²åŒ…å«' if has_a11y_tree else 'âŒ æœªåŒ…å«'}")
        if has_a11y_tree:
            print(f"   - é•¿åº¦: {len(accessibility_tree)} å­—ç¬¦")
            print(f"   - æ˜¯å¦æˆªæ–­: {'æ˜¯' if len(accessibility_tree) > 8000 else 'å¦'}")
        
        print(f"\nUser Content (æ–‡æœ¬éƒ¨åˆ†):")
        for item in user_content:
            if item.get("type") == "text":
                text = item.get("text", "")
                # æ£€æŸ¥æ˜¯å¦åŒ…å« accessibility tree
                if "Accessibility Tree" in text:
                    print(f"  âœ… [Accessibility Tree æ–‡æœ¬å—] ({len(text)} å­—ç¬¦)")
                    # æ‰“å°å‰ 500 å­—ç¬¦å’Œå 200 å­—ç¬¦ä»¥ä¾¿æŸ¥çœ‹
                    if len(text) > 700:
                        print(f"    å‰ 500 å­—ç¬¦: {text[:500]}...")
                        print(f"    ... (çœç•¥ä¸­é—´éƒ¨åˆ†) ...")
                        print(f"    å 200 å­—ç¬¦: ...{text[-200:]}")
                    else:
                        print(f"    å®Œæ•´å†…å®¹é¢„è§ˆ: {text[:1000]}...")
                else:
                    print(f"  {text[:200]}..." if len(text) > 200 else f"  {text}")
            elif item.get("type") == "image_url":
                print(f"  ğŸ“· [æˆªå›¾å›¾åƒ]")
        print("=" * 60 + "\n")
        
        return messages
    
    def _parse_actions(self, response_text: str) -> List[str]:
        """
        ä»æ¨¡å‹å“åº”ä¸­è§£æåŠ¨ä½œåˆ—è¡¨
        
        Args:
            response_text: æ¨¡å‹çš„æ–‡æœ¬å“åº”
        
        Returns:
            åŠ¨ä½œå­—ç¬¦ä¸²åˆ—è¡¨
        """
        actions = []
        
        # æå–ä»£ç å—
        code_blocks = re.findall(r'```python\n(.*?)```', response_text, re.DOTALL)
        
        if code_blocks:
            for block in code_blocks:
                lines = block.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                    if not line or line.startswith('#'):
                        continue
                    # åªæ¥å— pyautogui å‘½ä»¤æˆ–æ§åˆ¶ç¬¦
                    if line.startswith('pyautogui.') or line in ['DONE', 'FAIL', 'WAIT']:
                        actions.append(line)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•ç›´æ¥æå–
        if not actions:
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('pyautogui.') or line in ['DONE', 'FAIL', 'WAIT']:
                    actions.append(line)
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰åŠ¨ä½œï¼Œè¿”å› WAIT
        if not actions:
            actions = ['WAIT']
        
        # é™åˆ¶æ¯è½®æœ€å¤š 5 ä¸ªåŠ¨ä½œ
        return actions[:5]
    
    def _update_trajectory(self, instruction: str, observation: Dict, response: str, actions: List[str]):
        """æ›´æ–°å†å²è½¨è¿¹ï¼ˆä»…åœ¨ use_trajectory=True æ—¶ï¼‰"""
        if not self.use_trajectory:
            return
        
        self.trajectory.append({
            'instruction': instruction,
            'screenshot_path': observation.get('screenshot_path'),
            'screenshot_b64': observation.get('screenshot_b64'),  # ä¿å­˜æˆªå›¾çš„ base64 ç¼–ç 
            'response': response,
            'actions': actions,
            'timestamp': datetime.now().isoformat()
        })
        
        # åªä¿ç•™æœ€è¿‘çš„è½¨è¿¹
        if len(self.trajectory) > self.max_trajectory_length:
            self.trajectory = self.trajectory[-self.max_trajectory_length:]
    
    def _mock_prediction(self, instruction: str, observation: Dict) -> Tuple[str, List[str]]:
        """æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›ç¤ºä¾‹åŠ¨ä½œï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        response = f"æ”¶åˆ°ä»»åŠ¡: {instruction}\n\nç”±äº OpenAI API æœªé…ç½®ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ã€‚"
        actions = ["WAIT"]
        return response, actions
    
    def reset(self):
        """é‡ç½®ä»£ç†çŠ¶æ€"""
        self.trajectory = []
        print("ğŸ”„ ä»£ç†å·²é‡ç½®")
    
    def get_trajectory(self) -> List[Dict]:
        """è·å–æ‰§è¡Œè½¨è¿¹"""
        return self.trajectory


class GUIAgentService:
    """
    GUI-Agent æœåŠ¡ - æ•´åˆç¯å¢ƒå’Œä»£ç†
    """
    
    def __init__(self):
        self.env = None
        self.agent = None
        self.is_running = False
        self.stats = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0
        }
    
    def initialize(
        self,
        provider_name: str = "local",
        os_type: str = "macOS",
        model: str = "gpt-4o",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        require_a11y_tree: bool = False
    ) -> Dict:
        """
        åˆå§‹åŒ–ç¯å¢ƒå’Œä»£ç†
        
        Args:
            provider_name: æä¾›è€…åç§°ï¼ˆ"local" æˆ– "vm"ï¼‰
            os_type: æ“ä½œç³»ç»Ÿç±»å‹
            model: æ¨¡å‹åç§°
            api_key: API å¯†é’¥
            base_url: API åŸºç¡€ URL
            require_a11y_tree: æ˜¯å¦å¯ç”¨ accessibility treeï¼ˆé»˜è®¤ Falseï¼Œä¸ OSWorld çš„ True ä¸åŒï¼‰
                             æ³¨æ„ï¼šOSWorld é»˜è®¤ Trueï¼Œä½†æˆ‘ä»¬é»˜è®¤ False ä»¥é¿å…æƒé™é—®é¢˜
        """
        try:
            # åˆå§‹åŒ–ç¯å¢ƒ
            self.env = SimpleDesktopEnv(
                provider_name=provider_name,
                os_type=os_type,
                action_space="pyautogui",
                require_a11y_tree=require_a11y_tree
            )
            
            # åˆå§‹åŒ–ä»£ç†
            self.agent = SimplePromptAgent(
                model=model,
                action_space="pyautogui",
                api_key=api_key,
                base_url=base_url
            )
            
            return {
                'status': 'success',
                'message': f'ç¯å¢ƒå’Œä»£ç†åˆå§‹åŒ–æˆåŠŸ - å¹³å°: {provider_name}, æ¨¡å‹: {model}'
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'åˆå§‹åŒ–å¤±è´¥: {str(e)}'
            }
    
    def run_task(
        self,
        instruction: str,
        max_steps: int = 15,
        sleep_after_execution: float = 2.0
    ) -> Dict:
        """
        è¿è¡Œä¸€ä¸ªä»»åŠ¡
        
        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤
            max_steps: æœ€å¤§æ­¥æ•°
            sleep_after_execution: æ¯æ­¥åç­‰å¾…æ—¶é—´ï¼ˆé»˜è®¤ 2.0 ç§’ï¼Œä¸ OSWorld ä¸€è‡´ï¼‰
        
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        if not self.env or not self.agent:
            return {
                'status': 'error',
                'message': 'ç¯å¢ƒæˆ–ä»£ç†æœªåˆå§‹åŒ–'
            }
        
        try:
            self.is_running = True
            self.stats['total_tasks'] += 1
            
            # é‡ç½®ç¯å¢ƒå’Œä»£ç†
            task_config = {'instruction': instruction}
            obs = self.env.reset(task_config)
            self.agent.reset()
            
            # è®¾ç½®æœ€å¤§æ­¥æ•°
            self.env.max_steps = max_steps
            
            results = {
                'instruction': instruction,
                'steps': [],
                'final_status': 'running',
                'total_steps': 0
            }
            
            # ä¸»å¾ªç¯
            done = False
            while not done and self.env.step_count < max_steps:
                # æ¨¡å‹æ¨ç†
                response, actions = self.agent.predict(instruction, obs)
                
                step_result = {
                    'step': self.env.step_count + 1,
                    'response': response,
                    'actions': actions,
                    'action_results': []
                }
                
                # æ‰§è¡ŒåŠ¨ä½œ
                for action in actions:
                    obs, reward, done, info = self.env.step(action, pause=sleep_after_execution)
                    
                    step_result['action_results'].append({
                        'action': action,
                        'reward': reward,
                        'done': done,
                        'info': info,
                        'screenshot_path': obs.get('screenshot_path')
                    })
                    
                    if done:
                        if reward > 0:
                            results['final_status'] = 'completed'
                            self.stats['successful_tasks'] += 1
                        else:
                            results['final_status'] = 'failed'
                            self.stats['failed_tasks'] += 1
                        break
                
                results['steps'].append(step_result)
                results['total_steps'] = self.env.step_count
                
                if done:
                    break
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§æ­¥æ•°
            if not done and self.env.step_count >= max_steps:
                results['final_status'] = 'max_steps_reached'
                self.stats['failed_tasks'] += 1
            
            self.is_running = False
            
            return {
                'status': 'success',
                'results': results,
                'history': self.env.get_history(),
                'trajectory': self.agent.get_trajectory()
            }
            
        except Exception as e:
            self.is_running = False
            self.stats['failed_tasks'] += 1
            return {
                'status': 'error',
                'message': f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}'
            }
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_tasks': self.stats['total_tasks'],
            'successful_tasks': self.stats['successful_tasks'],
            'failed_tasks': self.stats['failed_tasks'],
            'is_running': self.is_running,
            'has_pyautogui': self.env.controller is not None if self.env else False,
            'has_openai': self.agent.client is not None if self.agent else False
        }
    
    def stop_task(self):
        """åœæ­¢å½“å‰ä»»åŠ¡"""
        self.is_running = False
        if self.env:
            self.env.close()


# å…¨å±€æœåŠ¡å®ä¾‹
gui_agent_service = GUIAgentService()

