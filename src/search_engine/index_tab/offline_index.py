#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦»çº¿æ¨¡å— - ç´¢å¼•æ„å»º+æ ·æœ¬æ”¶é›†
è´Ÿè´£å€’æ’ç´¢å¼•æ„å»ºã€æ–‡æ¡£ç®¡ç†ã€æ ·æœ¬æ”¶é›†ç­‰ç¦»çº¿ä»»åŠ¡
"""

import jieba
import re
import json
import math
from typing import List, Dict, Tuple, Set
from collections import defaultdict, Counter
import pandas as pd
from datetime import datetime
import os

class InvertedIndex:
    """å€’æ’ç´¢å¼•ç±»"""
    
    def __init__(self):
        self.index = defaultdict(set)  # è¯é¡¹ -> æ–‡æ¡£IDé›†åˆ
        self.doc_lengths = {}          # æ–‡æ¡£ID -> æ–‡æ¡£é•¿åº¦
        self.documents = {}            # æ–‡æ¡£ID -> æ–‡æ¡£å†…å®¹
        self.term_freq = defaultdict(dict)  # è¯é¡¹ -> {æ–‡æ¡£ID: è¯é¢‘}
        self.doc_freq = defaultdict(int)    # è¯é¡¹ -> æ–‡æ¡£é¢‘ç‡
        
        # åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
        }
    
    def preprocess_text(self, text: str) -> List[str]:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # åˆ†è¯
        words = jieba.lcut(text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        words = [word for word in words if len(word) > 1 and word not in self.stop_words]
        
        return words
    
    def add_document(self, doc_id: str, content: str):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        # ä¿å­˜åŸå§‹æ–‡æ¡£
        self.documents[doc_id] = content
        
        # é¢„å¤„ç†æ–‡æœ¬
        words = self.preprocess_text(content)
        
        # è®¡ç®—æ–‡æ¡£é•¿åº¦
        self.doc_lengths[doc_id] = len(words)
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(words)
        
        # æ›´æ–°å€’æ’ç´¢å¼•
        for word, freq in word_freq.items():
            self.index[word].add(doc_id)
            self.term_freq[word][doc_id] = freq
        
        # æ›´æ–°æ–‡æ¡£é¢‘ç‡
        for word in word_freq:
            self.doc_freq[word] = len(self.index[word])
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£ä»ç´¢å¼•"""
        if doc_id not in self.documents:
            return False
        
        # è·å–æ–‡æ¡£çš„è¯é¢‘ä¿¡æ¯
        content = self.documents[doc_id]
        words = self.preprocess_text(content)
        word_freq = Counter(words)
        
        # ä»å€’æ’ç´¢å¼•ä¸­ç§»é™¤æ–‡æ¡£
        for word in word_freq:
            if word in self.index:
                self.index[word].discard(doc_id)
                # å¦‚æœè¯é¡¹æ²¡æœ‰æ–‡æ¡£äº†ï¼Œåˆ é™¤è¯¥è¯é¡¹
                if not self.index[word]:
                    del self.index[word]
                    if word in self.term_freq:
                        del self.term_freq[word]
                    if word in self.doc_freq:
                        del self.doc_freq[word]
                else:
                    # æ›´æ–°è¯é¢‘ä¿¡æ¯
                    if word in self.term_freq and doc_id in self.term_freq[word]:
                        del self.term_freq[word][doc_id]
                    # æ›´æ–°æ–‡æ¡£é¢‘ç‡
                    if word in self.doc_freq:
                        self.doc_freq[word] = len(self.index[word])
        
        # åˆ é™¤æ–‡æ¡£ç›¸å…³æ•°æ®
        del self.documents[doc_id]
        if doc_id in self.doc_lengths:
            del self.doc_lengths[doc_id]
        
        return True
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:
        """æœç´¢æ–‡æ¡£ - ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨çœŸæ­£çš„å€’æ’ç´¢å¼•"""
        # é¢„å¤„ç†æŸ¥è¯¢
        query_words = self.preprocess_text(query)
        
        if not query_words:
            return []
        
        # ä½¿ç”¨å€’æ’ç´¢å¼•å¿«é€Ÿæ‰¾åˆ°å€™é€‰æ–‡æ¡£
        candidate_docs = set()
        for word in query_words:
            if word in self.index:
                candidate_docs.update(self.index[word])
        
        if not candidate_docs:
            return []
        
        # åªå¯¹å€™é€‰æ–‡æ¡£è®¡ç®—TF-IDFåˆ†æ•°
        scores = {}
        total_docs = len(self.documents)
        
        for doc_id in candidate_docs:
            score = 0
            for word in query_words:
                if word in self.index and doc_id in self.index[word]:
                    # TF
                    tf = self.term_freq[word][doc_id] / self.doc_lengths[doc_id]
                    # IDF
                    idf = math.log(total_docs / self.doc_freq[word])
                    # TF-IDF
                    score += tf * idf
            
            if score > 0:
                scores[doc_id] = score
        
        # æ’åºå¹¶è¿”å›ç»“æœ
        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        # ç”Ÿæˆæ‘˜è¦
        results = []
        for doc_id, score in sorted_results[:top_k]:
            summary = self.generate_summary(doc_id, query_words)
            results.append((doc_id, score, summary))
        
        return results
    
    def generate_summary(self, doc_id: str, query_words: List[str], max_length: int = 200) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ - ä¼˜åŒ–ç‰ˆæœ¬"""
        content = self.documents[doc_id]
        
        # å¿«é€Ÿç”Ÿæˆæ‘˜è¦ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªæŸ¥è¯¢è¯çš„ä½ç½®ï¼Œç„¶åæˆªå–å‘¨å›´æ–‡æœ¬
        if not query_words:
            return content[:max_length] + "..." if len(content) > max_length else content
        
        # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªæŸ¥è¯¢è¯çš„ä½ç½®
        best_pos = 0
        for word in query_words:
            pos = content.lower().find(word.lower())
            if pos != -1:
                best_pos = max(0, pos - max_length // 3)  # ä»æŸ¥è¯¢è¯å‰1/3ä½ç½®å¼€å§‹
                break
        
        # æˆªå–æ‘˜è¦
        summary = content[best_pos:best_pos + max_length]
        if best_pos > 0:
            summary = "..." + summary
        if len(content) > best_pos + max_length:
            summary = summary + "..."
        
        # é«˜äº®æŸ¥è¯¢è¯
        highlighted_summary = self.highlight_keywords(summary, query_words)
        
        return highlighted_summary
    
    def highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """é«˜äº®å…³é”®è¯"""
        highlighted_text = text
        for keyword in keywords:
            if keyword in highlighted_text:
                highlighted_text = highlighted_text.replace(
                    keyword, 
                    f'<span style="background-color: yellow; font-weight: bold;">{keyword}</span>'
                )
        return highlighted_text
    
    def get_document(self, doc_id: str) -> str:
        """è·å–æ–‡æ¡£å†…å®¹"""
        return self.documents.get(doc_id, "")
    
    def get_all_documents(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        return self.documents.copy()
    
    def get_index_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        total_documents = len(self.documents)
        total_terms = len(self.index)
        
        if total_documents > 0:
            average_doc_length = sum(self.doc_lengths.values()) / total_documents
        else:
            average_doc_length = 0
        
        return {
            'total_documents': total_documents,
            'total_terms': total_terms,
            'average_doc_length': average_doc_length
        }
    
    def save_to_file(self, filename: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        data = {
            'index': {k: list(v) for k, v in self.index.items()},
            'doc_lengths': self.doc_lengths,
            'documents': self.documents,
            'term_freq': {k: dict(v) for k, v in self.term_freq.items()},
            'doc_freq': dict(self.doc_freq)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {filename}")
    
    def load_from_file(self, filename: str):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.index = defaultdict(set)
        for k, v in data['index'].items():
            self.index[k] = set(v)
        
        self.doc_lengths = data['doc_lengths']
        self.documents = data['documents']
        
        self.term_freq = defaultdict(dict)
        for k, v in data['term_freq'].items():
            self.term_freq[k] = v
        
        self.doc_freq = defaultdict(int)
        for k, v in data['doc_freq'].items():
            self.doc_freq[k] = v
        
        print(f"âœ… ç´¢å¼•å·²ä»æ–‡ä»¶åŠ è½½: {filename}")

class SampleCollector:
    """æ ·æœ¬æ”¶é›†å™¨"""
    
    def __init__(self):
        self.samples = []
    
    def add_sample(self, sample: Dict):
        """æ·»åŠ æ ·æœ¬"""
        self.samples.append(sample)
    
    def get_samples(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ ·æœ¬"""
        return self.samples
    
    def export_samples(self, filename: str):
        """å¯¼å‡ºæ ·æœ¬åˆ°æ–‡ä»¶"""
        df = pd.DataFrame(self.samples)
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"âœ… æ ·æœ¬å·²å¯¼å‡ºåˆ°: {filename}")
    
    def get_stats(self) -> Dict:
        """è·å–æ ·æœ¬ç»Ÿè®¡"""
        if not self.samples:
            return {
                'total_samples': 0,
                'total_clicks': 0,
                'click_rate': 0
            }
        
        total_samples = len(self.samples)
        total_clicks = sum(sample.get('clicked', 0) for sample in self.samples)
        click_rate = total_clicks / total_samples if total_samples > 0 else 0
        
        return {
            'total_samples': total_samples,
            'total_clicks': total_clicks,
            'click_rate': click_rate
        }

def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    documents = {
        "doc1": """
        äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
        è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
        """,
        "doc2": """
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡å­¦æ–¹æ³•è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿ"å­¦ä¹ "ï¼ˆå³ï¼Œé€æ­¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼‰ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
        æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†ææ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
        """,
        "doc3": """
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚
        æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥è‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡è¡¨ç¤ºï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
        """,
        "doc4": """
        è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„ä¸€ä¸ªäº¤å‰é¢†åŸŸï¼Œå®ƒç ”ç©¶è®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚
        NLPæŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿå’ŒèŠå¤©æœºå™¨äººç­‰åº”ç”¨ã€‚
        """,
        "doc5": """
        è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å¾—é«˜å±‚æ¬¡çš„ç†è§£ã€‚
        è®¡ç®—æœºè§†è§‰æŠ€æœ¯åŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²å’Œè§†é¢‘åˆ†æç­‰ã€‚
        """,
        "doc6": """
        ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤§é‡ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒç»„æˆã€‚
        ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œåœ¨æ¨¡å¼è¯†åˆ«å’Œé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
        """,
        "doc7": """
        å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è®©æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
        å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚
        """,
        "doc8": """
        çŸ¥è¯†å›¾è°±æ˜¯ä¸€ç§ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å®ä½“å’Œå…³ç³»ç»„ç»‡æˆå›¾ç»“æ„ã€‚
        çŸ¥è¯†å›¾è°±åœ¨æœç´¢å¼•æ“ã€æ¨èç³»ç»Ÿå’Œé—®ç­”ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚
        """,
        "doc9": """
        å¤§æ•°æ®æ˜¯æŒ‡æ— æ³•ç”¨ä¼ ç»Ÿæ•°æ®å¤„ç†è½¯ä»¶åœ¨åˆç†æ—¶é—´å†…å¤„ç†çš„æ•°æ®é›†ã€‚
        å¤§æ•°æ®æŠ€æœ¯åŒ…æ‹¬æ•°æ®å­˜å‚¨ã€æ•°æ®å¤„ç†ã€æ•°æ®åˆ†æå’Œæ•°æ®å¯è§†åŒ–ç­‰æ–¹é¢ã€‚
        """,
        "doc10": """
        äº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºçš„æœåŠ¡æ¨¡å¼ã€‚
        äº‘è®¡ç®—åŒ…æ‹¬åŸºç¡€è®¾æ–½å³æœåŠ¡ã€å¹³å°å³æœåŠ¡å’Œè½¯ä»¶å³æœåŠ¡ç­‰ä¸åŒå±‚æ¬¡ã€‚
        """
    }
    return documents

def build_index_from_documents(documents: Dict[str, str], save_path: str = ""):
    """ä»æ–‡æ¡£æ„å»ºç´¢å¼•"""
    print("ğŸ”¨ æ„å»ºå€’æ’ç´¢å¼•...")
    
    index = InvertedIndex()
    
    for doc_id, content in documents.items():
        index.add_document(doc_id, content)
        print(f"   æ·»åŠ æ–‡æ¡£: {doc_id}")
    
    stats = index.get_index_stats()
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ:")
    print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"   æ€»è¯é¡¹æ•°: {stats['total_terms']}")
    print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['average_doc_length']:.2f}")
    
    if save_path:
        index.save_to_file(save_path)
    
    return index

def main():
    """ä¸»å‡½æ•° - æ„å»ºç´¢å¼•"""
    print("ğŸ—ï¸  ç¦»çº¿ç´¢å¼•æ„å»ºæ¨¡å—")
    print("=" * 50)
    
    # ä¼˜å…ˆä½¿ç”¨é¢„ç½®æ–‡æ¡£
    import os
    import json
    
    preloaded_path = os.path.join("data", "preloaded_documents.json")
    if os.path.exists(preloaded_path):
        print("ğŸ“„ ä½¿ç”¨é¢„ç½®æ–‡æ¡£æ„å»ºç´¢å¼•")
        with open(preloaded_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # æ”¯æŒä¸¤ç§æ ¼å¼
        if isinstance(data, dict) and 'documents' in data:
            documents = data['documents']
        else:
            documents = data
        print(f"âœ… åŠ è½½é¢„ç½®æ–‡æ¡£æˆåŠŸï¼Œå…±{len(documents)}ä¸ªæ–‡æ¡£")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„ç½®æ–‡æ¡£ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£")
        # å›é€€åˆ°ç¤ºä¾‹æ–‡æ¡£
        documents = create_sample_documents()
        print(f"âœ… åˆ›å»ºç¤ºä¾‹æ–‡æ¡£æˆåŠŸï¼Œå…±{len(documents)}ä¸ªæ–‡æ¡£")
    
    # æ„å»ºç´¢å¼•
    index = build_index_from_documents(documents, 'models/index_data.json')
    
    # æµ‹è¯•æœç´¢
    print("\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
    test_queries = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = index.search(query, top_k=3)
        for doc_id, score, summary in results:
            print(f"  - {doc_id}: {score:.4f}")
    
    print("\nâœ… ç¦»çº¿ç´¢å¼•æ„å»ºå®Œæˆ!")
    print("ğŸ’¡ ç°åœ¨å¯ä»¥å¯åŠ¨åœ¨çº¿æœåŠ¡: python online_service.py")

if __name__ == "__main__":
    main() 