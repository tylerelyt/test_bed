#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±æ„å»ºå’Œç®¡ç†æœåŠ¡
ä½¿ç”¨networkxæ„å»ºå›¾è°±ç»“æ„ï¼Œæ”¯æŒå®ä½“å’Œå…³ç³»çš„å­˜å‚¨ã€æŸ¥è¯¢å’Œå¯è§†åŒ–
"""

import json
import os
import networkx as nx
from typing import List, Dict, Tuple, Optional, Any, Set
from datetime import datetime
import pickle
from collections import defaultdict, Counter

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±ç±»"""
    
    def __init__(self, graph_file: str = "models/knowledge_graph.pkl"):
        """
        åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
        
        Args:
            graph_file: å›¾è°±æ–‡ä»¶è·¯å¾„
        """
        self.graph_file = graph_file
        self.graph = nx.MultiDiGraph()  # æœ‰å‘å¤šé‡å›¾ï¼Œæ”¯æŒå¤šç§å…³ç³»
        self.entity_docs = defaultdict(set)  # å®ä½“->æ–‡æ¡£æ˜ å°„
        self.doc_entities = defaultdict(set)  # æ–‡æ¡£->å®ä½“æ˜ å°„
        self.relation_types = Counter()  # å…³ç³»ç±»å‹ç»Ÿè®¡
        self.entity_types = Counter()  # å®ä½“ç±»å‹ç»Ÿè®¡
        
        # åŠ è½½ç°æœ‰å›¾è°±
        self.load_graph()
    
    def add_entity(self, entity_name: str, entity_type: str, description: str = "", doc_id: Optional[str] = None):
        """
        æ·»åŠ å®ä½“åˆ°å›¾è°±
        
        Args:
            entity_name: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹
            description: å®ä½“æè¿°
            doc_id: æ¥æºæ–‡æ¡£ID
        """
        # æ ‡å‡†åŒ–å®ä½“åç§°
        entity_name = entity_name.strip()
        if not entity_name:
            return
            
        # æ·»åŠ æˆ–æ›´æ–°å®ä½“èŠ‚ç‚¹
        if self.graph.has_node(entity_name):
            # æ›´æ–°ç°æœ‰å®ä½“
            node_data = self.graph.nodes[entity_name]
            if description and len(description) > len(node_data.get("description", "")):
                node_data["description"] = description
            node_data["doc_count"] = node_data.get("doc_count", 0) + 1
        else:
            # æ·»åŠ æ–°å®ä½“
            self.graph.add_node(entity_name, 
                              entity_type=entity_type,
                              description=description,
                              doc_count=1,
                              created_at=datetime.now().isoformat())
        
        # æ›´æ–°ç»Ÿè®¡
        self.entity_types[entity_type] += 1
        
        # æ›´æ–°æ–‡æ¡£æ˜ å°„
        if doc_id:
            self.entity_docs[entity_name].add(doc_id)
            self.doc_entities[doc_id].add(entity_name)
    
    def add_relation(self, subject: str, predicate: str, object_entity: str, 
                    description: str = "", doc_id: Optional[str] = None):
        """
        æ·»åŠ å…³ç³»åˆ°å›¾è°±
        
        Args:
            subject: ä¸»ä½“å®ä½“
            predicate: å…³ç³»ç±»å‹
            object_entity: å®¢ä½“å®ä½“
            description: å…³ç³»æè¿°
            doc_id: æ¥æºæ–‡æ¡£ID
        """
        # æ ‡å‡†åŒ–åç§°
        subject = subject.strip()
        object_entity = object_entity.strip()
        predicate = predicate.strip()
        
        if not all([subject, predicate, object_entity]):
            return
        
        # ç¡®ä¿å®ä½“å­˜åœ¨
        if not self.graph.has_node(subject):
            self.add_entity(subject, "æœªåˆ†ç±»", "", doc_id)
        if not self.graph.has_node(object_entity):
            self.add_entity(object_entity, "æœªåˆ†ç±»", "", doc_id)
        
        # æ·»åŠ å…³ç³»è¾¹
        self.graph.add_edge(subject, object_entity,
                          predicate=predicate,
                          description=description,
                          doc_id=doc_id,
                          created_at=datetime.now().isoformat())
        
        # æ›´æ–°ç»Ÿè®¡
        self.relation_types[predicate] += 1
    
    def build_from_ner_results(self, ner_results: Dict[str, Any]):
        """
        ä»NERç»“æœæ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            ner_results: NERæå–ç»“æœ
        """
        print("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        for doc_id, doc_result in ner_results.items():
            if "error" in doc_result:
                print(f"âŒ [KG-Build] è·³è¿‡æ–‡æ¡£ {doc_id}ï¼ŒNERæå–é”™è¯¯: {doc_result['error']}")
                continue
            
            print(f"âœ… [KG-Build] å¤„ç†æ–‡æ¡£ {doc_id}")
            entities = doc_result.get("entities", [])
            relations = doc_result.get("relations", [])
            print(f"ğŸ“Š [KG-Build] æ–‡æ¡£ {doc_id}: {len(entities)} ä¸ªå®ä½“, {len(relations)} ä¸ªå…³ç³»")
            
            # æ·»åŠ å®ä½“
            for entity in doc_result.get("entities", []):
                self.add_entity(
                    entity_name=entity.get("name", ""),
                    entity_type=entity.get("type", "æœªåˆ†ç±»"),
                    description=entity.get("description", ""),
                    doc_id=doc_id
                )
            
            # æ·»åŠ å…³ç³»
            for relation in doc_result.get("relations", []):
                self.add_relation(
                    subject=relation.get("subject", ""),
                    predicate=relation.get("predicate", ""),
                    object_entity=relation.get("object", ""),
                    description=relation.get("description", ""),
                    doc_id=doc_id
                )
        
        print(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œå…± {self.graph.number_of_nodes()} ä¸ªå®ä½“ï¼Œ{self.graph.number_of_edges()} æ¡å…³ç³»")
    
    def search_entities(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢å®ä½“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: åŒ¹é…çš„å®ä½“åˆ—è¡¨
        """
        query = query.lower()
        matches = []
        
        for node in self.graph.nodes():
            node_data = self.graph.nodes[node]
            
            # åç§°åŒ¹é…
            if query in node.lower():
                score = 1.0 if query == node.lower() else 0.8
                matches.append({
                    "entity": node,
                    "type": node_data.get("entity_type", "æœªåˆ†ç±»"),
                    "description": node_data.get("description", ""),
                    "doc_count": node_data.get("doc_count", 0),
                    "score": score
                })
            # æè¿°åŒ¹é…
            elif query in node_data.get("description", "").lower():
                matches.append({
                    "entity": node,
                    "type": node_data.get("entity_type", "æœªåˆ†ç±»"),
                    "description": node_data.get("description", ""),
                    "doc_count": node_data.get("doc_count", 0),
                    "score": 0.6
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x["score"], reverse=True)
        return matches[:limit]
    
    def get_entity_relations(self, entity: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        è·å–å®ä½“çš„æ‰€æœ‰å…³ç³»
        
        Args:
            entity: å®ä½“åç§°
            
        Returns:
            Dict: å®ä½“çš„å…³ç³»ä¿¡æ¯
        """
        if not self.graph.has_node(entity):
            return {"outgoing": [], "incoming": []}
        
        outgoing = []
        incoming = []
        
        # å‡ºè¾¹ï¼ˆä¸»ä½“å…³ç³»ï¼‰
        for target in self.graph.successors(entity):
            for edge_data in self.graph[entity][target].values():
                outgoing.append({
                    "target": target,
                    "predicate": edge_data.get("predicate", ""),
                    "description": edge_data.get("description", ""),
                    "doc_id": edge_data.get("doc_id", "")
                })
        
        # å…¥è¾¹ï¼ˆå®¢ä½“å…³ç³»ï¼‰
        for source in self.graph.predecessors(entity):
            for edge_data in self.graph[source][entity].values():
                incoming.append({
                    "source": source,
                    "predicate": edge_data.get("predicate", ""),
                    "description": edge_data.get("description", ""),
                    "doc_id": edge_data.get("doc_id", "")
                })
        
        return {"outgoing": outgoing, "incoming": incoming}
    
    def get_related_entities(self, entity: str, max_distance: int = 2) -> List[Dict[str, Any]]:
        """
        è·å–ç›¸å…³å®ä½“ï¼ˆåŸºäºå›¾è·ç¦»ï¼‰
        
        Args:
            entity: å®ä½“åç§°
            max_distance: æœ€å¤§è·ç¦»
            
        Returns:
            List[Dict]: ç›¸å…³å®ä½“åˆ—è¡¨
        """
        if not self.graph.has_node(entity):
            return []
        
        related = []
        
        # ä½¿ç”¨BFSè·å–æŒ‡å®šè·ç¦»å†…çš„å®ä½“
        try:
            # è½¬æ¢ä¸ºæ— å‘å›¾è¿›è¡Œè·ç¦»è®¡ç®—
            undirected_graph = self.graph.to_undirected()
            distances = nx.single_source_shortest_path_length(
                undirected_graph, entity, cutoff=max_distance
            )
            
            for related_entity, distance in distances.items():
                if related_entity != entity and distance <= max_distance:
                    node_data = self.graph.nodes[related_entity]
                    related.append({
                        "entity": related_entity,
                        "type": node_data.get("entity_type", "æœªåˆ†ç±»"),
                        "description": node_data.get("description", ""),
                        "distance": distance,
                        "doc_count": node_data.get("doc_count", 0)
                    })
            
            # æŒ‰è·ç¦»å’Œæ–‡æ¡£æ•°é‡æ’åº
            related.sort(key=lambda x: (x["distance"], -x["doc_count"]))
            
        except Exception as e:
            print(f"è·å–ç›¸å…³å®ä½“å¤±è´¥: {e}")
        
        return related
    
    def get_entity_documents(self, entity: str) -> List[str]:
        """
        è·å–åŒ…å«æŒ‡å®šå®ä½“çš„æ–‡æ¡£åˆ—è¡¨
        
        Args:
            entity: å®ä½“åç§°
            
        Returns:
            List[str]: æ–‡æ¡£IDåˆ—è¡¨
        """
        return list(self.entity_docs.get(entity, set()))
    
    def graph_retrieval(self, query: str, top_k: int = 10) -> List[Tuple[str, float, str]]:
        """
        åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢è¯­å¥
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Tuple[str, float, str]]: (doc_id, score, reason)
        """
        # 1. æœç´¢ç›¸å…³å®ä½“
        matched_entities = self.search_entities(query, limit=20)
        
        if not matched_entities:
            return []
        
        # 2. æ”¶é›†ç›¸å…³æ–‡æ¡£
        doc_scores = defaultdict(float)
        doc_reasons = defaultdict(list)
        
        for entity_info in matched_entities:
            entity = entity_info["entity"]
            entity_score = entity_info["score"]
            
            # ç›´æ¥åŒ¹é…çš„æ–‡æ¡£
            for doc_id in self.get_entity_documents(entity):
                doc_scores[doc_id] += entity_score
                doc_reasons[doc_id].append(f"åŒ…å«å®ä½“: {entity}")
            
            # ç›¸å…³å®ä½“çš„æ–‡æ¡£
            related_entities = self.get_related_entities(entity, max_distance=2)
            for related_info in related_entities[:5]:  # é™åˆ¶ç›¸å…³å®ä½“æ•°é‡
                related_entity = related_info["entity"]
                distance = related_info["distance"]
                
                # è·ç¦»è¶Šè¿‘ï¼Œåˆ†æ•°è¶Šé«˜
                related_score = entity_score * (1.0 / (distance + 1)) * 0.5
                
                for doc_id in self.get_entity_documents(related_entity):
                    doc_scores[doc_id] += related_score
                    doc_reasons[doc_id].append(f"ç›¸å…³å®ä½“: {related_entity} (è·ç¦»: {distance})")
        
        # 3. æ’åºå’Œè¿”å›
        results = []
        for doc_id, score in sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]:
            reason = "; ".join(doc_reasons[doc_id][:3])  # é™åˆ¶åŸå› æ•°é‡
            results.append((doc_id, score, reason))
        
        return results
    
    def save_graph(self, filepath: Optional[str] = None):
        """
        ä¿å­˜çŸ¥è¯†å›¾è°±
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        if filepath is None:
            filepath = self.graph_file
        
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            graph_data = {
                "graph": self.graph,
                "entity_docs": dict(self.entity_docs),
                "doc_entities": dict(self.doc_entities),
                "relation_types": dict(self.relation_types),
                "entity_types": dict(self.entity_types),
                "saved_at": datetime.now().isoformat()
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(graph_data, f)
            
            print(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            print(f"ä¿å­˜çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    def load_from_json_file(self, filepath: str) -> bool:
        """
        ä»JSONæ–‡ä»¶åŠ è½½é¢„ç½®çŸ¥è¯†å›¾è°±
        æ”¯æŒä¸¤ç§ç»“æ„ï¼š
        1) å¯¼å‡ºæ ¼å¼ï¼š{"entities": [...], "relations": [...]}ï¼ˆä¸export_graph_dataä¸€è‡´ï¼‰
        2) ç®€åŒ–ä¸‰å…ƒç»„åˆ—è¡¨ï¼š{"triples": [{"subject":..., "predicate":..., "object":...}, ...]}
        """
        try:
            if not os.path.exists(filepath):
                print(f"é¢„ç½®å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return False
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # æ¸…ç©ºç°æœ‰å›¾
            self.clear_graph()
            # æƒ…å†µ1ï¼šåŒ…å«entities/relations
            if isinstance(data, dict) and 'entities' in data and 'relations' in data:
                for ent in data.get('entities', []):
                    self.add_entity(
                        entity_name=ent.get('name', ''),
                        entity_type=ent.get('type', 'æœªåˆ†ç±»'),
                        description=ent.get('description', ''),
                        doc_id=None
                    )
                    # æ–‡æ¡£æ˜ å°„ï¼ˆå¦‚æœæä¾›ï¼‰
                    for did in ent.get('documents', []):
                        self.entity_docs[ent.get('name', '')].add(did)
                        self.doc_entities[did].add(ent.get('name', ''))
                for rel in data.get('relations', []):
                    self.add_relation(
                        subject=rel.get('subject', ''),
                        predicate=rel.get('predicate', ''),
                        object_entity=rel.get('object', ''),
                        description=rel.get('description', ''),
                        doc_id=rel.get('doc_id', None)
                    )
            # æƒ…å†µ2ï¼štriples åˆ—è¡¨
            elif isinstance(data, dict) and 'triples' in data:
                for t in data.get('triples', []):
                    s = (t.get('subject') or '').strip()
                    p = (t.get('predicate') or '').strip()
                    o = (t.get('object') or '').strip()
                    if s:
                        self.add_entity(s, 'æœªåˆ†ç±»')
                    if o:
                        self.add_entity(o, 'æœªåˆ†ç±»')
                    if s and p and o:
                        self.add_relation(s, p, o)
            else:
                print("ä¸æ”¯æŒçš„é¢„ç½®å›¾è°±JSONç»“æ„")
                return False
            print(f"âœ… é¢„ç½®çŸ¥è¯†å›¾è°±åŠ è½½å®Œæˆï¼š{self.graph.number_of_nodes()} ä¸ªå®ä½“ï¼Œ{self.graph.number_of_edges()} æ¡å…³ç³»")
            return True
        except Exception as e:
            print(f"åŠ è½½é¢„ç½®çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return False


    def load_from_openkg_triples(self, filepath: str, max_triples: int = 5000) -> bool:
        """
        ä» OpenKG ä¸‰å…ƒç»„æ–‡ä»¶åŠ è½½ï¼ˆTSV æ ¼å¼ï¼šsubject \t predicate \t objectï¼‰
        ä»…åŠ è½½å‰ max_triples æ¡ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
        """
        try:
            if not os.path.exists(filepath):
                print(f"é¢„ç½®OpenKGä¸‰å…ƒç»„æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return False
            # æ¸…ç©ºç°æœ‰å›¾
            self.clear_graph()
            loaded = 0
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    if max_triples and loaded >= max_triples:
                        break
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split('\t')
                    if len(parts) < 3:
                        continue
                    subject = parts[0].strip()
                    predicate = parts[1].strip()
                    obj = parts[2].strip()
                    if subject and predicate and obj:
                        # ç®€å•çš„ç±»å‹æ ‡æ³¨ä¸º"æœªåˆ†ç±»"
                        if not self.graph.has_node(subject):
                            self.add_entity(subject, 'æœªåˆ†ç±»')
                        if not self.graph.has_node(obj):
                            self.add_entity(obj, 'æœªåˆ†ç±»')
                        self.add_relation(subject, predicate, obj)
                        loaded += 1
            print(f"âœ… é¢„ç½®OpenKGå›¾è°±åŠ è½½å®Œæˆï¼š{self.graph.number_of_nodes()} ä¸ªå®ä½“ï¼Œ{self.graph.number_of_edges()} æ¡å…³ç³»ï¼ˆè½½å…¥ä¸‰å…ƒç»„ {loaded} æ¡ï¼‰")
            return loaded > 0
        except Exception as e:
            print(f"åŠ è½½OpenKGä¸‰å…ƒç»„å¤±è´¥: {e}")
            return False
    
    def load_graph(self, filepath: Optional[str] = None):
        """
        åŠ è½½çŸ¥è¯†å›¾è°±
        
        Args:
            filepath: åŠ è½½è·¯å¾„
        """
        if filepath is None:
            filepath = self.graph_file
        
        if not os.path.exists(filepath):
            print(f"çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return
        
        try:
            with open(filepath, 'rb') as f:
                graph_data = pickle.load(f)
            
            self.graph = graph_data.get("graph", nx.MultiDiGraph())
            self.entity_docs = defaultdict(set, {k: set(v) for k, v in graph_data.get("entity_docs", {}).items()})
            self.doc_entities = defaultdict(set, {k: set(v) for k, v in graph_data.get("doc_entities", {}).items()})
            self.relation_types = Counter(graph_data.get("relation_types", {}))
            self.entity_types = Counter(graph_data.get("entity_types", {}))
            
            print(f"çŸ¥è¯†å›¾è°±å·²åŠ è½½: {self.graph.number_of_nodes()} ä¸ªå®ä½“ï¼Œ{self.graph.number_of_edges()} æ¡å…³ç³»")
            
        except Exception as e:
            print(f"åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "entity_count": self.graph.number_of_nodes(),
            "relation_count": self.graph.number_of_edges(),
            "entity_types": dict(self.entity_types),
            "relation_types": dict(self.relation_types),
            "document_count": len(self.doc_entities),
            "avg_entities_per_doc": len(self.doc_entities) and sum(len(entities) for entities in self.doc_entities.values()) / len(self.doc_entities) or 0,
            "avg_relations_per_entity": self.graph.number_of_nodes() and self.graph.number_of_edges() / self.graph.number_of_nodes() or 0
        }
    
    def clear_graph(self):
        """æ¸…ç©ºçŸ¥è¯†å›¾è°±"""
        self.graph.clear()
        self.entity_docs.clear()
        self.doc_entities.clear()
        self.relation_types.clear()
        self.entity_types.clear()
        print("çŸ¥è¯†å›¾è°±å·²æ¸…ç©º")
    
    def export_graph_data(self) -> Dict[str, Any]:
        """
        å¯¼å‡ºå›¾è°±æ•°æ®
        
        Returns:
            Dict: å›¾è°±æ•°æ®
        """
        entities = []
        relations = []
        
        # å¯¼å‡ºå®ä½“
        for node in self.graph.nodes():
            node_data = self.graph.nodes[node]
            entities.append({
                "name": node,
                "type": node_data.get("entity_type", "æœªåˆ†ç±»"),
                "description": node_data.get("description", ""),
                "doc_count": node_data.get("doc_count", 0),
                "documents": list(self.entity_docs.get(node, set()))
            })
        
        # å¯¼å‡ºå…³ç³»
        for source, target in self.graph.edges():
            for edge_data in self.graph[source][target].values():
                relations.append({
                    "subject": source,
                    "predicate": edge_data.get("predicate", ""),
                    "object": target,
                    "description": edge_data.get("description", ""),
                    "doc_id": edge_data.get("doc_id", "")
                })
        
        return {
            "entities": entities,
            "relations": relations,
            "stats": self.get_stats(),
            "exported_at": datetime.now().isoformat()
        } 