#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºLLMçš„å‘½åå®ä½“è¯†åˆ«æœåŠ¡
ç”¨äºä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±
æ”¯æŒ Ollama æœ¬åœ°éƒ¨ç½²å’Œ OpenAI å…¼å®¹çš„ API æ¥å£
"""

import json
import requests
import os
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime
import re

# å°è¯•å¯¼å…¥ OpenAI å®¢æˆ·ç«¯
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("âš ï¸ openai åŒ…æœªå®‰è£…ï¼Œåªèƒ½ä½¿ç”¨ Ollama æ–¹å¼")

class NERService:
    """åŸºäºLLMçš„å‘½åå®ä½“è¯†åˆ«æœåŠ¡"""
    
    def __init__(self, 
                 api_type: str = "ollama",
                 ollama_url: str = "http://localhost:11434",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 default_model: Optional[str] = None):
        """
        åˆå§‹åŒ–NERæœåŠ¡
        
        Args:
            api_type: APIç±»å‹ ("ollama" æˆ– "openai")
            ollama_url: OllamaæœåŠ¡URL (å½“api_typeä¸ºollamaæ—¶ä½¿ç”¨)
            api_key: APIå¯†é’¥ (å½“api_typeä¸ºopenaiæ—¶ä½¿ç”¨)
            base_url: APIåŸºç¡€URL (å½“api_typeä¸ºopenaiæ—¶ä½¿ç”¨)
            default_model: é»˜è®¤æ¨¡å‹åç§°
        """
        self.api_type = api_type.lower()
        self.ollama_url = ollama_url
        
        # API é…ç½®
        if self.api_type == "openai":
            if not HAS_OPENAI:
                raise ValueError("ä½¿ç”¨ OpenAI API éœ€è¦å®‰è£… openai åŒ…: pip install openai")
            
            self.api_key = api_key or os.environ.get("DASHSCOPE_API_KEY") or os.environ.get("OPENAI_API_KEY")
            self.base_url = base_url or os.environ.get("OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
            
            print(f"ğŸ”‘ [NER-Config] APIå¯†é’¥: {self.api_key[:15] if self.api_key else 'None'}...")
            print(f"ğŸŒ [NER-Config] åŸºç¡€URL: {self.base_url}")
            
            if not self.api_key:
                raise ValueError("ä½¿ç”¨ OpenAI API éœ€è¦è®¾ç½® api_key æˆ–ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY/OPENAI_API_KEY")
            
            # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
            self.openai_client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            self.default_model = default_model or os.environ.get("LLM_MODEL", "qwen-plus")
        else:
            # Ollama é…ç½®
            self.default_model = default_model or "qwen2.5-coder:latest"
        
    def extract_entities_and_relations(self, text: str, model: Optional[str] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæå–å®ä½“å’Œå…³ç³»
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            Dict: åŒ…å«å®ä½“å’Œå…³ç³»çš„å­—å…¸
        """
        if model is None:
            model = self.default_model
            
        # æ„å»ºNERæç¤ºè¯
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚

æ–‡æœ¬ï¼š{text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
{{
    "entities": [
        {{
            "name": "å®ä½“åç§°",
            "type": "å®ä½“ç±»å‹",
            "description": "å®ä½“æè¿°"
        }}
    ],
    "relations": [
        {{
            "subject": "ä¸»ä½“å®ä½“",
            "predicate": "å…³ç³»ç±»å‹",
            "object": "å®¢ä½“å®ä½“",
            "description": "å…³ç³»æè¿°"
        }}
    ]
}}

å®ä½“ç±»å‹åŒ…æ‹¬ï¼šäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€æ¦‚å¿µã€æŠ€æœ¯ã€äº§å“ã€äº‹ä»¶ç­‰ã€‚
å…³ç³»ç±»å‹åŒ…æ‹¬ï¼šå±äºã€ä½äºã€å¼€å‘ã€ä½¿ç”¨ã€ç›¸å…³ã€å½±å“ç­‰ã€‚

è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""
        
        try:
            print(f"ğŸ” [NER] å¼€å§‹å¤„ç†æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")
            print(f"ğŸ¤– [NER] ä½¿ç”¨æ¨¡å‹: {model}")
            print(f"ğŸ”§ [NER] APIç±»å‹: {self.api_type}")
            
            # æ ¹æ®APIç±»å‹è°ƒç”¨ä¸åŒçš„æ¥å£
            if self.api_type == "openai":
                llm_response = self._call_openai_api(prompt, model)
            else:
                llm_response = self._call_ollama_api(prompt, model)
            
            # æ£€æŸ¥APIè°ƒç”¨æ˜¯å¦å‡ºé”™
            if llm_response.startswith("ERROR:"):
                return {"error": llm_response}
            
            print(f"âœ… [NER] LLMå“åº”æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(llm_response)}")
            print(f"ğŸ“ [NER] LLMåŸå§‹å“åº”: {llm_response[:500]}...")
            
            # è§£æJSONå“åº”
            parsed_result = self._parse_ner_response(llm_response)
            
            if "error" in parsed_result:
                print(f"âŒ [NER] è§£æé”™è¯¯: {parsed_result['error']}")
            else:
                entities_count = len(parsed_result.get("entities", []))
                relations_count = len(parsed_result.get("relations", []))
                print(f"âœ… [NER] è§£ææˆåŠŸ: {entities_count}ä¸ªå®ä½“, {relations_count}ä¸ªå…³ç³»")
            
            return parsed_result
                
        except Exception as e:
            error_msg = f"NERæå–å¤±è´¥: {str(e)}"
            print(f"âŒ [NER] å¼‚å¸¸: {error_msg}")
            import traceback
            print(f"ğŸ“ [NER] è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {"error": error_msg}
    
    def _call_ollama_api(self, prompt: str, model: str) -> str:
        """è°ƒç”¨Ollama API"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                error_msg = f"Ollama APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                print(f"âŒ [NER] {error_msg}")
                print(f"ğŸ“ [NER] å“åº”å†…å®¹: {response.text[:500]}...")
                return f"ERROR: {error_msg}"
                
        except Exception as e:
            error_msg = f"Ollama APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
            print(f"âŒ [NER] {error_msg}")
            return f"ERROR: {error_msg}"
    
    def _call_openai_api(self, prompt: str, model: str) -> str:
        """è°ƒç”¨OpenAIå…¼å®¹API"""
        try:
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            completion = self.openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
                stream=False
            )
            
            return completion.choices[0].message.content
            
        except Exception as e:
            error_msg = f"OpenAI APIè°ƒç”¨å¼‚å¸¸: {str(e)}"
            print(f"âŒ [NER] {error_msg}")
            return f"ERROR: {error_msg}"
    
    def _parse_ner_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æLLMçš„NERå“åº”
        
        Args:
            response: LLMçš„å“åº”æ–‡æœ¬
            
        Returns:
            Dict: è§£æåçš„å®ä½“å’Œå…³ç³»
        """
        try:
            print(f"ğŸ” [NER-Parse] å¼€å§‹è§£æå“åº”ï¼Œé•¿åº¦: {len(response)}")
            
            # å°è¯•ç›´æ¥è§£æJSON
            if response.strip().startswith('{'):
                print(f"âœ… [NER-Parse] æ£€æµ‹åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ")
                result = json.loads(response)
                print(f"âœ… [NER-Parse] ç›´æ¥è§£ææˆåŠŸ")
                return result
            
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            print(f"ğŸ” [NER-Parse] åœ¨å“åº”ä¸­æŸ¥æ‰¾JSONéƒ¨åˆ†")
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                print(f"âœ… [NER-Parse] æ‰¾åˆ°JSONéƒ¨åˆ†: {json_str[:200]}...")
                result = json.loads(json_str)
                print(f"âœ… [NER-Parse] JSONè§£ææˆåŠŸ")
                return result
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›ç©ºç»“æœ
            print(f"âš ï¸  [NER-Parse] æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œè¿”å›ç©ºç»“æœ")
            return {"entities": [], "relations": []}
            
        except json.JSONDecodeError as e:
            print(f"âŒ [NER-Parse] JSONè§£æå¤±è´¥: {str(e)}")
            print(f"ğŸ“ [NER-Parse] å°è¯•å¤‡ç”¨è§£ææ–¹æ³•")
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æ–‡æœ¬è§£æ
            return self._fallback_parse(response)
        except Exception as e:
            print(f"âŒ [NER-Parse] è§£æè¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}
    
    def _fallback_parse(self, response: str) -> Dict[str, Any]:
        """
        å¤‡ç”¨è§£ææ–¹æ³•ï¼Œå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨
        
        Args:
            response: LLMçš„å“åº”æ–‡æœ¬
            
        Returns:
            Dict: è§£æåçš„å®ä½“å’Œå…³ç³»
        """
        entities = []
        relations = []
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if 'å®ä½“' in line or 'Entity' in line:
                # å°è¯•æå–å®ä½“ä¿¡æ¯
                if ':' in line:
                    entity_info = line.split(':', 1)[1].strip()
                    entities.append({
                        "name": entity_info,
                        "type": "æœªåˆ†ç±»",
                        "description": ""
                    })
            elif 'å…³ç³»' in line or 'Relation' in line:
                # å°è¯•æå–å…³ç³»ä¿¡æ¯
                if ':' in line:
                    relation_info = line.split(':', 1)[1].strip()
                    relations.append({
                        "subject": relation_info,
                        "predicate": "ç›¸å…³",
                        "object": "",
                        "description": ""
                    })
        
        return {"entities": entities, "relations": relations}
    
    def extract_from_document(self, doc_id: str, content: str, model: Optional[str] = None) -> Dict[str, Any]:
        """
        ä»å•ä¸ªæ–‡æ¡£æå–å®ä½“å’Œå…³ç³»
        
        Args:
            doc_id: æ–‡æ¡£ID
            content: æ–‡æ¡£å†…å®¹
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            Dict: åŒ…å«æ–‡æ¡£IDå’Œæå–ç»“æœçš„å­—å…¸
        """
        # æ¸…ç†æ–‡æ¡£å†…å®¹ï¼Œå»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned_content = content.strip()
        if not cleaned_content:
            return {"doc_id": doc_id, "error": "æ–‡æ¡£å†…å®¹ä¸ºç©º"}
        
        # å¦‚æœæ–‡æ¡£è¿‡é•¿ï¼Œè¿›è¡Œåˆ†æ®µå¤„ç†
        max_length = 2000
        if len(cleaned_content) > max_length:
            # åˆ†æ®µå¤„ç†
            chunks = [cleaned_content[i:i+max_length] for i in range(0, len(cleaned_content), max_length)]
            
            all_entities = []
            all_relations = []
            
            for i, chunk in enumerate(chunks):
                print(f"å¤„ç†æ–‡æ¡£ {doc_id} çš„ç¬¬ {i+1}/{len(chunks)} æ®µ")
                chunk_result = self.extract_entities_and_relations(chunk, model)
                
                if "error" not in chunk_result:
                    all_entities.extend(chunk_result.get("entities", []))
                    all_relations.extend(chunk_result.get("relations", []))
            
            # å»é‡å’Œåˆå¹¶
            entities = self._deduplicate_entities(all_entities)
            relations = self._deduplicate_relations(all_relations)
            
        else:
            # ç›´æ¥å¤„ç†
            result = self.extract_entities_and_relations(cleaned_content, model)
            if "error" in result:
                return {"doc_id": doc_id, "error": result["error"]}
            
            entities = result.get("entities", [])
            relations = result.get("relations", [])
        
        return {
            "doc_id": doc_id,
            "entities": entities,
            "relations": relations,
            "entity_count": len(entities),
            "relation_count": len(relations)
        }
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å»é‡å®ä½“"""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            key = (entity.get("name", "").lower(), entity.get("type", ""))
            if key not in seen and entity.get("name"):
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _deduplicate_relations(self, relations: List[Dict]) -> List[Dict]:
        """å»é‡å…³ç³»"""
        seen = set()
        unique_relations = []
        
        for relation in relations:
            key = (
                relation.get("subject", "").lower(),
                relation.get("predicate", "").lower(),
                relation.get("object", "").lower()
            )
            if key not in seen and all(relation.get(k) for k in ["subject", "predicate", "object"]):
                seen.add(key)
                unique_relations.append(relation)
        
        return unique_relations
    
    def batch_extract_from_documents(self, documents: Dict[str, str], model: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡ä»æ–‡æ¡£æå–å®ä½“å’Œå…³ç³»
        
        Args:
            documents: æ–‡æ¡£å­—å…¸ {doc_id: content}
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            Dict: æ‰¹é‡æå–ç»“æœ
        """
        results = {}
        total_docs = len(documents)
        
        print(f"å¼€å§‹æ‰¹é‡NERæå–ï¼Œå…± {total_docs} ä¸ªæ–‡æ¡£")
        
        for i, (doc_id, content) in enumerate(documents.items(), 1):
            print(f"ğŸ“„ [Batch-NER] å¤„ç†æ–‡æ¡£ {i}/{total_docs}: {doc_id}")
            print(f"ğŸ“„ [Batch-NER] æ–‡æ¡£å†…å®¹é•¿åº¦: {len(content)}")
            print(f"ğŸ“„ [Batch-NER] æ–‡æ¡£å†…å®¹é¢„è§ˆ: {content[:200]}...")
            
            result = self.extract_from_document(doc_id, content, model)
            
            if "error" in result:
                print(f"âŒ [Batch-NER] æ–‡æ¡£ {doc_id} å¤„ç†å¤±è´¥: {result['error']}")
            else:
                entities_count = len(result.get("entities", []))
                relations_count = len(result.get("relations", []))
                print(f"âœ… [Batch-NER] æ–‡æ¡£ {doc_id} å¤„ç†æˆåŠŸ: {entities_count}ä¸ªå®ä½“, {relations_count}ä¸ªå…³ç³»")
                
            results[doc_id] = result
        
        print(f"æ‰¹é‡NERæå–å®Œæˆ")
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–NERæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "service_name": "NER Service",
            "api_type": self.api_type,
            "default_model": self.default_model,
            "supported_entity_types": ["äººç‰©", "åœ°ç‚¹", "ç»„ç»‡", "æ¦‚å¿µ", "æŠ€æœ¯", "äº§å“", "äº‹ä»¶"],
            "supported_relation_types": ["å±äº", "ä½äº", "å¼€å‘", "ä½¿ç”¨", "ç›¸å…³", "å½±å“"]
        }
        
        if self.api_type == "ollama":
            stats["ollama_url"] = self.ollama_url
        else:
            stats["base_url"] = self.base_url
            stats["has_api_key"] = bool(self.api_key)
        
        return stats 