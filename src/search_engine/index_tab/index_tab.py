import gradio as gr
from datetime import datetime
import json
import os
import tempfile

def show_index_stats(search_engine):
    """æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = search_engine.get_stats()
        html_content = f"""
        <div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
            <h4>ğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯</h4>
            <ul>
                <li><strong>æ€»æ–‡æ¡£æ•°:</strong> {stats.get('total_documents', 0)}</li>
                <li><strong>æ€»è¯é¡¹æ•°:</strong> {stats.get('total_terms', 0)}</li>
                <li><strong>å¹³å‡æ–‡æ¡£é•¿åº¦:</strong> {stats.get('average_doc_length', 0):.2f}</li>
            </ul>
            <p style="color: #6c757d; font-size: 0.9em;">ç»Ÿè®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        """
        return html_content
    except Exception as e:
        return f"<p style='color: red;'>è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {str(e)}</p>"

def check_index_quality(search_engine):
    """æ£€æŸ¥ç´¢å¼•è´¨é‡"""
    try:
        stats = search_engine.get_stats()
        total_docs = stats.get('total_documents', 0)
        total_terms = stats.get('total_terms', 0)
        avg_length = stats.get('average_doc_length', 0)
        
        issues = []
        recommendations = []
        
        if total_docs == 0:
            issues.append("ç´¢å¼•ä¸­æ²¡æœ‰æ–‡æ¡£")
            recommendations.append("æ·»åŠ æ›´å¤šæ–‡æ¡£åˆ°ç´¢å¼•")
        
        if total_terms <= 50:
            issues.append("è¯é¡¹æ•°é‡è¾ƒå°‘")
            recommendations.append("å¢åŠ æ–‡æ¡£å¤šæ ·æ€§")
        
        if avg_length < 10:
            issues.append("æ–‡æ¡£å¹³å‡é•¿åº¦è¿‡çŸ­")
            recommendations.append("å¢åŠ æ–‡æ¡£å†…å®¹é•¿åº¦")
        elif avg_length > 100:
            issues.append("æ–‡æ¡£å¹³å‡é•¿åº¦è¿‡é•¿")
            recommendations.append("è€ƒè™‘æ–‡æ¡£åˆ†æ®µ")
        
        html_content = f"""
        <div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;">
            <h4>ğŸ” ç´¢å¼•è´¨é‡æ£€æŸ¥æŠ¥å‘Š</h4>
            <h5>ğŸ“ˆ è´¨é‡æŒ‡æ ‡:</h5>
            <ul>
                <li>æ–‡æ¡£æ•°é‡: {total_docs} ä¸ª</li>
                <li>è¯é¡¹æ•°é‡: {total_terms} ä¸ª</li>
                <li>å¹³å‡æ–‡æ¡£é•¿åº¦: {avg_length:.2f} ä¸ªè¯</li>
            </ul>
        """
        
        if issues:
            html_content += f"""
            <h5>âš ï¸ å‘ç°çš„é—®é¢˜:</h5>
            <ul style="color: #dc3545;">
                {''.join([f'<li>{issue}</li>' for issue in issues])}
            </ul>
            """
        
        if recommendations:
            html_content += f"""
            <h5>ğŸ’¡ æ”¹è¿›å»ºè®®:</h5>
            <ul style="color: #007bff;">
                {''.join([f'<li>{rec}</li>' for rec in recommendations])}
            </ul>
            """
        
        html_content += f"""
            <p style="color: #6c757d; font-size: 0.9em;">æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        """
        return html_content
    except Exception as e:
        return f"<p style='color: red;'>ç´¢å¼•è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}</p>"

def view_inverted_index(search_engine):
    """æŸ¥çœ‹å€’æ’ç´¢å¼•å†…å®¹"""
    try:
        index_service = search_engine.index_service
        # ç›´æ¥è®¿é—®åº•å±‚InvertedIndexå¯¹è±¡
        inverted_index = index_service.index.index
        # å–å‰20ä¸ªè¯é¡¹
        items = list(inverted_index.items())[:20]
        data = [[term, ', '.join(list(doc_ids)[:10])] for term, doc_ids in items]
        return data
    except Exception as e:
        return [["é”™è¯¯", str(e)]]

def get_all_documents(search_engine):
    """è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨"""
    try:
        documents = search_engine.get_all_documents()
        if not documents:
            return [["æš‚æ— æ–‡æ¡£", "è¯·å…ˆå¯¼å…¥æ–‡æ¡£æ–‡ä»¶"]]
        
        data = []
        for doc_id, content in documents.items():
            # æˆªå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
            preview = content[:100] + "..." if len(content) > 100 else content
            data.append([doc_id, preview])
        
        return data
    except Exception as e:
        return [["é”™è¯¯", str(e)]]

def export_documents(search_engine):
    """å¯¼å‡ºæ‰€æœ‰æ–‡æ¡£åˆ°JSONæ–‡ä»¶"""
    try:
        documents = search_engine.get_all_documents()
        if not documents:
            return None, "âŒ æ²¡æœ‰æ–‡æ¡£å¯å¯¼å‡º"
        
        # ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_filename = f"documents_export_{timestamp}.json"
        
        # å¯¼å‡ºæ–‡æ¡£æ•°æ®
        export_data = {
            "export_time": datetime.now().isoformat(),
            "total_documents": len(documents),
            "documents": documents
        }
        
        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".json", mode="w", encoding="utf-8") as tmp:
            json.dump(export_data, tmp, ensure_ascii=False, indent=2)
            tmp_path = tmp.name
        
        return tmp_path, f"âœ… æ–‡æ¡£å¯¼å‡ºæˆåŠŸï¼\næ–‡æ¡£æ•°é‡: {len(documents)}\nç‚¹å‡»ä¸Šæ–¹ä¸‹è½½æŒ‰é’®è·å–æ–‡ä»¶"
    except Exception as e:
        return None, f"âŒ å¯¼å‡ºæ–‡æ¡£å¤±è´¥: {str(e)}"

def import_documents_from_file(search_engine, file):
    """ä»æ–‡ä»¶å¯¼å…¥æ–‡æ¡£å¹¶æ›´æ–°ç´¢å¼•"""
    try:
        if file is None:
            return "âŒ è¯·é€‰æ‹©è¦å¯¼å…¥çš„æ–‡ä»¶"
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file.name, 'r', encoding='utf-8') as f:
            import_data = json.load(f)
        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        if "documents" not in import_data:
            return "âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ 'documents' å­—æ®µ"
        
        documents = import_data["documents"]
        if not isinstance(documents, dict):
            return "âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼š'documents' åº”è¯¥æ˜¯å­—å…¸æ ¼å¼"
        
        if not documents:
            return "âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æ–‡æ¡£æ•°æ®"
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        search_engine.clear_index()
        
        # æ‰¹é‡æ·»åŠ æ–°æ–‡æ¡£
        success_count = search_engine.batch_add_documents(documents)
        
        # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
        search_engine.save_index()
        
        return f"âœ… æ–‡æ¡£å¯¼å…¥æˆåŠŸï¼\nå¯¼å…¥æ–‡æ¡£æ•°: {success_count}\næ€»æ–‡æ¡£æ•°: {len(documents)}"
    except Exception as e:
        return f"âŒ å¯¼å…¥æ–‡æ¡£å¤±è´¥: {str(e)}"

def build_index_tab(search_engine):
    with gr.Blocks() as index_tab:
        gr.Markdown("""
        ### ğŸ—ï¸ ç¬¬ä¸€éƒ¨åˆ†ï¼šç¦»çº¿ç´¢å¼•æ„å»º
        """)
        
        with gr.Tabs():
            # ç´¢å¼•ä¿¡æ¯æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“Š ç´¢å¼•ä¿¡æ¯"):
                with gr.Row():
                    with gr.Column(scale=2):
                        index_stats_btn = gr.Button("ğŸ“Š æŸ¥çœ‹ç´¢å¼•ç»Ÿè®¡", variant="primary")
                        index_stats_output = gr.HTML(value="<p>ç‚¹å‡»æŒ‰é’®æŸ¥çœ‹ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯...</p>", elem_id="index_stats_output")
                        index_quality_btn = gr.Button("ğŸ” ç´¢å¼•è´¨é‡æ£€æŸ¥", variant="secondary")
                        index_quality_output = gr.HTML(value="<p>ç‚¹å‡»æŒ‰é’®è¿›è¡Œç´¢å¼•è´¨é‡æ£€æŸ¥...</p>", elem_id="index_quality_output")
                        view_index_btn = gr.Button("ğŸ“– æŸ¥çœ‹å€’æ’ç´¢å¼•", variant="secondary")
                        view_index_output = gr.Dataframe(headers=["è¯é¡¹", "æ–‡æ¡£IDåˆ—è¡¨"], label="å€’æ’ç´¢å¼•ç‰‡æ®µ", interactive=False)
                    with gr.Column(scale=3):
                        gr.HTML("<p>ç´¢å¼•æ„å»ºè¯¦ç»†ä¿¡æ¯...</p>")
            
            # æ–‡æ¡£ç®¡ç†æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“„ æ–‡æ¡£ç®¡ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“‹ æ–‡æ¡£åˆ—è¡¨")
                        refresh_docs_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡æ¡£åˆ—è¡¨", variant="primary")
                        docs_list = gr.Dataframe(
                            headers=["æ–‡æ¡£ID", "å†…å®¹é¢„è§ˆ"], 
                            label="æ‰€æœ‰æ–‡æ¡£", 
                            interactive=False,
                            wrap=True
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“¤ å¯¼å‡ºæ–‡æ¡£")
                        gr.HTML("<p style='color: #6c757d;'>å¯¼å‡ºæ‰€æœ‰æ–‡æ¡£åˆ°JSONæ–‡ä»¶ï¼ŒåŒ…å«æ–‡æ¡£IDå’Œå†…å®¹</p>")
                        export_docs_btn = gr.Button("ğŸ“¤ å¯¼å‡ºæ‰€æœ‰æ–‡æ¡£", variant="primary")
                        export_download = gr.File(label="ä¸‹è½½æ–‡æ¡£æ–‡ä»¶", interactive=False)
                        export_result = gr.Textbox(label="å¯¼å‡ºç»“æœ", interactive=False)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“¥ å¯¼å…¥æ–‡æ¡£")
                        gr.HTML("<p style='color: #6c757d;'>ä¸Šä¼ JSONæ–‡ä»¶å¯¼å…¥æ–‡æ¡£ï¼Œå°†æ›¿æ¢ç°æœ‰ç´¢å¼•</p>")
                        import_file = gr.File(
                            label="é€‰æ‹©æ–‡æ¡£æ–‡ä»¶", 
                            file_types=[".json"],
                            file_count="single"
                        )
                        import_docs_btn = gr.Button("ğŸ“¥ å¯¼å…¥æ–‡æ¡£å¹¶æ›´æ–°ç´¢å¼•", variant="primary")
                        import_result = gr.Textbox(label="å¯¼å…¥ç»“æœ", interactive=False)
        
        # ç»‘å®šäº‹ä»¶
        # ç´¢å¼•ä¿¡æ¯ç›¸å…³
        index_stats_btn.click(
            fn=lambda: show_index_stats(search_engine), 
            outputs=index_stats_output
        )
        index_quality_btn.click(
            fn=lambda: check_index_quality(search_engine), 
            outputs=index_quality_output
        )
        view_index_btn.click(
            fn=lambda: view_inverted_index(search_engine), 
            outputs=view_index_output
        )
        
        # æ–‡æ¡£ç®¡ç†ç›¸å…³
        refresh_docs_btn.click(
            fn=lambda: get_all_documents(search_engine),
            outputs=docs_list
        )
        
        export_docs_btn.click(
            fn=lambda: export_documents(search_engine),
            outputs=[export_download, export_result]
        )
        
        import_docs_btn.click(
            fn=lambda file: import_documents_from_file(search_engine, file),
            inputs=import_file,
            outputs=import_result
        )
        
        # é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨åˆ·æ–°æ–‡æ¡£åˆ—è¡¨
        index_tab.load(
            fn=lambda: get_all_documents(search_engine),
            outputs=docs_list
        )
    
    return index_tab 