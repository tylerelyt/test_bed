#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæ ‡ç­¾é¡µUIå®ç°
"""

import gradio as gr
import json
from typing import Dict, Any, Tuple, List
from .rag_service import RAGService

def build_rag_tab(index_service, inference_model=None):
    """æ„å»ºRAGæ ‡ç­¾é¡µ
    
    Args:
        index_service: ç´¢å¼•æœåŠ¡
        inference_model: å…±äº«çš„InferenceModelå®ä¾‹ï¼ˆå¯é€‰ï¼‰
    """
    
    # åˆå§‹åŒ–RAGæœåŠ¡
    rag_service = RAGService(index_service)
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥inference_modelï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
    if inference_model is None:
        from ..training_tab.inference_model import InferenceModel
        inference_model = InferenceModel()
    
    with gr.Column():
        gr.Markdown("""
        # ğŸ¤– ä¸Šä¸‹æ–‡å·¥ç¨‹
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        - **DashScope API**: ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®APIï¼ˆåœ¨çº¿ï¼‰
        - **æœ¬åœ°æ¨¡å‹**: ä½¿ç”¨è®­ç»ƒå¥½çš„SFT/DPOæ¨¡å‹ï¼ˆéœ€å…ˆåŠ è½½ï¼‰
        """)
        
        # 1. æ¨¡å‹é€‰æ‹©ä¸åŠ è½½
        with gr.Row():
            with gr.Column(scale=2):
                inference_mode = gr.Radio(
                    choices=["DashScope API", "æœ¬åœ°æ¨¡å‹"],
                    value="DashScope API",
                    label="æ¨ç†æ¨¡å¼"
                )
                
                # æœ¬åœ°æ¨¡å‹é€‰æ‹©ï¼ˆä»…åœ¨é€‰æ‹©"æœ¬åœ°æ¨¡å‹"æ—¶æ˜¾ç¤ºï¼‰
                with gr.Column(visible=False) as local_model_box:
                    with gr.Row():
                        local_model_dropdown = gr.Dropdown(
                            choices=[],  # åˆå§‹ä¸ºç©ºï¼Œé€šè¿‡refreshæ›´æ–°
                            value=None,
                            label="é€‰æ‹©æœ¬åœ°æ¨¡å‹",
                            info="ä»SFTæˆ–DPOè®­ç»ƒçš„æ¨¡å‹ä¸­é€‰æ‹©",
                            scale=4
                        )
                        refresh_local_models_btn = gr.Button("ğŸ”„", scale=1)
                    
                    with gr.Row():
                        load_model_btn = gr.Button("â–¶ï¸ åŠ è½½æ¨¡å‹", variant="primary")
                        unload_model_btn = gr.Button("â¹ï¸ å¸è½½æ¨¡å‹", variant="secondary")
            
            with gr.Column(scale=1):
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€",
                    value="DashScope API æ¨¡å¼ï¼ˆæ— éœ€åŠ è½½ï¼‰",
                    interactive=False,
                    lines=4
            )
        
        # 2. æŸ¥è¯¢ç•Œé¢
        with gr.Row():
            with gr.Column(scale=2):
                query_input = gr.Textbox(
                    label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                    placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                    lines=2
                )
                
                with gr.Row():
                    top_k_slider = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=5,
                        step=1,
                        label="æ£€ç´¢æ–‡æ¡£æ•°é‡"
                    )

                with gr.Row():
                    retrieval_enabled = gr.Checkbox(
                        label="å¼€å¯æ£€ç´¢å¢å¼º (RAG)",
                        value=True
                    )
                    multi_step_enabled = gr.Checkbox(
                        label="å¼€å¯å¤šæ­¥æ¨ç†",
                        value=False
                    )
                
                rag_query_btn = gr.Button("ğŸš€ æ‰§è¡ŒæŸ¥è¯¢", variant="primary")
                
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
                stats_display = gr.JSON(label="ä¸Šä¸‹æ–‡å·¥ç¨‹æœåŠ¡çŠ¶æ€")
        
        # 3. ç»“æœå±•ç¤º
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ“ ç”Ÿæˆå›ç­”")
                answer_output = gr.Textbox(
                    label="å›ç­”",
                    lines=10,
                    max_lines=15,
                    interactive=False,
                    show_copy_button=True
                )
                
                processing_info = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=2,
                    interactive=False
                )
        
        # 4. æç¤ºè¯å±•ç¤º
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ“ æç¤ºè¯/æ¨ç†è½¨è¿¹")
                prompt_display = gr.Textbox(
                    label="å®Œæ•´æç¤ºè¯æˆ–æ¨ç†è½¨è¿¹",
                    lines=20,
                    max_lines=30,
                    interactive=False,
                    placeholder="æ‰§è¡ŒæŸ¥è¯¢åï¼Œè¿™é‡Œæ˜¾ç¤ºå‘é€ç»™LLMçš„æç¤ºè¯æˆ–ReActæ¨ç†è½¨è¿¹",
                    show_copy_button=True,
                    autoscroll=False
                )
        
        # 5. æ£€ç´¢è¯¦æƒ…
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ” æ£€ç´¢ç»“æœè¯¦æƒ…")
                retrieved_docs = gr.DataFrame(
                    headers=["æ–‡æ¡£ID", "ç›¸å…³åº¦åˆ†æ•°", "æ–‡æ¡£å†…å®¹"],
                    label="æ£€ç´¢åˆ°çš„æ–‡æ¡£",
                    interactive=False
                )
                
                context_output = gr.Textbox(
                    label="æ„å»ºçš„ä¸Šä¸‹æ–‡",
                    lines=12,
                    max_lines=20,
                    interactive=False,
                    show_copy_button=True
                )
    
    # äº‹ä»¶å¤„ç†å‡½æ•°
    def refresh_local_models():
        """åˆ·æ–°æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
        try:
            from ..training_tab.llmops_tab import get_trained_models
            sft_models = get_trained_models("sft")
            dpo_models = get_trained_models("dpo")
            all_models = sft_models + dpo_models
            return gr.update(choices=all_models, value=all_models[0] if all_models else None)
        except Exception as e:
            print(f"âŒ åˆ·æ–°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return gr.update(choices=[], value=None)
    
    def toggle_model_box(mode):
        """åˆ‡æ¢æ¨ç†æ¨¡å¼æ—¶æ˜¾ç¤º/éšè—æœ¬åœ°æ¨¡å‹é€‰æ‹©æ¡†"""
        if mode == "æœ¬åœ°æ¨¡å‹":
            status = "è¯·é€‰æ‹©å¹¶åŠ è½½æœ¬åœ°æ¨¡å‹" if not inference_model.loaded else "æ¨¡å‹å·²åŠ è½½"
            # åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨åˆ·æ–°æ¨¡å‹åˆ—è¡¨
            return gr.update(visible=True), status, refresh_local_models()
        else:
            return gr.update(visible=False), "DashScope API æ¨¡å¼ï¼ˆæ— éœ€åŠ è½½ï¼‰", gr.update()
    
    def load_local_model(model_path):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        if not model_path:
            yield "âŒ è¯·é€‰æ‹©æ¨¡å‹"
            return
        
        base_model = "Qwen/Qwen2-0.5B"
        for msg in inference_model.load_model(
            base_model=base_model,
            adapter_path=model_path,
            template="qwen"
        ):
            yield msg
    
    def unload_local_model():
        """å¸è½½æœ¬åœ°æ¨¡å‹"""
        for msg in inference_model.unload_model():
            yield msg
    
    def get_rag_stats():
        """è·å–RAGæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        return rag_service.get_stats()
    
    def process_rag_query(query: str, top_k: int, mode: str, retrieval_enabled_flag: bool, multi_step_flag: bool):
        """å¤„ç†RAGæŸ¥è¯¢ï¼ˆæ”¯æŒDashScope APIå’Œæœ¬åœ°æ¨¡å‹ï¼‰"""
        if not query.strip():
            return (
                "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                "æœªå¤„ç†",
                [],
                "",
                ""
            )
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ¨ç†æ–¹å¼
        if mode == "DashScope API":
            # ä½¿ç”¨DashScope API
            result = rag_service.rag_query(
            query=query,
            top_k=top_k,
                model="qwen-plus",  # ä½¿ç”¨é€šä¹‰åƒé—®
            retrieval_enabled=retrieval_enabled_flag,
            multi_step=multi_step_flag
        )
        else:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            if not inference_model.loaded:
                return (
                    "âŒ è¯·å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹\n\nç‚¹å‡»ä¸Šæ–¹çš„ã€Œâ–¶ï¸ åŠ è½½æ¨¡å‹ã€æŒ‰é’®",
                    "æœªå¤„ç†",
                    [],
                    "",
                    ""
                )
            
            # æ£€ç´¢æ–‡æ¡£
            if retrieval_enabled_flag:
                docs = rag_service.index_service.search(query, top_k)
                # docs æ˜¯ List[Tuple[str, float, str]] æ ¼å¼: (doc_id, score, reason/text)
                retrieved_docs = [(doc_id, score, text) for doc_id, score, text in docs]
                context = "\n\n".join([f"æ–‡æ¡£{i+1}: {text}" for i, (doc_id, score, text) in enumerate(docs)])
                
                # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯
                prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç»™å‡ºè¯¦ç»†çš„å›ç­”ï¼š"""
            else:
                retrieved_docs = []
                context = ""
                prompt = query
            
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå›ç­”
            import time
            start_time = time.time()
            
            answer = inference_model.generate_once(
                prompt=prompt,
                temperature=0.7,
                max_new_tokens=512
            )
            
            processing_time = time.time() - start_time
            processing_info = f"""å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
æ¨ç†æ¨¡å¼: æœ¬åœ°æ¨¡å‹
æ£€ç´¢æ–‡æ¡£æ•°: {len(retrieved_docs)}"""
            
            # æ„å»ºæ£€ç´¢ç»“æœè¡¨æ ¼
            retrieved_table = []
            for doc_id, score, content in retrieved_docs:
                truncated_content = content[:100] + "..." if len(content) > 100 else content
                retrieved_table.append([doc_id, f"{score:.4f}", truncated_content])
            
            return (
                answer,
                processing_info,
                retrieved_table,
                context,
                prompt
            )
        
        # æ„å»ºæ£€ç´¢ç»“æœè¡¨æ ¼
        retrieved_table = []
        for doc_id, score, content in result.get("retrieved_docs", []):
            # æˆªæ–­å†…å®¹ä»¥é€‚åº”è¡¨æ ¼æ˜¾ç¤º
            truncated_content = content[:100] + "..." if len(content) > 100 else content
            retrieved_table.append([doc_id, f"{score:.4f}", truncated_content])
        
        # æ„å»ºå¤„ç†ä¿¡æ¯
        processing_info = f"""å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}ç§’
æ¨ç†æ¨¡å¼: {mode}
æ£€ç´¢æ–‡æ¡£æ•°: {len(result.get('retrieved_docs', []))}"""
        
        return (
            result.get("answer", "ç”Ÿæˆå›ç­”å¤±è´¥"),
            processing_info,
            retrieved_table,
            result.get("context", ""),
            result.get("prompt_sent", "")
        )
    
    # ç»‘å®šäº‹ä»¶
    
    # æ¨ç†æ¨¡å¼åˆ‡æ¢äº‹ä»¶
    inference_mode.change(
        fn=toggle_model_box,
        inputs=[inference_mode],
        outputs=[local_model_box, model_status, local_model_dropdown]
    )
    
    # åˆ·æ–°æœ¬åœ°æ¨¡å‹åˆ—è¡¨
    refresh_local_models_btn.click(
        fn=refresh_local_models,
        outputs=[local_model_dropdown]
    )
    
    # æœ¬åœ°æ¨¡å‹åŠ è½½/å¸è½½äº‹ä»¶
    load_model_btn.click(
        fn=load_local_model,
        inputs=[local_model_dropdown],
        outputs=[model_status]
    )
    
    unload_model_btn.click(
        fn=unload_local_model,
        outputs=[model_status]
    )
    
    # RAGæŸ¥è¯¢äº‹ä»¶
    rag_query_btn.click(
        fn=process_rag_query,
        inputs=[query_input, top_k_slider, inference_mode, retrieval_enabled, multi_step_enabled],
        outputs=[answer_output, processing_info, retrieved_docs, context_output, prompt_display]
    )
    
    # é¡µé¢åŠ è½½æ—¶è·å–ç»Ÿè®¡ä¿¡æ¯
    stats_display.value = get_rag_stats()
    
    return gr.Column() 