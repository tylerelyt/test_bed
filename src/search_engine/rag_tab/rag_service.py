#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæœåŠ¡æ¨¡å—
åŸºäºç°æœ‰çš„å€’æ’ç´¢å¼•å’ŒTF-IDFå®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ
"""

import json
import requests
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

class RAGService:
    """RAGæœåŠ¡ï¼šåŸºäºå€’æ’ç´¢å¼•çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
    
    def __init__(self, index_service, ollama_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        
        Args:
            index_service: ç´¢å¼•æœåŠ¡å®ä¾‹
            ollama_url: OllamaæœåŠ¡URL
        """
        self.index_service = index_service
        self.ollama_url = ollama_url
        self.default_model = "llama3.1:8b"
        
    def check_ollama_connection(self) -> Tuple[bool, str]:
        """æ£€æŸ¥Ollamaè¿æ¥çŠ¶æ€"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                return True, f"âœ… Ollamaè¿æ¥æˆåŠŸï¼\nå¯ç”¨æ¨¡å‹: {', '.join(model_names)}"
            else:
                return False, f"âŒ Ollamaè¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
        except requests.exceptions.RequestException as e:
            return False, f"âŒ Ollamaè¿æ¥å¤±è´¥: {str(e)}"
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            else:
                return [self.default_model]
        except:
            return [self.default_model]
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:
        """
        ä½¿ç”¨å€’æ’ç´¢å¼•æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›top_kä¸ªæ–‡æ¡£
            
        Returns:
            List[Tuple[str, float, str]]: (doc_id, score, content)
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„ç´¢å¼•æœåŠ¡è¿›è¡Œæ£€ç´¢
            results = self.index_service.search(query, top_k)
            print(f"ğŸ“– æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def generate_answer(self, query: str, context: str, model: Optional[str] = None) -> str:
        """
        ä½¿ç”¨Ollamaç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
            
        # æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
        
        try:
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
            else:
                return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                
        except requests.exceptions.RequestException as e:
            return f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
    
    def generate_answer_with_prompt(self, prompt: str, model: Optional[str] = None) -> str:
        """
        ç›´æ¥ä½¿ç”¨æç¤ºè¯ç”Ÿæˆå›ç­”
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
            
        try:
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
            else:
                return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                
        except requests.exceptions.RequestException as e:
            return f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
    
    def rag_query(self, query: str, top_k: int = 5, model: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆçš„å­—å…¸
        """
        start_time = datetime.now()
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_documents(query, top_k)
        
        if not retrieved_docs:
            return {
                "query": query,
                "retrieved_docs": [],
                "context": "",
                "answer": "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, (doc_id, score, content) in enumerate(retrieved_docs, 1):
            context_parts.append(f"æ–‡æ¡£{i} (ID: {doc_id}, ç›¸å…³åº¦: {score:.4f}):\n{content}")
        
        context = "\n\n".join(context_parts)
        
        # 3. æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
        
        # 4. ç”Ÿæˆå›ç­”
        answer = self.generate_answer_with_prompt(prompt, model)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "query": query,
            "retrieved_docs": retrieved_docs,
            "context": context,
            "answer": answer,
            "processing_time": processing_time,
            "model_used": model or self.default_model,
            "prompt_sent": prompt
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–RAGæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        index_stats = self.index_service.get_stats()
        ollama_connected, ollama_status = self.check_ollama_connection()
        
        return {
            "ollama_connected": ollama_connected,
            "ollama_status": ollama_status,
            "ollama_url": self.ollama_url,
            "available_models": self.get_available_models(),
            "index_stats": index_stats
        } 