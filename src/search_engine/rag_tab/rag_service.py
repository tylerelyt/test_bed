#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæœåŠ¡æ¨¡å—
åŸºäºç°æœ‰çš„å€’æ’ç´¢å¼•å’ŒTF-IDFå®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ
"""

import json
import re
import requests
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

class RAGService:
    """RAGæœåŠ¡ï¼šåŸºäºå€’æ’ç´¢å¼•çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
    
    def __init__(self, index_service, ollama_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        
        Args:
            index_service: ç´¢å¼•æœåŠ¡å®ä¾‹
            ollama_url: OllamaæœåŠ¡URL
        """
        self.index_service = index_service
        self.ollama_url = ollama_url
        self.default_model = "llama3.1:8b"
        
    def check_ollama_connection(self) -> Tuple[bool, str]:
        """æ£€æŸ¥Ollamaè¿æ¥çŠ¶æ€"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                return True, f"âœ… Ollamaè¿æ¥æˆåŠŸï¼\nå¯ç”¨æ¨¡å‹: {', '.join(model_names)}"
            else:
                return False, f"âŒ Ollamaè¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
        except requests.exceptions.RequestException as e:
            return False, f"âŒ Ollamaè¿æ¥å¤±è´¥: {str(e)}"
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            else:
                return [self.default_model]
        except:
            return [self.default_model]
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:
        """
        ä½¿ç”¨å€’æ’ç´¢å¼•æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›top_kä¸ªæ–‡æ¡£
            
        Returns:
            List[Tuple[str, float, str]]: (doc_id, score, content)
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„ç´¢å¼•æœåŠ¡è¿›è¡Œæ£€ç´¢
            results = self.index_service.search(query, top_k)
            print(f"ğŸ“– æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def generate_answer(self, query: str, context: str, model: Optional[str] = None) -> str:
        """
        ä½¿ç”¨Ollamaç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
            
        # æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
        
        try:
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
            else:
                return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                
        except requests.exceptions.RequestException as e:
            return f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
    
    def generate_answer_with_prompt(self, prompt: str, model: Optional[str] = None) -> str:
        """
        ç›´æ¥ä½¿ç”¨æç¤ºè¯ç”Ÿæˆå›ç­”
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if model is None:
            model = self.default_model
            
        try:
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
            else:
                return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                
        except requests.exceptions.RequestException as e:
            return f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
    
    def _react_reasoning(self, query: str, model: Optional[str], retrieval_enabled: bool, top_k: int = 5, max_steps: int = 5) -> Tuple[str, str]:
        """
        ReActé£æ ¼å¤šæ­¥æ¨ç†ï¼šThought -> Action(SEARCH/FINISH) -> Observationï¼Œå¾ªç¯ç›´åˆ°FINISHæˆ–æ­¥æ•°ä¸Šé™ã€‚
        è¿”å› (final_answer, trace_text)
        """
        if model is None:
            model = self.default_model
        
        trace_lines: List[str] = []
        observations: List[str] = []

        tool_desc = (
            "ä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå·¥å…·ï¼šSEARCH(\"æŸ¥è¯¢è¯\")ï¼Œå®ƒä¼šè¿”å›ä¸æŸ¥è¯¢è¯æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ã€‚"
        )
        format_instructions = (
            "æ¯è½®è¯·ä¸¥æ ¼è¾“å‡ºä»¥ä¸‹æ ¼å¼ä¸­çš„ä¸€è¡ŒActionï¼Œä¾¿äºè§£æï¼š\n"
            "Thought: <ä½ çš„ç®€çŸ­æ€è€ƒ>\n"
            "Action: SEARCH(\"<æŸ¥è¯¢è¯>\") æˆ– Action: FINISH(\"<æœ€ç»ˆç­”æ¡ˆ>\")\n"
            "ä¸è¦è¾“å‡ºå…¶ä»–å¤šä½™å†…å®¹ã€‚"
        )

        search_pattern = re.compile(r"Action:\s*SEARCH\(\"([\s\S]*?)\"\)")
        finish_pattern = re.compile(r"Action:\s*FINISH\(\"([\s\S]*?)\"\)")

        scratchpad = ""
        for step in range(1, max_steps + 1):
            prompt = (
                f"ä½ æ˜¯ä¸€ä¸ªä¼šé€æ­¥æ€è€ƒå¹¶åˆç†ä½¿ç”¨å·¥å…·çš„åŠ©æ‰‹ã€‚\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                f"å·¥å…·è¯´æ˜ï¼š{tool_desc}\n"
                f"æ³¨æ„ï¼š{'å½“å‰ç¦æ­¢ä½¿ç”¨SEARCHå·¥å…·ã€‚' if not retrieval_enabled else 'å¯ä»¥ä½¿ç”¨SEARCHå·¥å…·ã€‚'}\n\n"
                f"å†å²æ¨ç†ï¼š\n{scratchpad}\n\n"
                f"è¯·å¼€å§‹ç¬¬{step}æ­¥ã€‚\n{format_instructions}"
            )
            try:
                resp = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={"model": model, "prompt": prompt, "stream": False},
                    timeout=60
                )
                if resp.status_code != 200:
                    trace_lines.append(f"ç³»ç»Ÿ: æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status_code}")
                    break
                text = resp.json().get("response", "").strip()
            except requests.exceptions.RequestException as e:
                trace_lines.append(f"ç³»ç»Ÿ: æ¨¡å‹è°ƒç”¨å¼‚å¸¸ {str(e)}")
                break

            # è®°å½•æ¨¡å‹è¾“å‡º
            trace_lines.append(f"Step {step} æ¨¡å‹è¾“å‡º:\n{text}")

            # è§£æåŠ¨ä½œ
            finish_match = finish_pattern.search(text)
            if finish_match:
                final_answer = finish_match.group(1)
                trace_lines.append("Action: FINISH")
                return final_answer, "\n\n".join(trace_lines)

            search_match = search_pattern.search(text)
            if search_match:
                search_query = search_match.group(1).strip()
                if retrieval_enabled:
                    # æ‰§è¡Œæ£€ç´¢
                    docs = self.retrieve_documents(search_query, top_k=top_k)
                    if not docs:
                        observation = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
                    else:
                        # åªå–å‰3æ¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                        obs_parts = []
                        for i, (doc_id, score, content) in enumerate(docs[:3], 1):
                            snippet = content[:400]
                            obs_parts.append(f"[{i}] id={doc_id} score={score:.4f} snippet={snippet}")
                        observation = "\n".join(obs_parts)
                    observations.append(observation)
                    trace_lines.append(f"Observation:\n{observation}")
                    scratchpad += f"Thought/Action(SEARCH): {search_query}\nObservation: {observation}\n\n"
                    continue
                else:
                    observation = "SEARCHå·¥å…·è¢«ç¦ç”¨ã€‚è¯·ç›´æ¥FINISHã€‚"
                    observations.append(observation)
                    trace_lines.append(f"Observation:\n{observation}")
                    scratchpad += f"Action(SEARCHè¢«æ‹’): {search_query}\nObservation: {observation}\n\n"
                    continue

            # è‹¥æ— æ³•è§£æåŠ¨ä½œï¼Œæç¤ºå¹¶ç»§ç»­ä¸‹ä¸€æ­¥
            notice = "æœªè§£æåˆ°æœ‰æ•ˆçš„Actionï¼Œè¯·æŒ‰æ ¼å¼è¾“å‡ºã€‚"
            trace_lines.append(f"ç³»ç»Ÿ: {notice}")
            scratchpad += f"ç³»ç»Ÿæç¤º: {notice}\n\n"

        # æœªæ˜¾å¼FINISHæ—¶ï¼Œå°è¯•è®©æ¨¡å‹åŸºäºè§‚å¯Ÿåšæœ€ç»ˆæ€»ç»“
        summary_context = "\n\n".join(observations[-3:]) if observations else ""
        final_prompt = (
            f"è¯·åŸºäºä»¥ä¸‹è§‚å¯Ÿä¸ä½ å·²æœ‰çš„æ¨ç†ï¼Œç»™å‡ºé—®é¢˜çš„æœ€ç»ˆä¸­æ–‡ç­”æ¡ˆã€‚è‹¥è§‚å¯Ÿä¸ºç©ºï¼Œè¯·ç›´æ¥æ ¹æ®å¸¸è¯†ä½œç­”ã€‚\n\n"
            f"é—®é¢˜ï¼š{query}\n\n"
            f"è§‚å¯Ÿï¼š\n{summary_context}\n\n"
            f"è¯·ç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œä¸è¦å†è¾“å‡ºæ€ç»´è¿‡ç¨‹ã€‚"
        )
        try:
            final_resp = requests.post(
                f"{self.ollama_url}/api/generate",
                json={"model": model, "prompt": final_prompt, "stream": False},
                timeout=60
            )
            if final_resp.status_code != 200:
                answer = f"âŒ å¤šæ­¥æ¨ç†æ€»ç»“å¤±è´¥ï¼ŒçŠ¶æ€ç : {final_resp.status_code}"
            else:
                answer = final_resp.json().get("response", "ç”Ÿæˆå›ç­”å¤±è´¥")
        except requests.exceptions.RequestException as e:
            answer = f"âŒ è°ƒç”¨Ollamaå¤±è´¥: {str(e)}"
        trace_lines.append("ç³»ç»Ÿ: æœªæ£€æµ‹åˆ°FINISHï¼Œå·²è¿›è¡Œè‡ªåŠ¨æ€»ç»“ã€‚")
        return answer, "\n\n".join(trace_lines)

    def rag_query(self, query: str, top_k: int = 5, model: Optional[str] = None, retrieval_enabled: bool = True, multi_step: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            model: ä½¿ç”¨çš„æ¨¡å‹
            retrieval_enabled: æ˜¯å¦å¼€å¯æ£€ç´¢å¢å¼º
            multi_step: æ˜¯å¦å¼€å¯å¤šæ­¥æ¨ç†
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆçš„å­—å…¸
        """
        start_time = datetime.now()
        
        # å¦‚æœå…³é—­æ£€ç´¢ä¸å¤šæ­¥æ¨ç†ï¼Œåˆ™ç›´æ¥é—® LLMï¼ˆæ— ä¸Šä¸‹æ–‡ç›´è¿ï¼‰
        if not retrieval_enabled and not multi_step:
            direct_prompt = f"è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\né—®é¢˜ï¼š{query}"
            answer = self.generate_answer_with_prompt(direct_prompt, model)
            return {
                "query": query,
                "retrieved_docs": [],
                "context": "",
                "answer": answer,
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "model_used": model or self.default_model,
                "prompt_sent": direct_prompt
            }

        # 1) è‹¥å¼€å¯æ£€ç´¢ï¼Œå…ˆæ£€ç´¢å¹¶æ„å»ºä¸Šä¸‹æ–‡ï¼›å¦åˆ™ä¸Šä¸‹æ–‡ä¸ºç©º
        retrieved_docs = []
        context = ""
        if retrieval_enabled:
            retrieved_docs = self.retrieve_documents(query, top_k)
            # å³ä½¿æœªæ£€ç´¢åˆ°æ–‡æ¡£ï¼Œä¹Ÿç»§ç»­ï¼Œè®©æ¨¡å‹ç›´æ¥å›ç­”æˆ–å¤šæ­¥æ¨ç†
            if retrieved_docs:
                context_parts = []
                for i, (doc_id, score, content) in enumerate(retrieved_docs, 1):
                    context_parts.append(f"æ–‡æ¡£{i} (ID: {doc_id}, ç›¸å…³åº¦: {score:.4f}):\n{content}")
                context = "\n\n".join(context_parts)

        # 2) ç”Ÿæˆå›ç­”ï¼šå¤šæ­¥æ¨ç†ä¼˜å…ˆï¼Œå¦åˆ™æ™®é€šå•æ­¥å›ç­”
        if multi_step:
            answer, trace_text = self._react_reasoning(
                query=query,
                model=model,
                retrieval_enabled=retrieval_enabled,
                top_k=top_k
            )
            prompt_used = trace_text  # å°†å®Œæ•´æ¨ç†è½¨è¿¹å›æ˜¾
        else:
            # æ„å»ºæ ‡å‡†æç¤º
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”ã€‚
            
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}
            
ç”¨æˆ·é—®é¢˜ï¼š{query}
            
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
            answer = self.generate_answer_with_prompt(prompt, model)
            prompt_used = prompt
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "query": query,
            "retrieved_docs": retrieved_docs,
            "context": context,
            "answer": answer,
            "processing_time": processing_time,
            "model_used": model or self.default_model,
            "prompt_sent": prompt_used if prompt_used is not None else "å¤šæ­¥æ¨ç†ï¼ˆå†…éƒ¨å¤šæç¤ºï¼‰"
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–RAGæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        index_stats = self.index_service.get_stats()
        ollama_connected, ollama_status = self.check_ollama_connection()
        
        return {
            "ollama_connected": ollama_connected,
            "ollama_status": ollama_status,
            "ollama_url": self.ollama_url,
            "available_models": self.get_available_models(),
            "index_stats": index_stats
        } 