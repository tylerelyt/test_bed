# ðŸ”„ LLMOps é—­çŽ¯ç³»ç»ŸæŠ€æœ¯æŒ‡å— ([è¿”å›žREADME](../README.md))

## ðŸ“‹ ç›®å½•

- [1. ç³»ç»Ÿæ¦‚è¿°](#1-ç³»ç»Ÿæ¦‚è¿°)
- [2. æŠ€æœ¯æž¶æž„è®¾è®¡](#2-æŠ€æœ¯æž¶æž„è®¾è®¡)
- [3. è®­ç»ƒæµç¨‹è¯¦è§£](#3-è®­ç»ƒæµç¨‹è¯¦è§£)
- [4. æ•°æ®ç”Ÿæˆä¸Žç®¡ç†](#4-æ•°æ®ç”Ÿæˆä¸Žç®¡ç†)
- [5. æ ¸å¿ƒç»„ä»¶å®žçŽ°](#5-æ ¸å¿ƒç»„ä»¶å®žçŽ°)
- [6. åœ¨çº¿é—­çŽ¯ä¼˜åŒ–](#6-åœ¨çº¿é—­çŽ¯ä¼˜åŒ–)
- [7. é…ç½®ä¸Žéƒ¨ç½²](#7-é…ç½®ä¸Žéƒ¨ç½²)
- [8. ä½¿ç”¨æŒ‡å—](#8-ä½¿ç”¨æŒ‡å—)
- [9. æœ€ä½³å®žè·µ](#9-æœ€ä½³å®žè·µ)
- [10. æ•…éšœæŽ’é™¤](#10-æ•…éšœæŽ’é™¤)

---

## 1. ç³»ç»Ÿæ¦‚è¿°

### 1.1 åŠŸèƒ½ä»‹ç»

LLMOps é—­çŽ¯ç³»ç»Ÿæ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒä¸Žä¼˜åŒ–å¹³å°ï¼Œæ”¯æŒï¼š

- ðŸ”„ **å®Œæ•´è®­ç»ƒæµç¨‹**ï¼šCPT â†’ SFT â†’ DPO ä¸‰é˜¶æ®µè®­ç»ƒ
- ðŸ“Š **æ•°æ®ç”Ÿæˆç®¡é“**ï¼šSelf-Instruct è‡ªåŠ¨ç”Ÿæˆã€åå¥½æ•°æ®æ”¶é›†
- ðŸ” **åœ¨çº¿é—­çŽ¯ä¼˜åŒ–**ï¼šA/B æµ‹è¯• â†’ åå¥½æ”¶é›† â†’ DPO è®­ç»ƒ â†’ éƒ¨ç½²
- ðŸ“ˆ **å®žéªŒç®¡ç†**ï¼šé…ç½®ç®¡ç†ã€è®­ç»ƒç›‘æŽ§ã€ç»“æžœå¯¹æ¯”
- ðŸŽ¯ **ç¦»çº¿/åœ¨çº¿æ¨¡å¼**ï¼šæ”¯æŒå¤§ç‰ˆæœ¬å¼€å‘å’ŒæŒç»­ä¼˜åŒ–

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

#### ç¦»çº¿è®­ç»ƒ vs åœ¨çº¿ä¼˜åŒ–

| ç‰¹æ€§ | ç¦»çº¿è®­ç»ƒ | åœ¨çº¿ä¼˜åŒ– |
|------|---------|---------|
| **ç”¨é€”** | å¤§ç‰ˆæœ¬å¼€å‘ï¼ˆv1.0 â†’ v2.0ï¼‰ | æŒç»­è¿­ä»£ï¼ˆv1.0 â†’ v1.1ï¼‰ |
| **é˜¶æ®µ** | CPT + SFT | DPO |
| **é¢‘çŽ‡** | æ•°æœˆä¸€æ¬¡ | æ¯å‘¨/æœˆä¸€æ¬¡ |
| **æˆæœ¬** | é«˜ï¼ˆå…¨å‚æ•°æˆ–å¤§è§„æ¨¡ LoRAï¼‰ | ä½Žï¼ˆä»… LoRAï¼‰ |
| **æ•°æ®** | å¤§è§„æ¨¡è¯­æ–™ + æŒ‡ä»¤é›† | çœŸå®žç”¨æˆ·åé¦ˆ |
| **äº§å‡º** | Completion + Chat æ¨¡åž‹ | Chat æ¨¡åž‹å¢žé‡ç‰ˆæœ¬ |

### 1.3 ç³»ç»Ÿç‰¹ç‚¹

- âœ… **é›†æˆ LLaMA-Factory**ï¼šåŸºäºŽæˆç†Ÿçš„è®­ç»ƒæ¡†æž¶
- âœ… **Web å¯è§†åŒ–ç•Œé¢**ï¼šåŸºäºŽ Gradio çš„å‹å¥½äº¤äº’
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ•°æ®ã€è®­ç»ƒã€æŽ¨ç†åˆ†ç¦»
- âœ… **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šé…ç½®ç”Ÿæˆã€æ•°æ®è½¬æ¢ã€ç»“æžœè®°å½•
- âœ… **æ¼”ç¤ºæ¨¡å¼**ï¼šæ— éœ€ GPU å³å¯ä½“éªŒå®Œæ•´æµç¨‹

---

## 2. æŠ€æœ¯æž¶æž„è®¾è®¡

### 2.1 æ•´ä½“æž¶æž„å›¾

```mermaid
graph TB
    subgraph "æ•°æ®å±‚ - Data Layer"
        A1[é¢†åŸŸè¯­æ–™] --> A[DomainCorpusProcessor]
        A2[é¢„ç½®æ–‡æ¡£] --> A
        A --> A3[domain_corpus.jsonl]
        
        B1[ç§å­æŒ‡ä»¤] --> B[SelfInstructGenerator]
        B --> B2[sft_data.json]
        
        C1[ç”¨æˆ·åé¦ˆ] --> C[PreferenceCollector]
        C2[A/B æµ‹è¯•] --> C
        C --> C3[prefs.jsonl]
    end
    
    subgraph "é…ç½®å±‚ - Config Layer"
        D[LLaMAFactoryConfig]
        D --> D1[CPT Config]
        D --> D2[SFT Config]
        D --> D3[DPO Config]
    end
    
    subgraph "è®­ç»ƒå±‚ - Training Layer"
        E[LLaMAFactoryTrainer]
        A3 --> E
        B2 --> E
        C3 --> E
        D1 --> E
        D2 --> E
        D3 --> E
        
        E --> E1[CPT Training]
        E --> E2[SFT Training]
        E --> E3[DPO Training]
    end
    
    subgraph "æ¨¡åž‹å±‚ - Model Layer"
        E1 --> F1[Completion Model]
        E2 --> F2[Chat Model v1.0]
        E3 --> F3[Chat Model v1.1]
    end
    
    subgraph "æŽ¨ç†å±‚ - Inference Layer"
        G[InferenceModel]
        F1 --> G
        F2 --> G
        F3 --> G
        
        G --> G1[A/B æµ‹è¯•å¯¹æ¯”]
        G --> G2[ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²]
    end
    
    subgraph "åé¦ˆå±‚ - Feedback Layer"
        G1 --> H[ç”¨æˆ·è¯„åˆ†]
        H --> C
    end
    
    subgraph "å¼•æ“Žå±‚ - Engine Layer"
        I[LLMOpsEngine]
        I --> I1[Manager]
        I --> I2[Runner]
        I --> I3[Control]
    end
```

### 2.2 ç›®å½•ç»“æž„

```
src/search_engine/training_tab/
â”œâ”€â”€ llmops_tab.py                    # ä¸»ç•Œé¢å…¥å£
â”œâ”€â”€ llmops_engine.py                 # å¼•æ“Žåè°ƒå™¨
â”œâ”€â”€ llmops_manager.py                # ç»„ä»¶ç®¡ç†å™¨
â”œâ”€â”€ llmops_runner.py                 # è®­ç»ƒè¿è¡Œå™¨
â”œâ”€â”€ llmops_control.py                # æŽ§åˆ¶é€»è¾‘
â”œâ”€â”€ llmops_models.py                 # æ¨¡åž‹é…ç½®
â”œâ”€â”€ llama_factory_config.py          # LLaMA-Factory é…ç½®ç”Ÿæˆ
â”œâ”€â”€ llamafactory_trainer.py          # è®­ç»ƒæœåŠ¡
â”œâ”€â”€ self_instruct_generator.py       # Self-Instruct æ•°æ®ç”Ÿæˆ
â”œâ”€â”€ domain_corpus_processor.py       # é¢†åŸŸè¯­æ–™å¤„ç†
â”œâ”€â”€ preference_collector.py          # åå¥½æ•°æ®æ”¶é›†
â”œâ”€â”€ inference_model.py               # æŽ¨ç†æœåŠ¡
â””â”€â”€ README_LLMOPS.md                 # å¿«é€Ÿå…¥é—¨

data/llmops/
â”œâ”€â”€ dataset_info.json                # æ•°æ®é›†æ³¨å†Œè¡¨
â”œâ”€â”€ cpt/                            # CPT æ•°æ®
â”‚   â””â”€â”€ domain_corpus_*.jsonl
â”œâ”€â”€ sft/                            # SFT æ•°æ®
â”‚   â””â”€â”€ sft_data_*.json
â””â”€â”€ dpo/                            # DPO æ•°æ®
    â””â”€â”€ prefs.jsonl

checkpoints/
â”œâ”€â”€ cpt/                            # CPT æ¨¡åž‹è¾“å‡º
â”œâ”€â”€ sft/                            # SFT æ¨¡åž‹è¾“å‡º
â””â”€â”€ dpo/                            # DPO æ¨¡åž‹è¾“å‡º

configs/llmops/
â”œâ”€â”€ cpt_config_*.yaml               # CPT é…ç½®æ–‡ä»¶
â”œâ”€â”€ sft_config_*.yaml               # SFT é…ç½®æ–‡ä»¶
â””â”€â”€ dpo_config_*.yaml               # DPO é…ç½®æ–‡ä»¶
```

### 2.3 æ•°æ®æµè®¾è®¡

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant UI as Webç•Œé¢
    participant DG as æ•°æ®ç”Ÿæˆå™¨
    participant TC as è®­ç»ƒé…ç½®
    participant TR as è®­ç»ƒå™¨
    participant IF as æŽ¨ç†æœåŠ¡
    participant PC as åå¥½æ”¶é›†
    
    Note over U,PC: ç¦»çº¿è®­ç»ƒæµç¨‹
    U->>UI: 1. ç”Ÿæˆé¢†åŸŸè¯­æ–™
    UI->>DG: å¤„ç†æ–‡æ¡£
    DG-->>UI: domain_corpus.jsonl
    
    U->>UI: 2. ç”ŸæˆæŒ‡ä»¤æ•°æ®
    UI->>DG: Self-Instruct
    DG-->>UI: sft_data.json
    
    U->>UI: 3. é…ç½® CPT/SFT
    UI->>TC: ç”Ÿæˆé…ç½®æ–‡ä»¶
    TC-->>U: YAML é…ç½®
    
    U->>TR: 4. æ‰§è¡Œè®­ç»ƒ
    TR-->>TR: CPT â†’ Completion Model
    TR-->>TR: SFT â†’ Chat Model v1.0
    
    Note over U,PC: åœ¨çº¿ä¼˜åŒ–æµç¨‹
    U->>UI: 5. A/B æµ‹è¯•
    UI->>IF: è°ƒç”¨ä¸¤ä¸ªæ¨¡åž‹
    IF-->>UI: å¯¹æ¯”ç»“æžœ
    U->>UI: é€‰æ‹©æ›´å¥½çš„å›žç­”
    UI->>PC: è®°å½•åå¥½
    PC-->>PC: ç´¯ç§¯æ•°æ®
    
    Note over PC: æ¯å‘¨/æœˆä¸€æ¬¡
    U->>UI: 6. DPO è®­ç»ƒ
    UI->>TC: ç”Ÿæˆ DPO é…ç½®
    U->>TR: æ‰§è¡Œ DPO
    TR-->>TR: Chat Model v1.1
    
    Note over U,PC: æŒç»­è¿­ä»£
    U->>IF: 7. éƒ¨ç½²æ–°ç‰ˆæœ¬
    IF-->>IF: æ›´æ–°æŽ¨ç†æœåŠ¡
```

---

## 3. è®­ç»ƒæµç¨‹è¯¦è§£

### 3.1 CPT - ç»§ç»­é¢„è®­ç»ƒ (Continued Pre-Training)

#### 3.1.1 ç›®æ ‡ä¸Žç”¨é€”

**è®­ç»ƒç›®æ ‡**ï¼š
- åœ¨åŸºç¡€æ¨¡åž‹ä¸Šæ³¨å…¥é¢†åŸŸçŸ¥è¯†
- æå‡ç‰¹å®šé¢†åŸŸçš„è¯­è¨€ç†è§£èƒ½åŠ›
- ä¸ºåŽç»­ SFT æ‰“ä¸‹é¢†åŸŸåŸºç¡€

**é€‚ç”¨åœºæ™¯**ï¼š
- åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èžç­‰ä¸“ä¸šé¢†åŸŸ
- ç‰¹å®šè¯­è¨€æˆ–æ–¹è¨€é€‚é…
- ä»£ç ç”Ÿæˆç­‰ä¸“é¡¹ä»»åŠ¡

**äº§å‡ºæ¨¡åž‹**ï¼š
- **Completion Model**ï¼šå¯ç›´æŽ¥ç”¨äºŽæ–‡æœ¬è¡¥å…¨ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡

#### 3.1.2 æ•°æ®è¦æ±‚

```python
# CPT æ•°æ®æ ¼å¼ï¼šçº¯æ–‡æœ¬
{
    "text": "è¿™æ˜¯ä¸€æ®µé¢†åŸŸæ–‡æœ¬ã€‚å¯ä»¥æ˜¯å®Œæ•´çš„æ®µè½ã€æ–‡æ¡£æˆ–ä»£ç ç‰‡æ®µã€‚"
}
```

**æ•°æ®ç‰¹ç‚¹**ï¼š
- å¤§è§„æ¨¡ï¼šé€šå¸¸ 10M-100M tokens
- é«˜è´¨é‡ï¼šé¢†åŸŸç›¸å…³æ€§å¼º
- æ— æ ‡æ³¨ï¼šä¸éœ€è¦é—®ç­”å¯¹

#### 3.1.3 æ ¸å¿ƒé…ç½®

```python
# æ–‡ä»¶: src/search_engine/training_tab/llama_factory_config.py
@staticmethod
def create_cpt_config(
    model_name: str = "Qwen/Qwen2-1.5B",
    dataset: str = "domain_corpus",
    output_dir: str = "checkpoints/cpt/domain-cpt",
    num_train_epochs: int = 1,
    learning_rate: float = 1e-5,
    per_device_train_batch_size: int = 2,
    gradient_accumulation_steps: int = 8,
    max_seq_length: int = 2048,
    lora_r: int = 16,
    lora_alpha: int = 32
) -> Dict[str, Any]:
    """CPT é…ç½®ç”Ÿæˆ"""
    return {
        "stage": "pt",  # pretrain
        "model_name_or_path": model_name,
        "dataset": dataset,
        "finetuning_type": "lora",
        "lora_rank": lora_r,
        "lora_alpha": lora_alpha,
        "num_train_epochs": num_train_epochs,
        "learning_rate": learning_rate,
        ...
    }
```

#### 3.1.4 è®­ç»ƒæµç¨‹

```bash
# 1. ç”Ÿæˆé…ç½®
llamafactory-cli export configs/llmops/cpt_config_20231115.yaml

# 2. æ‰§è¡Œè®­ç»ƒ
llamafactory-cli train configs/llmops/cpt_config_20231115.yaml

# 3. è®­ç»ƒè¾“å‡º
checkpoints/cpt/domain-cpt/
â”œâ”€â”€ adapter_config.json      # LoRA é…ç½®
â”œâ”€â”€ adapter_model.safetensors # LoRA æƒé‡
â”œâ”€â”€ trainer_log.jsonl        # è®­ç»ƒæ—¥å¿—
â””â”€â”€ training_args.bin        # è®­ç»ƒå‚æ•°
```

### 3.2 SFT - ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)

#### 3.2.1 ç›®æ ‡ä¸Žç”¨é€”

**è®­ç»ƒç›®æ ‡**ï¼š
- å°†åŸºç¡€æ¨¡åž‹è½¬æ¢ä¸ºå¯¹è¯æ¨¡åž‹
- å­¦ä¹ æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- é€‚é…ç‰¹å®šä»»åŠ¡æ ¼å¼

**é€‚ç”¨åœºæ™¯**ï¼š
- é€šç”¨å¯¹è¯åŠ©æ‰‹
- ä»»åŠ¡åž‹å¯¹è¯ç³»ç»Ÿ
- å¤šè½®å¯¹è¯åœºæ™¯

**äº§å‡ºæ¨¡åž‹**ï¼š
- **Chat Model**ï¼šå¯ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒçš„å¯¹è¯æœåŠ¡

#### 3.2.2 æ•°æ®è¦æ±‚

```python
# SFT æ•°æ®æ ¼å¼ï¼šmessages æ ¼å¼ï¼ˆOpenAI é£Žæ ¼ï¼‰
{
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸ..."
        }
    ]
}
```

**æ•°æ®ç‰¹ç‚¹**ï¼š
- ä¸­ç­‰è§„æ¨¡ï¼š10K-100K æ ·æœ¬
- é«˜è´¨é‡ï¼šäººå·¥æ ‡æ³¨æˆ– GPT ç”Ÿæˆ
- å¤šæ ·æ€§ï¼šè¦†ç›–å¤šç§ä»»åŠ¡ç±»åž‹

#### 3.2.3 æ ¸å¿ƒé…ç½®

```python
@staticmethod
def create_sft_config(
    model_name: str = "Qwen/Qwen2-1.5B",
    dataset: str = "sft_data",
    output_dir: str = "checkpoints/sft/sft-lora",
    num_train_epochs: int = 3,
    learning_rate: float = 5e-5,
    template: str = "qwen"
) -> Dict[str, Any]:
    """SFT é…ç½®ç”Ÿæˆ"""
    return {
        "stage": "sft",
        "model_name_or_path": model_name,
        "dataset": dataset,
        "template": template,  # æ¨¡åž‹å¯¹è¯æ¨¡æ¿
        "finetuning_type": "lora",
        ...
    }
```

#### 3.2.4 è®­ç»ƒæµç¨‹

```bash
# 1. ä»Ž CPT æ¨¡åž‹ç»§ç»­è®­ç»ƒ
llamafactory-cli train configs/llmops/sft_config.yaml \
    --model_name_or_path checkpoints/cpt/domain-cpt

# 2. æˆ–ä»ŽåŽŸå§‹æ¨¡åž‹ç›´æŽ¥ SFT
llamafactory-cli train configs/llmops/sft_config.yaml \
    --model_name_or_path Qwen/Qwen2-1.5B

# 3. è®­ç»ƒè¾“å‡º
checkpoints/sft/sft-lora/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ trainer_log.jsonl
```

### 3.3 DPO - ç›´æŽ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization)

#### 3.3.1 ç›®æ ‡ä¸Žç”¨é€”

**è®­ç»ƒç›®æ ‡**ï¼š
- æ ¹æ®äººç±»åå¥½ä¼˜åŒ–æ¨¡åž‹è¾“å‡º
- æå‡ç”Ÿæˆè´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦
- å®žçŽ°åœ¨çº¿é—­çŽ¯è¿­ä»£

**é€‚ç”¨åœºæ™¯**ï¼š
- æ¨¡åž‹ä¸Šçº¿åŽæŒç»­ä¼˜åŒ–
- ä¿®æ­£æ¨¡åž‹åå·®
- é€‚é…ç”¨æˆ·åå¥½

**äº§å‡ºæ¨¡åž‹**ï¼š
- **Chat Model v1.1, v1.2...**ï¼šå¢žé‡ä¼˜åŒ–ç‰ˆæœ¬

#### 3.3.2 æ•°æ®è¦æ±‚

```python
# DPO æ•°æ®æ ¼å¼ï¼šåå¥½å¯¹æ¯”
{
    "conversations": [
        {
            "role": "user",
            "content": "Python å’Œ Java å“ªä¸ªæ›´å¥½ï¼Ÿ"
        }
    ],
    "chosen": {
        "role": "assistant",
        "content": "Python å’Œ Java å„æœ‰ä¼˜åŠ¿ï¼Œå–å†³äºŽå…·ä½“åœºæ™¯..."
    },
    "rejected": {
        "role": "assistant",
        "content": "Python æ›´å¥½ã€‚"
    }
}
```

**æ•°æ®ç‰¹ç‚¹**ï¼š
- å°è§„æ¨¡ï¼š100-1000 æ ·æœ¬å³å¯ç”Ÿæ•ˆ
- çœŸå®žæ€§ï¼šæ¥è‡ªçœŸå®žç”¨æˆ·åé¦ˆ
- å¯¹æ¯”æ€§ï¼šå¿…é¡»æœ‰ chosen/rejected å¯¹

#### 3.3.3 æ ¸å¿ƒé…ç½®

```python
@staticmethod
def create_dpo_config(
    model_name: str = "Qwen/Qwen2-1.5B",
    adapter_path: str = "checkpoints/sft/sft-lora",
    dataset: str = "prefs",
    output_dir: str = "checkpoints/dpo/dpo-lora",
    num_train_epochs: int = 1,
    learning_rate: float = 5e-6,
    pref_beta: float = 0.1
) -> Dict[str, Any]:
    """DPO é…ç½®ç”Ÿæˆ"""
    return {
        "stage": "dpo",
        "model_name_or_path": model_name,
        "adapter_name_or_path": adapter_path,  # åŠ è½½ SFT æƒé‡
        "dataset": dataset,
        "pref_beta": pref_beta,  # KL æ•£åº¦æƒé‡
        "pref_loss": "sigmoid",  # æŸå¤±å‡½æ•°
        ...
    }
```

#### 3.3.4 è®­ç»ƒæµç¨‹

```bash
# 1. DPO å¿…é¡»ä»Ž SFT æ¨¡åž‹ç»§ç»­
llamafactory-cli train configs/llmops/dpo_config.yaml

# 2. å¿«é€Ÿè¿­ä»£ï¼ˆé€šå¸¸ 30 åˆ†é’Ÿ - 2 å°æ—¶ï¼‰
# 3. è®­ç»ƒè¾“å‡º
checkpoints/dpo/dpo-lora/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors
â””â”€â”€ trainer_log.jsonl
```

### 3.4 è®­ç»ƒé˜¶æ®µå¯¹æ¯”

| é˜¶æ®µ | æ•°æ®é‡ | è®­ç»ƒæ—¶é—´ | æˆæœ¬ | é¢‘çŽ‡ | äº§å‡º |
|------|--------|---------|------|------|------|
| **CPT** | 10M-100M tokens | 2-8 å°æ—¶ | é«˜ | æ•°æœˆä¸€æ¬¡ | Completion Model |
| **SFT** | 10K-100K æ ·æœ¬ | 1-4 å°æ—¶ | ä¸­ | æ•°æœˆä¸€æ¬¡ | Chat Model v1.0 |
| **DPO** | 100-1K æ ·æœ¬ | 0.5-2 å°æ—¶ | ä½Ž | æ¯å‘¨/æœˆ | Chat Model v1.x |

---

## 4. æ•°æ®ç”Ÿæˆä¸Žç®¡ç†

### 4.1 é¢†åŸŸè¯­æ–™å¤„ç†

#### 4.1.1 DomainCorpusProcessor

```python
# æ–‡ä»¶: src/search_engine/training_tab/domain_corpus_processor.py
class DomainCorpusProcessor:
    """é¢†åŸŸè¯­æ–™å¤„ç†å™¨ - ç”¨äºŽ CPT"""
    
    def process_documents(
        self, 
        documents: List[str], 
        chunk_size: int = 512
    ) -> List[Dict[str, str]]:
        """
        å¤„ç†æ–‡æ¡£ä¸º CPT è®­ç»ƒæ ¼å¼
        
        Args:
            documents: åŽŸå§‹æ–‡æ¡£åˆ—è¡¨
            chunk_size: åˆ†å—å¤§å°ï¼ˆtokensï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„è®­ç»ƒæ•°æ®
        """
        corpus = []
        for doc in documents:
            # åˆ†å—
            chunks = self._chunk_text(doc, chunk_size)
            for chunk in chunks:
                corpus.append({"text": chunk})
        
        return corpus
```

#### 4.1.2 æ•°æ®æ ¼å¼

```jsonl
# data/llmops/cpt/domain_corpus_20231115.jsonl
{"text": "ç¬¬ä¸€æ®µé¢†åŸŸæ–‡æœ¬..."}
{"text": "ç¬¬äºŒæ®µé¢†åŸŸæ–‡æœ¬..."}
{"text": "ç¬¬ä¸‰æ®µé¢†åŸŸæ–‡æœ¬..."}
```

#### 4.1.3 æ•°æ®é›†æ³¨å†Œ

```json
// data/llmops/dataset_info.json
{
    "domain_corpus_20231115": {
        "file_name": "cpt/domain_corpus_20231115.jsonl",
        "columns": {
            "prompt": "text"
        }
    }
}
```

### 4.2 Self-Instruct æ•°æ®ç”Ÿæˆ

#### 4.2.1 SelfInstructGenerator

```python
# æ–‡ä»¶: src/search_engine/training_tab/self_instruct_generator.py
class SelfInstructGenerator:
    """Self-Instruct æ•°æ®ç”Ÿæˆå™¨ - ç”¨äºŽ SFT"""
    
    def generate_instructions(
        self, 
        num_instructions: int = 100,
        use_mock: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆæŒ‡ä»¤æ•°æ®
        
        Args:
            num_instructions: ç”Ÿæˆæ•°é‡
            use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰
        
        Returns:
            ShareGPT æ ¼å¼çš„æŒ‡ä»¤æ•°æ®
        """
        if use_mock:
            return self._generate_mock_data(num_instructions)
        else:
            return self._generate_with_llm(num_instructions)
```

#### 4.2.2 æ•°æ®æ ¼å¼

```json
// data/llmops/sft/sft_data_20231115.json
[
    {
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "åˆ¤æ–­è¿™æ¡è¯„è®ºæ˜¯å¥½è¯„è¿˜æ˜¯å·®è¯„\n\nç‰©æµé€Ÿåº¦å¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½ï¼Œè´¨é‡ä¸é”™"
            },
            {
                "role": "assistant",
                "content": "å¥½è¯„"
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "å°†ä¸‹é¢çš„æ–‡æœ¬æ€»ç»“ä¸ºä¸€å¥è¯\n\näººå·¥æ™ºèƒ½æ˜¯..."
            },
            {
                "role": "assistant",
                "content": "äººå·¥æ™ºèƒ½æ˜¯åˆ›å»ºèƒ½æ‰§è¡Œç±»äººæ™ºèƒ½ä»»åŠ¡çš„è®¡ç®—æœºç³»ç»Ÿã€‚"
            }
        ]
    }
]
```

#### 4.2.3 ä»»åŠ¡ç±»åž‹æ¨¡æ¿

```python
TASK_TYPES = {
    "classification": "åˆ†ç±»ä»»åŠ¡",
    "generation": "ç”Ÿæˆä»»åŠ¡",
    "summarization": "æ€»ç»“ä»»åŠ¡",
    "qa": "é—®ç­”ä»»åŠ¡",
    "rewrite": "æ”¹å†™ä»»åŠ¡",
    "reasoning": "æŽ¨ç†ä»»åŠ¡"
}
```

### 4.3 åå¥½æ•°æ®æ”¶é›†

#### 4.3.1 PreferenceCollector

```python
# æ–‡ä»¶: src/search_engine/training_tab/preference_collector.py
class PreferenceCollector:
    """åå¥½æ•°æ®æ”¶é›†å™¨ - ç”¨äºŽ DPO"""
    
    def collect_preference(
        self,
        question: str,
        response_a: str,
        response_b: str,
        choice: str,  # "A" or "B"
        metadata: Dict[str, Any] = None
    ):
        """
        æ”¶é›†ç”¨æˆ·åå¥½
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            response_a: å›žç­” A
            response_b: å›žç­” B
            choice: ç”¨æˆ·é€‰æ‹©ï¼ˆ"A" æˆ– "B"ï¼‰
            metadata: é™„åŠ ä¿¡æ¯ï¼ˆæ¨¡åž‹ç‰ˆæœ¬ç­‰ï¼‰
        """
        chosen = response_a if choice == "A" else response_b
        rejected = response_b if choice == "A" else response_a
        
        preference = {
            "conversations": [
                {"role": "user", "content": question}
            ],
            "chosen": {"role": "assistant", "content": chosen},
            "rejected": {"role": "assistant", "content": rejected},
            "metadata": metadata
        }
        
        self._save_preference(preference)
```

#### 4.3.2 æ•°æ®æ ¼å¼

```jsonl
# data/llmops/dpo/prefs.jsonl
{"conversations": [{"role": "user", "content": "Pythonå’ŒJavaå“ªä¸ªæ›´å¥½ï¼Ÿ"}], "chosen": {"role": "assistant", "content": "Python å’Œ Java å„æœ‰ä¼˜åŠ¿..."}, "rejected": {"role": "assistant", "content": "Python æ›´å¥½ã€‚"}}
{"conversations": [{"role": "user", "content": "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ"}], "chosen": {"role": "assistant", "content": "ç³»ç»Ÿå­¦ä¹ ï¼Œä»ŽåŸºç¡€åˆ°å®žè·µ..."}, "rejected": {"role": "assistant", "content": "çœ‹ä¹¦å°±è¡Œã€‚"}}
```

#### 4.3.3 æ•°æ®é›†æ³¨å†Œ

```json
// data/llmops/dataset_info.json
{
    "prefs": {
        "file_name": "dpo/prefs.jsonl",
        "formatting": "sharegpt",
        "ranking": true,
        "columns": {
            "messages": "conversations",
            "chosen": "chosen",
            "rejected": "rejected"
        }
    }
}
```

### 4.4 æ•°æ®ç®¡ç†æœ€ä½³å®žè·µ

#### 4.4.1 æ•°æ®è´¨é‡æŽ§åˆ¶

```python
# æ•°æ®éªŒè¯
def validate_data(data: List[Dict]) -> bool:
    """éªŒè¯æ•°æ®æ ¼å¼"""
    for item in data:
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if "conversations" not in item:
            return False
        
        # æ£€æŸ¥å¯¹è¯æ ¼å¼
        for conv in item["conversations"]:
            if "from" not in conv or "value" not in conv:
                return False
    
    return True
```

#### 4.4.2 æ•°æ®åŽ»é‡

```python
def deduplicate_data(data: List[Dict]) -> List[Dict]:
    """æ•°æ®åŽ»é‡"""
    seen = set()
    unique_data = []
    
    for item in data:
        # ä½¿ç”¨é—®é¢˜çš„å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
        question = item["conversations"][0]["value"]
        hash_key = hashlib.md5(question.encode()).hexdigest()
        
        if hash_key not in seen:
            seen.add(hash_key)
            unique_data.append(item)
    
    return unique_data
```

#### 4.4.3 æ•°æ®ç‰ˆæœ¬ç®¡ç†

```bash
# ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºç‰ˆæœ¬æ ‡è¯†
data/llmops/
â”œâ”€â”€ cpt/
â”‚   â”œâ”€â”€ domain_corpus_20231115_120000.jsonl
â”‚   â””â”€â”€ domain_corpus_20231116_093000.jsonl
â”œâ”€â”€ sft/
â”‚   â”œâ”€â”€ sft_data_20231115.json
â”‚   â””â”€â”€ sft_data_20231116.json
â””â”€â”€ dpo/
    â””â”€â”€ prefs.jsonl  # æŒç»­è¿½åŠ 
```

---

## 5. æ ¸å¿ƒç»„ä»¶å®žçŽ°

### 5.1 LLaMAFactoryTrainer - è®­ç»ƒæœåŠ¡

```python
# æ–‡ä»¶: src/search_engine/training_tab/llamafactory_trainer.py
class LLaMAFactoryTrainer:
    """LLaMA-Factory è®­ç»ƒæœåŠ¡ï¼ˆä½¿ç”¨ subprocess å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹ï¼‰"""
    
    def __init__(self):
        self.current_process: Optional[subprocess.Popen] = None
        self.training_status = {
            'running': False,
            'stage': None,
            'output_dir': None
        }
    
    def start_training(self, config: Dict[str, Any]) -> bool:
        """
        å¯åŠ¨è®­ç»ƒ
        
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        
        Returns:
            æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        try:
            # 1. æž„å»ºè®­ç»ƒå‚æ•°
            args = self._build_train_args(config)
            
            # 2. ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶
            config_file = self._create_temp_config(args)
            
            # 3. æž„å»ºè®­ç»ƒå‘½ä»¤
            cmd = [
                "llamafactory-cli",
                "train",
                config_file
            ]
            
            # 4. å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            self.current_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            # 5. æ›´æ–°çŠ¶æ€
            self.training_status['running'] = True
            self.training_status['stage'] = config.get('stage')
            self.training_status['output_dir'] = config.get('output_dir')
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨è®­ç»ƒå¤±è´¥: {e}")
            return False
```

### 5.2 LLMOpsEngine - å¼•æ“Žåè°ƒ

```python
# æ–‡ä»¶: src/search_engine/training_tab/llmops_engine.py
class LLMOpsEngine:
    """LLMOps å¼•æ“Žï¼ˆå‚è€ƒ LLaMA-Factory çš„ Engine è®¾è®¡ï¼‰"""
    
    def __init__(self, demo_mode: bool = False):
        self.demo_mode = demo_mode
        self.manager = LLMOpsManager()  # ç»„ä»¶ç®¡ç†
        self.runner = LLMOpsRunner(self.manager)  # è®­ç»ƒè¿è¡Œ
    
    def resume(self):
        """æ¢å¤ç»„ä»¶åˆå§‹çŠ¶æ€"""
        user_config = load_config() if not self.demo_mode else {}
        
        init_dict = {
            "train.output_dir": {"value": f"train_{get_time()}"}
        }
        
        yield self._update_component(init_dict)
        
        # å¦‚æžœè®­ç»ƒæ­£åœ¨è¿è¡Œï¼Œæ¢å¤è®­ç»ƒçŠ¶æ€
        if self.runner.running and self.runner.running_data:
            output_dict = {}
            for elem, value in self.runner.running_data.items():
                output_dict[elem] = gr.update(value=value)
            yield output_dict
```

### 5.3 InferenceModel - æŽ¨ç†æœåŠ¡

```python
# æ–‡ä»¶: src/search_engine/training_tab/inference_model.py
class InferenceModel:
    """æŽ¨ç†æœåŠ¡ - ç”¨äºŽ A/B æµ‹è¯•"""
    
    def __init__(self):
        self.models = {}  # æ¨¡åž‹ç¼“å­˜
    
    def load_model(
        self,
        model_name: str,
        base_model: str,
        adapter_path: str = None
    ):
        """
        åŠ è½½æ¨¡åž‹
        
        Args:
            model_name: æ¨¡åž‹æ ‡è¯†
            base_model: åŸºç¡€æ¨¡åž‹è·¯å¾„
            adapter_path: LoRA é€‚é…å™¨è·¯å¾„
        """
        if model_name in self.models:
            return
        
        # å®žé™…ç”Ÿäº§ä¸­åº”è¯¥ä½¿ç”¨ vLLM æˆ– TGI
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ Transformers å®žçŽ°
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        model = AutoModelForCausalLM.from_pretrained(base_model)
        
        if adapter_path:
            # åŠ è½½ LoRA æƒé‡
            from peft import PeftModel
            model = PeftModel.from_pretrained(model, adapter_path)
        
        self.models[model_name] = {
            'model': model,
            'tokenizer': tokenizer
        }
    
    def generate(
        self,
        model_name: str,
        prompt: str,
        max_length: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        ç”Ÿæˆå›žç­”
        
        Args:
            model_name: æ¨¡åž‹æ ‡è¯†
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if model_name not in self.models:
            raise ValueError(f"æ¨¡åž‹ {model_name} æœªåŠ è½½")
        
        model_dict = self.models[model_name]
        tokenizer = model_dict['tokenizer']
        model = model_dict['model']
        
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
```

---

## 6. åœ¨çº¿é—­çŽ¯ä¼˜åŒ–

### 6.1 é—­çŽ¯æµç¨‹å›¾

```mermaid
graph LR
    A[Chat Model v1.0] --> B[ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²]
    B --> C[ç”¨æˆ·äº¤äº’]
    C --> D[A/B æµ‹è¯•]
    D --> E[æ”¶é›†åå¥½]
    E --> F{ç´¯ç§¯è¶³å¤Ÿæ•°æ®?}
    F -->|å¦| C
    F -->|æ˜¯| G[DPO è®­ç»ƒ]
    G --> H[Chat Model v1.1]
    H --> I[è¯„ä¼°éªŒè¯]
    I --> J{æ•ˆæžœæå‡?}
    J -->|æ˜¯| B
    J -->|å¦| K[è°ƒæ•´å‚æ•°]
    K --> G
```

### 6.2 A/B æµ‹è¯•å®žçŽ°

#### 6.2.1 æµ‹è¯•ç•Œé¢

```python
def create_ab_test_interface():
    """åˆ›å»º A/B æµ‹è¯•ç•Œé¢"""
    with gr.Tab("A/B æµ‹è¯•ä¸Žåé¦ˆé—­çŽ¯"):
        with gr.Row():
            with gr.Column():
                question_input = gr.Textbox(
                    label="ç”¨æˆ·é—®é¢˜",
                    placeholder="è¾“å…¥è¦æµ‹è¯•çš„é—®é¢˜..."
                )
                generate_btn = gr.Button("ðŸŽ² ç”Ÿæˆå¯¹æ¯”å›žç­”")
            
            with gr.Column():
                model_a_config = gr.Dropdown(
                    label="æ¨¡åž‹ A",
                    choices=get_trained_models("sft")
                )
                model_b_config = gr.Dropdown(
                    label="æ¨¡åž‹ B",
                    choices=get_trained_models("dpo")
                )
        
        with gr.Row():
            with gr.Column():
                response_a = gr.Textbox(
                    label="å›žç­” A",
                    lines=8,
                    interactive=False
                )
                vote_a_btn = gr.Button("ðŸ‘ A æ›´å¥½")
            
            with gr.Column():
                response_b = gr.Textbox(
                    label="å›žç­” B",
                    lines=8,
                    interactive=False
                )
                vote_b_btn = gr.Button("ðŸ‘ B æ›´å¥½")
        
        feedback_status = gr.Textbox(label="åé¦ˆçŠ¶æ€")
        
        # ç»‘å®šäº‹ä»¶
        generate_btn.click(
            fn=generate_ab_responses,
            inputs=[question_input, model_a_config, model_b_config],
            outputs=[response_a, response_b]
        )
        
        vote_a_btn.click(
            fn=lambda q, a, b: record_preference(q, a, b, "A"),
            inputs=[question_input, response_a, response_b],
            outputs=[feedback_status]
        )
        
        vote_b_btn.click(
            fn=lambda q, a, b: record_preference(q, a, b, "B"),
            inputs=[question_input, response_a, response_b],
            outputs=[feedback_status]
        )
```

#### 6.2.2 åå¥½è®°å½•

```python
def record_preference(
    question: str,
    response_a: str,
    response_b: str,
    choice: str
) -> str:
    """è®°å½•ç”¨æˆ·åå¥½"""
    try:
        collector = PreferenceCollector()
        collector.collect_preference(
            question=question,
            response_a=response_a,
            response_b=response_b,
            choice=choice,
            metadata={
                "timestamp": datetime.now().isoformat(),
                "model_a": "v1.0",
                "model_b": "v1.1"
            }
        )
        
        return f"âœ… åå¥½å·²è®°å½•ï¼é€‰æ‹©äº†å›žç­” {choice}"
        
    except Exception as e:
        return f"âŒ è®°å½•å¤±è´¥: {e}"
```

### 6.3 è¿­ä»£ç­–ç•¥

#### 6.3.1 æ•°æ®æ”¶é›†è®¡åˆ’

```python
# æ¯å‘¨æ”¶é›†ç›®æ ‡
WEEKLY_COLLECTION_TARGET = {
    "min_samples": 100,      # æœ€å°‘æ ·æœ¬æ•°
    "target_samples": 300,   # ç›®æ ‡æ ·æœ¬æ•°
    "optimal_samples": 500   # æœ€ä½³æ ·æœ¬æ•°
}

# æ¯æœˆè¿­ä»£è®¡åˆ’
MONTHLY_ITERATION_PLAN = {
    "week_1": "æ”¶é›†åå¥½æ•°æ®",
    "week_2": "æ”¶é›†åå¥½æ•°æ®",
    "week_3": "DPO è®­ç»ƒ",
    "week_4": "éƒ¨ç½²å’ŒéªŒè¯"
}
```

#### 6.3.2 è®­ç»ƒè§¦å‘æ¡ä»¶

```python
def should_trigger_dpo_training() -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘ DPO è®­ç»ƒ"""
    prefs_file = "data/llmops/dpo/prefs.jsonl"
    
    if not os.path.exists(prefs_file):
        return False
    
    # ç»Ÿè®¡åå¥½æ•°æ®é‡
    with open(prefs_file, 'r') as f:
        count = sum(1 for _ in f)
    
    # åˆ¤æ–­æ¡ä»¶
    min_samples = 100
    time_since_last_training = get_time_since_last_training()
    days_threshold = 7  # æ¯å‘¨è‡³å°‘ä¸€æ¬¡
    
    return (
        count >= min_samples and
        time_since_last_training >= days_threshold
    )
```

#### 6.3.3 æ•ˆæžœè¯„ä¼°

```python
def evaluate_model_improvement(
    old_model: str,
    new_model: str,
    test_set: List[Dict]
) -> Dict[str, float]:
    """è¯„ä¼°æ¨¡åž‹æ”¹è¿›æ•ˆæžœ"""
    results = {
        "win_rate": 0.0,      # æ–°æ¨¡åž‹èƒœçŽ‡
        "avg_score": 0.0,     # å¹³å‡è¯„åˆ†
        "consistency": 0.0    # ä¸€è‡´æ€§
    }
    
    inference = InferenceModel()
    inference.load_model("old", old_model)
    inference.load_model("new", new_model)
    
    wins = 0
    total = len(test_set)
    
    for item in test_set:
        question = item["question"]
        
        old_response = inference.generate("old", question)
        new_response = inference.generate("new", question)
        
        # äººå·¥è¯„åˆ†æˆ–è‡ªåŠ¨è¯„ä¼°
        score = compare_responses(old_response, new_response)
        if score > 0:
            wins += 1
    
    results["win_rate"] = wins / total
    return results
```

### 6.4 ç”Ÿäº§éƒ¨ç½²æµç¨‹

```bash
# 1. è®­ç»ƒå®ŒæˆåŽçš„æ¨¡åž‹è·¯å¾„
MODEL_PATH="checkpoints/dpo/chat-v1.1"

# 2. è¯„ä¼°éªŒè¯
python tools/evaluate_model.py --model $MODEL_PATH

# 3. ç°åº¦å‘å¸ƒï¼ˆ5% æµé‡ï¼‰
python tools/deploy_model.py --model $MODEL_PATH --traffic 0.05

# 4. ç›‘æŽ§æŒ‡æ ‡
python tools/monitor_metrics.py --model $MODEL_PATH

# 5. å…¨é‡å‘å¸ƒï¼ˆ100% æµé‡ï¼‰
python tools/deploy_model.py --model $MODEL_PATH --traffic 1.0
```

---

## 7. é…ç½®ä¸Žéƒ¨ç½²

### 7.1 çŽ¯å¢ƒé…ç½®

#### 7.1.1 ä¾èµ–å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install torch transformers datasets
pip install accelerate peft bitsandbytes

# LLaMA-Factory
pip install llamafactory

# Web ç•Œé¢
pip install gradio

# å¯é€‰ï¼šæŽ¨ç†åŠ é€Ÿ
pip install vllm  # ç”Ÿäº§çŽ¯å¢ƒæŽ¨è
```

#### 7.1.2 ç›®å½•åˆå§‹åŒ–

```bash
# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p data/llmops/{cpt,sft,dpo}
mkdir -p checkpoints/{cpt,sft,dpo}
mkdir -p configs/llmops

# åˆå§‹åŒ–æ•°æ®é›†é…ç½®
cat > data/llmops/dataset_info.json << 'EOF'
{}
EOF
```

### 7.2 è®­ç»ƒé…ç½®æ¨¡æ¿

#### 7.2.1 CPT é…ç½®ç¤ºä¾‹

```yaml
# configs/llmops/cpt_config_template.yaml
model_name_or_path: Qwen/Qwen2-1.5B
stage: pt
do_train: true
dataset: domain_corpus
dataset_dir: data/llmops
template: default
finetuning_type: lora
lora_target: all
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
output_dir: checkpoints/cpt/domain-cpt
overwrite_output_dir: true
num_train_epochs: 1
learning_rate: 1.0e-5
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
lr_scheduler_type: cosine
warmup_steps: 100
logging_steps: 10
save_steps: 500
fp16: true
cutoff_len: 2048
plot_loss: true
```

#### 7.2.2 SFT é…ç½®ç¤ºä¾‹

```yaml
# configs/llmops/sft_config_template.yaml
model_name_or_path: Qwen/Qwen2-1.5B
stage: sft
do_train: true
dataset: sft_data
dataset_dir: data/llmops
template: qwen
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
output_dir: checkpoints/sft/sft-lora
num_train_epochs: 3
learning_rate: 5.0e-5
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
fp16: true
cutoff_len: 1024
plot_loss: true
```

#### 7.2.3 DPO é…ç½®ç¤ºä¾‹

```yaml
# configs/llmops/dpo_config_template.yaml
model_name_or_path: Qwen/Qwen2-1.5B
adapter_name_or_path: checkpoints/sft/sft-lora
stage: dpo
do_train: true
dataset: prefs
dataset_dir: data/llmops
template: qwen
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
output_dir: checkpoints/dpo/dpo-lora
num_train_epochs: 1
learning_rate: 5.0e-6
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
pref_beta: 0.1
pref_loss: sigmoid
fp16: true
cutoff_len: 1024
```

### 7.3 æ¨¡åž‹éƒ¨ç½²

#### 7.3.1 æœ¬åœ°éƒ¨ç½²

```bash
# ä½¿ç”¨ LLaMA-Factory CLI å¿«é€Ÿéƒ¨ç½²
llamafactory-cli api \
    --model_name_or_path Qwen/Qwen2-1.5B \
    --adapter_name_or_path checkpoints/dpo/dpo-lora \
    --template qwen \
    --port 8000
```

#### 7.3.2 ç”Ÿäº§éƒ¨ç½²ï¼ˆvLLMï¼‰

```bash
# å…ˆåˆå¹¶ LoRA æƒé‡
llamafactory-cli export \
    --model_name_or_path Qwen/Qwen2-1.5B \
    --adapter_name_or_path checkpoints/dpo/dpo-lora \
    --template qwen \
    --export_dir models/merged/chat-v1.1

# ä½¿ç”¨ vLLM éƒ¨ç½²
python -m vllm.entrypoints.openai.api_server \
    --model models/merged/chat-v1.1 \
    --port 8000 \
    --tensor-parallel-size 2
```

---

## 8. ä½¿ç”¨æŒ‡å—

### 8.1 å¿«é€Ÿå¼€å§‹

#### 8.1.1 å¯åŠ¨ç³»ç»Ÿ

```bash
cd /Users/tyler/courseware-1/projects/Testbed
python start_system.py
```

è®¿é—® Web ç•Œé¢ï¼Œè¿›å…¥ **è®­ç»ƒ & å®žéªŒ** â†’ **LLMOps é—­çŽ¯** æ ‡ç­¾é¡µã€‚

### 8.2 ç¦»çº¿è®­ç»ƒæµç¨‹

#### æ­¥éª¤ 1: æ•°æ®ç”Ÿæˆ

1. **é¢†åŸŸè¯­æ–™**ï¼š
   - ç‚¹å‡»"åŠ è½½é¢„ç½®æ–‡æ¡£"
   - ç‚¹å‡»"å¤„ç†è¯­æ–™"
   - ç‚¹å‡»"ä¿å­˜è¯­æ–™"

2. **æŒ‡ä»¤æ•°æ®**ï¼š
   - è®¾ç½®ç”Ÿæˆæ•°é‡ï¼ˆ50-100ï¼‰
   - ç‚¹å‡»"ç”ŸæˆæŒ‡ä»¤æ•°æ®"
   - ç‚¹å‡»"ä¿å­˜æŒ‡ä»¤æ•°æ®"

#### æ­¥éª¤ 2: é…ç½®è®­ç»ƒ

1. **CPT é…ç½®**ï¼š
   - è¿›å…¥"è®­ç»ƒé…ç½®" â†’ "CPT"
   - è®¾ç½®åŸºç¡€æ¨¡åž‹ï¼š`Qwen/Qwen2-1.5B`
   - ç‚¹å‡»"ç”Ÿæˆ CPT é…ç½®"

2. **SFT é…ç½®**ï¼š
   - è¿›å…¥"è®­ç»ƒé…ç½®" â†’ "SFT"
   - è®¾ç½®æ¨¡åž‹è·¯å¾„ï¼š`checkpoints/cpt/domain-cpt`
   - ç‚¹å‡»"ç”Ÿæˆ SFT é…ç½®"

#### æ­¥éª¤ 3: æ‰§è¡Œè®­ç»ƒ

```bash
# CPT è®­ç»ƒ
llamafactory-cli train configs/llmops/cpt_config_*.yaml

# SFT è®­ç»ƒ
llamafactory-cli train configs/llmops/sft_config_*.yaml
```

### 8.3 åœ¨çº¿ä¼˜åŒ–æµç¨‹ï¼ˆæŽ¨èï¼‰

#### æ­¥éª¤ 1: æ”¶é›†åå¥½

1. è¿›å…¥ "A/B æµ‹è¯•ä¸Žåé¦ˆé—­çŽ¯"
2. è¾“å…¥é—®é¢˜ï¼Œç”Ÿæˆå¯¹æ¯”å›žç­”
3. é€‰æ‹©æ›´å¥½çš„å›žç­”
4. ç³»ç»Ÿè‡ªåŠ¨è®°å½•

**ç›®æ ‡**ï¼šæ¯å‘¨æ”¶é›† 100-200 æ¡çœŸå®žåå¥½

#### æ­¥éª¤ 2: DPO è®­ç»ƒ

1. è¿›å…¥"è®­ç»ƒé…ç½®" â†’ "DPO"
2. è®¾ç½®æ¨¡åž‹è·¯å¾„ï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰
3. ç‚¹å‡»"ç”Ÿæˆ DPO é…ç½®"
4. æ‰§è¡Œè®­ç»ƒï¼š

```bash
llamafactory-cli train configs/llmops/dpo_config_*.yaml
```

#### æ­¥éª¤ 3: éƒ¨ç½²æ–°ç‰ˆæœ¬

1. è®­ç»ƒå®ŒæˆåŽå¾—åˆ°æ–°çš„ LoRA
2. æ›´æ–°æŽ¨ç†æœåŠ¡é…ç½®
3. ç»§ç»­æ”¶é›†åé¦ˆ

---

## 9. æœ€ä½³å®žè·µ

### 9.1 æ•°æ®æœ€ä½³å®žè·µ

#### 9.1.1 CPT æ•°æ®

- âœ… é«˜è´¨é‡ï¼šç¡®ä¿é¢†åŸŸç›¸å…³æ€§å¼º
- âœ… å¤§è§„æ¨¡ï¼šè‡³å°‘ 10M tokens
- âœ… æ¸…æ´—ï¼šåŽ»é™¤å™ªå£°å’Œé‡å¤
- âœ… åˆ†å—ï¼šåˆç†çš„ chunk_sizeï¼ˆ512-2048ï¼‰

#### 9.1.2 SFT æ•°æ®

- âœ… å¤šæ ·æ€§ï¼šè¦†ç›–å¤šç§ä»»åŠ¡ç±»åž‹
- âœ… é«˜è´¨é‡ï¼šäººå·¥æ ‡æ³¨æˆ– GPT ç”Ÿæˆ
- âœ… æ ¼å¼è§„èŒƒï¼šä¸¥æ ¼éµå¾ª ShareGPT æ ¼å¼
- âœ… è§„æ¨¡é€‚ä¸­ï¼š10K-100K æ ·æœ¬

#### 9.1.3 DPO æ•°æ®

- âœ… çœŸå®žæ€§ï¼šæ¥è‡ªçœŸå®žç”¨æˆ·åé¦ˆ
- âœ… å¯¹æ¯”æ€§ï¼šæ˜Žç¡®çš„å¥½åå¯¹æ¯”
- âœ… æŒç»­æ”¶é›†ï¼šæ¯å‘¨ 100-200 æ¡
- âœ… è´¨é‡æŽ§åˆ¶ï¼šå®šæœŸå®¡æŸ¥åå¥½æ•°æ®

### 9.2 è®­ç»ƒæœ€ä½³å®žè·µ

#### 9.2.1 è¶…å‚æ•°è°ƒä¼˜

```python
# CPT å‚æ•°å»ºè®®
CPT_PARAMS = {
    "learning_rate": 1e-5,  # è¾ƒå°çš„å­¦ä¹ çŽ‡
    "num_epochs": 1,        # 1-3 ä¸ª epoch
    "lora_rank": 16,        # è¾ƒå¤§çš„ rank
    "batch_size": 2-4,      # æ ¹æ®æ˜¾å­˜è°ƒæ•´
}

# SFT å‚æ•°å»ºè®®
SFT_PARAMS = {
    "learning_rate": 5e-5,  # ä¸­ç­‰å­¦ä¹ çŽ‡
    "num_epochs": 3,        # 3-5 ä¸ª epoch
    "lora_rank": 8,         # ä¸­ç­‰ rank
    "batch_size": 4-8,      # æ ¹æ®æ˜¾å­˜è°ƒæ•´
}

# DPO å‚æ•°å»ºè®®
DPO_PARAMS = {
    "learning_rate": 5e-6,  # å¾ˆå°çš„å­¦ä¹ çŽ‡
    "num_epochs": 1,        # 1 ä¸ª epoch
    "lora_rank": 8,         # ä¸Ž SFT ä¿æŒä¸€è‡´
    "pref_beta": 0.1,       # KL æ•£åº¦æƒé‡
}
```

#### 9.2.2 è®­ç»ƒç›‘æŽ§

```python
# ç›‘æŽ§å…³é”®æŒ‡æ ‡
MONITOR_METRICS = {
    "loss": "è®­ç»ƒæŸå¤±ï¼ˆåº”æŒç»­ä¸‹é™ï¼‰",
    "learning_rate": "å­¦ä¹ çŽ‡å˜åŒ–",
    "gradient_norm": "æ¢¯åº¦èŒƒæ•°ï¼ˆé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰",
    "samples_per_second": "è®­ç»ƒé€Ÿåº¦"
}

# æ—©åœæ¡ä»¶
EARLY_STOPPING = {
    "patience": 3,          # 3 ä¸ª epoch æ— æ”¹è¿›å³åœæ­¢
    "min_delta": 0.001,     # æœ€å°æ”¹è¿›é˜ˆå€¼
    "monitor": "eval_loss"  # ç›‘æŽ§éªŒè¯é›†æŸå¤±
}
```

### 9.3 éƒ¨ç½²æœ€ä½³å®žè·µ

#### 9.3.1 ç°åº¦å‘å¸ƒ

```python
# ç°åº¦å‘å¸ƒç­–ç•¥
GRADUAL_ROLLOUT = {
    "stage_1": {"traffic": 0.05, "duration": "1 day"},   # 5% æµé‡
    "stage_2": {"traffic": 0.20, "duration": "2 days"},  # 20% æµé‡
    "stage_3": {"traffic": 0.50, "duration": "3 days"},  # 50% æµé‡
    "stage_4": {"traffic": 1.00, "duration": "stable"}   # 100% æµé‡
}
```

#### 9.3.2 å›žæ»šç­–ç•¥

```python
# è‡ªåŠ¨å›žæ»šæ¡ä»¶
ROLLBACK_CONDITIONS = {
    "error_rate": 0.05,      # é”™è¯¯çŽ‡è¶…è¿‡ 5%
    "latency_p99": 2000,     # P99 å»¶è¿Ÿè¶…è¿‡ 2s
    "user_feedback": -0.2    # ç”¨æˆ·åé¦ˆä¸‹é™ 20%
}
```

---

## 10. æ•…éšœæŽ’é™¤

### 10.1 è®­ç»ƒé—®é¢˜

#### 10.1.1 OOM (å†…å­˜ä¸è¶³)

**é—®é¢˜**ï¼š`RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# å‡å° batch size
per_device_train_batch_size: 1

# å¢žå¤§æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps: 16

# å‡å°åºåˆ—é•¿åº¦
cutoff_len: 512

# ä½¿ç”¨ gradient checkpointing
gradient_checkpointing: true

# ä½¿ç”¨ 8-bit é‡åŒ–
quantization_bit: 8
```

#### 10.1.2 è®­ç»ƒä¸æ”¶æ•›

**é—®é¢˜**ï¼šLoss ä¸ä¸‹é™æˆ–éœ‡è¡

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# é™ä½Žå­¦ä¹ çŽ‡
learning_rate: 1.0e-6

# å¢žåŠ  warmup
warmup_steps: 500

# ä½¿ç”¨æ¢¯åº¦è£å‰ª
max_grad_norm: 1.0

# æ£€æŸ¥æ•°æ®è´¨é‡
# - ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
# - åŽ»é™¤å¼‚å¸¸æ ·æœ¬
```

#### 10.1.3 è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**ï¼šè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ä½¿ç”¨å¤š GPU
# å¯åŠ¨ DeepSpeed
deepspeed --num_gpus=4 llamafactory-cli train config.yaml

# 2. ä½¿ç”¨æ··åˆç²¾åº¦
fp16: true  # æˆ– bf16: true

# 3. å¢žå¤§ batch size
per_device_train_batch_size: 8

# 4. ä¼˜åŒ–æ•°æ®åŠ è½½
dataloader_num_workers: 4
preprocessing_num_workers: 8
```

### 10.2 æ•°æ®é—®é¢˜

#### 10.2.1 æ•°æ®æ ¼å¼é”™è¯¯

**é—®é¢˜**ï¼š`KeyError: 'conversations'`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥æ•°æ®æ ¼å¼
def validate_sft_data(data_file):
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    for i, item in enumerate(data):
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        assert "conversations" in item, f"ç¬¬ {i} æ¡ç¼ºå°‘ conversations"
        
        # æ£€æŸ¥å¯¹è¯æ ¼å¼
        for conv in item["conversations"]:
            assert "from" in conv, f"ç¬¬ {i} æ¡å¯¹è¯ç¼ºå°‘ from"
            assert "value" in conv, f"ç¬¬ {i} æ¡å¯¹è¯ç¼ºå°‘ value"
            assert conv["from"] in ["human", "gpt"], f"from å­—æ®µå€¼é”™è¯¯"
```

#### 10.2.2 æ•°æ®é›†æœªæ³¨å†Œ

**é—®é¢˜**ï¼š`Dataset 'xxx' not found in dataset_info.json`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```json
// æ‰‹åŠ¨æ·»åŠ åˆ° data/llmops/dataset_info.json
{
    "your_dataset_name": {
        "file_name": "path/to/your_data.jsonl",
        "columns": {
            "prompt": "text"  // CPT
            // æˆ–
            "messages": "conversations"  // SFT/DPO
        },
        "formatting": "sharegpt",  // SFT/DPO å¿…éœ€
        "ranking": true  // ä»… DPO éœ€è¦
    }
}
```

### 10.3 æ¨¡åž‹é—®é¢˜

#### 10.3.1 æ¨¡åž‹åŠ è½½å¤±è´¥

**é—®é¢˜**ï¼š`OSError: Unable to load weights`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. æ£€æŸ¥æ¨¡åž‹è·¯å¾„
ls -la checkpoints/sft/sft-lora/
# åº”è¯¥åŒ…å«ï¼šadapter_config.json, adapter_model.safetensors

# 2. æ£€æŸ¥æ–‡ä»¶æƒé™
chmod -R 755 checkpoints/

# 3. é‡æ–°ä¸‹è½½åŸºç¡€æ¨¡åž‹
huggingface-cli download Qwen/Qwen2-1.5B
```

#### 10.3.2 æŽ¨ç†æ•ˆæžœå·®

**é—®é¢˜**ï¼šæ¨¡åž‹è¾“å‡ºè´¨é‡ä¸å¥½

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æ£€æŸ¥æŽ¨ç†å‚æ•°
INFERENCE_PARAMS = {
    "temperature": 0.7,      # é™ä½Žéšæœºæ€§
    "top_p": 0.9,            # æ ¸é‡‡æ ·
    "top_k": 50,             # Top-K é‡‡æ ·
    "repetition_penalty": 1.1,  # é‡å¤æƒ©ç½š
    "max_length": 512        # æœ€å¤§é•¿åº¦
}

# 2. æ£€æŸ¥ Prompt æ¨¡æ¿
# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å¯¹è¯æ¨¡æ¿ï¼ˆå¦‚ Qwen æ¨¡æ¿ï¼‰

# 3. å°è¯•ä¸åŒçš„ checkpoint
# æœ‰æ—¶æœ€åŽçš„ checkpoint ä¸æ˜¯æœ€å¥½çš„
```

---

## é™„å½•

### A. é…ç½®å‚è€ƒ

```python
# å®Œæ•´é…ç½®ç¤ºä¾‹
FULL_CONFIG = {
    # æ¨¡åž‹é…ç½®
    "model_name_or_path": "Qwen/Qwen2-1.5B",
    "adapter_name_or_path": "checkpoints/sft/sft-lora",
    
    # è®­ç»ƒé˜¶æ®µ
    "stage": "dpo",  # pt, sft, dpo, rm, ppo
    
    # æ•°æ®é…ç½®
    "dataset": "prefs",
    "dataset_dir": "data/llmops",
    "template": "qwen",
    
    # LoRA é…ç½®
    "finetuning_type": "lora",
    "lora_target": "all",
    "lora_rank": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    
    # è®­ç»ƒå‚æ•°
    "num_train_epochs": 1,
    "learning_rate": 5e-6,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 100,
    
    # ä¼˜åŒ–é…ç½®
    "fp16": True,
    "bf16": False,
    "gradient_checkpointing": True,
    "max_grad_norm": 1.0,
    
    # DPO ç‰¹å®š
    "pref_beta": 0.1,
    "pref_loss": "sigmoid",
    "pref_ftx": 0.0,
    
    # è¾“å‡ºé…ç½®
    "output_dir": "checkpoints/dpo/dpo-lora",
    "logging_steps": 10,
    "save_steps": 100,
    "plot_loss": True
}
```

### B. API å‚è€ƒ

```python
# è®­ç»ƒ API
from llamafactory_trainer import get_trainer

trainer = get_trainer()
trainer.start_training(config)
trainer.stop_training()
trainer.get_status()

# æ•°æ®ç”Ÿæˆ API
from self_instruct_generator import SelfInstructGenerator

generator = SelfInstructGenerator()
data = generator.generate_instructions(num_instructions=100)
generator.save_data(data, "data/llmops/sft/sft_data.json")

# åå¥½æ”¶é›† API
from preference_collector import PreferenceCollector

collector = PreferenceCollector()
collector.collect_preference(
    question="...",
    response_a="...",
    response_b="...",
    choice="A"
)

# æŽ¨ç† API
from inference_model import InferenceModel

model = InferenceModel()
model.load_model("chat-v1", "Qwen/Qwen2-1.5B", "checkpoints/sft/sft-lora")
response = model.generate("chat-v1", "ä½ å¥½", max_length=512)
```

### C. ç›¸å…³èµ„æº

- [LLaMA-Factory æ–‡æ¡£](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen2 æ¨¡åž‹](https://huggingface.co/Qwen)
- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [DPO è®ºæ–‡](https://arxiv.org/abs/2305.18290)
- [Self-Instruct è®ºæ–‡](https://arxiv.org/abs/2212.10560)

### D. æœ¯è¯­è¡¨

| æœ¯è¯­ | å…¨ç§° | è¯´æ˜Ž |
|------|------|------|
| **CPT** | Continued Pre-Training | ç»§ç»­é¢„è®­ç»ƒ |
| **SFT** | Supervised Fine-Tuning | ç›‘ç£å¾®è°ƒ |
| **DPO** | Direct Preference Optimization | ç›´æŽ¥åå¥½ä¼˜åŒ– |
| **LoRA** | Low-Rank Adaptation | ä½Žç§©é€‚é… |
| **RLHF** | Reinforcement Learning from Human Feedback | åŸºäºŽäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  |
| **PPO** | Proximal Policy Optimization | è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– |

