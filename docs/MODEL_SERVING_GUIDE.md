# ğŸ¤– æ¨¡å‹æœåŠ¡æŠ€æœ¯æŒ‡å— ([è¿”å›README](../README.md))

## 1. æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»MLOpsæœç´¢å¼•æ“æµ‹è¯•åºŠä¸­çš„æ¨¡å‹æœåŠ¡(Model Serving)ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€APIæ¥å£ã€éƒ¨ç½²æ–¹å¼å’Œè¿ç»´ç®¡ç†ã€‚

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æ•´ä½“æ¶æ„

æ¨¡å‹æœåŠ¡é‡‡ç”¨**ç‹¬ç«‹è¿›ç¨‹æ¶æ„**ï¼Œä»å¤–åˆ°å†…åˆ†ä¸º4å±‚ï¼š

1. **å®¢æˆ·ç«¯å±‚**: å„ç§å®¢æˆ·ç«¯é€šè¿‡HTTPè¯·æ±‚è®¿é—®æœåŠ¡
2. **APIç½‘å…³å±‚**: FlaskæœåŠ¡å™¨å¤„ç†HTTPè¯·æ±‚å’Œå“åº”
3. **ä¸šåŠ¡é€»è¾‘å±‚**: ModelServiceæ ¸å¿ƒä¸šåŠ¡é€»è¾‘
4. **æ¨¡å‹å±‚**: å…·ä½“çš„æœºå™¨å­¦ä¹ æ¨¡å‹å®ç°

#### ç‹¬ç«‹è¿›ç¨‹æ¶æ„ä¼˜åŠ¿

- **é«˜å¯ç”¨æ€§**: æ¨¡å‹æœåŠ¡å´©æºƒä¸å½±å“ä¸»ç³»ç»Ÿ
- **ç‹¬ç«‹æ‰©å±•**: å¯ä»¥ç‹¬ç«‹æ‰©å±•æ¨¡å‹æœåŠ¡èµ„æº
- **è¿›ç¨‹éš”ç¦»**: å†…å­˜å’ŒCPUèµ„æºéš”ç¦»
- **ç‹¬ç«‹ç›‘æ§**: å¯ä»¥ç‹¬ç«‹ç›‘æ§æ¨¡å‹æœåŠ¡æ€§èƒ½
- **æ˜“äºéƒ¨ç½²**: æ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²

#### æ¶æ„åˆ†å±‚è¯´æ˜

**å®¢æˆ·ç«¯å±‚**:
- æœç´¢ç•Œé¢ã€è®­ç»ƒç•Œé¢ã€å¤–éƒ¨ç³»ç»Ÿç­‰å®¢æˆ·ç«¯
- é€šè¿‡HTTPåè®®ä¸æ¨¡å‹æœåŠ¡é€šä¿¡

**APIç½‘å…³å±‚**:
- Flask WebæœåŠ¡å™¨ï¼Œç›‘å¬8501ç«¯å£
- å¤„ç†HTTPè¯·æ±‚è·¯ç”±ã€å‚æ•°éªŒè¯ã€å“åº”æ ¼å¼åŒ–
- æä¾›RESTful APIæ¥å£

**ä¸šåŠ¡é€»è¾‘å±‚**:
- `ModelService`ç±»ï¼šæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ç¼–æ’
- `create_model_instance()`ï¼šåˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹å®ä¾‹
- `get_model_instance()`ï¼šè·å–æ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒåŠ¨æ€åŠ è½½
- `predict_ctr()`ï¼šæ‰§è¡ŒCTRé¢„æµ‹ï¼Œåè°ƒä¸åŒæ¨¡å‹
- `switch_model()`ï¼šåˆ‡æ¢å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç±»å‹

**æ¨¡å‹å±‚**:
- `CTRModel`ï¼šé€»è¾‘å›å½’æ¨¡å‹å®ç°ï¼ˆsklearnï¼‰
- `WideAndDeepCTRModel`ï¼šWide & Deepæ¨¡å‹å®ç°ï¼ˆTensorFlowï¼‰
- ç‰¹å¾æå–ï¼šæ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„ç‰¹å¾å·¥ç¨‹é€»è¾‘
- æ¨¡å‹è®­ç»ƒï¼šæ”¯æŒåœ¨çº¿è®­ç»ƒå’Œæ¨¡å‹æ›´æ–°


```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯å±‚"
        A[æœç´¢ç•Œé¢] --> B[HTTPè¯·æ±‚]
        C[è®­ç»ƒç•Œé¢] --> B
        D[å¤–éƒ¨ç³»ç»Ÿ] --> B
    end
    
    subgraph "APIç½‘å…³å±‚"
        B --> E[Flask API Server<br/>ç«¯å£:8501]
    end
    
    subgraph "ä¸šåŠ¡é€»è¾‘å±‚"
        E --> F[ModelService<br/>æ¨¡å‹æœåŠ¡æ ¸å¿ƒ]
        F --> G[create_model_instance<br/>æ¨¡å‹å®ä¾‹ç®¡ç†]
        F --> H[get_model_instance<br/>æ¨¡å‹è·å–]
        F --> I[predict_ctr<br/>é¢„æµ‹å¼•æ“]
    end
    
    subgraph "æ¨¡å‹å±‚"
        G --> J[CTRModel<br/>é€»è¾‘å›å½’sklearn]
        G --> K[WideAndDeepCTRModel<br/>Wide & Deep TensorFlow]
        H --> J
        H --> K
        I --> J
        I --> K
    end
```

### 2.2 æ ¸å¿ƒç»„ä»¶

#### 2.2.1 ModelServiceç±»
```python
# æ–‡ä»¶è·¯å¾„: src/search_engine/model_service.py
class ModelService:
    """æ¨¡å‹æœåŠ¡ï¼šè´Ÿè´£æ¨¡å‹è®­ç»ƒã€é…ç½®ç®¡ç†ã€æ¨¡å‹æ–‡ä»¶ç­‰"""
    
    def __init__(self, model_file: str = None):
        if model_file is None:
            model_file = os.path.join(os.getcwd(), "models", "ctr_model.pkl")
        self.model_file = model_file
        self.ctr_model = CTRModel()  # é»˜è®¤ä½¿ç”¨LRæ¨¡å‹
        self.current_model_type = "logistic_regression"
        self.model_instances = {}  # å­˜å‚¨ä¸åŒç±»å‹çš„æ¨¡å‹å®ä¾‹
        self._load_model()
        
        # Flask API æœåŠ¡ç›¸å…³
        self.flask_app = None
        self.api_running = False
```

#### 2.2.2 APIè·¯ç”±è®¾è®¡
```python
def _setup_api_routes(self):
    """è®¾ç½®APIè·¯ç”±"""
    
    @self.flask_app.route('/health', methods=['GET'])
    def health():
        """å¥åº·æ£€æŸ¥"""
        return jsonify({
            "status": "healthy",
            "model_type": self.current_model_type,
            "model_trained": self.ctr_model.is_trained
        })
    
    @self.flask_app.route('/v1/models', methods=['GET'])
    def list_models():
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        models = []
        for model_type in ['logistic_regression', 'wide_and_deep']:
            try:
                model_instance = self.get_model_instance(model_type)
                models.append({
                    "name": model_type,
                    "status": "loaded" if model_instance.is_trained else "unloaded",
                    "type": "pickle" if model_type == 'logistic_regression' else "tensorflow"
                })
            except:
                models.append({
                    "name": model_type,
                    "status": "error",
                    "type": "pickle" if model_type == 'logistic_regression' else "tensorflow"
                })
        
        return jsonify({"model": models})
    
    @self.flask_app.route('/v1/models/<model_name>/predict', methods=['POST'])
    def predict(model_name):
        """æ¨¡å‹é¢„æµ‹"""
        try:
            data = request.get_json()
            if not data:
                return jsonify({"error": "No JSON data provided"}), 400
            
            # æå–è¾“å…¥æ•°æ®
            inputs = data.get('inputs', {})
            if not inputs:
                return jsonify({"error": "No inputs provided"}), 400
            
            # æ‰§è¡Œé¢„æµ‹
            ctr_score = self.predict_ctr(inputs, model_name)
            
            return jsonify({
                "outputs": {"ctr_score": ctr_score}
            })
            
        except ValueError as e:
            return jsonify({"error": str(e)}), 404
        except Exception as e:
            return jsonify({"error": str(e)}), 500
```

## 3. APIæ¥å£è§„èŒƒ

### 3.1 å¥åº·æ£€æŸ¥æ¥å£

**æ¥å£åœ°å€**: `GET /health`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X GET http://localhost:8501/health
```

**å“åº”ç¤ºä¾‹**:
```json
{
    "status": "healthy",
    "model_type": "logistic_regression",
    "model_trained": true
}
```

**å“åº”å­—æ®µè¯´æ˜**:
- `status`: æœåŠ¡çŠ¶æ€ï¼Œ`healthy`è¡¨ç¤ºæ­£å¸¸
- `model_type`: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
- `model_trained`: æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ

### 3.2 æ¨¡å‹åˆ—è¡¨æ¥å£

**æ¥å£åœ°å€**: `GET /v1/models`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X GET http://localhost:8501/v1/models
```

**å“åº”ç¤ºä¾‹**:
```json
{
    "model": [
        {
            "name": "logistic_regression",
            "status": "loaded",
            "type": "pickle"
        },
        {
            "name": "wide_and_deep",
            "status": "loaded", 
            "type": "tensorflow"
        }
    ]
}
```

**å“åº”å­—æ®µè¯´æ˜**:
- `name`: æ¨¡å‹åç§°
- `status`: æ¨¡å‹çŠ¶æ€ï¼Œ`loaded`è¡¨ç¤ºå·²åŠ è½½ï¼Œ`unloaded`è¡¨ç¤ºæœªåŠ è½½
- `type`: æ¨¡å‹ç±»å‹ï¼Œ`pickle`è¡¨ç¤ºsklearnæ¨¡å‹ï¼Œ`tensorflow`è¡¨ç¤ºTensorFlowæ¨¡å‹

### 3.3 æ¨¡å‹ä¿¡æ¯æ¥å£

**æ¥å£åœ°å€**: `GET /v1/models/{model_name}`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X GET http://localhost:8501/v1/models/logistic_regression
```

**å“åº”ç¤ºä¾‹**:
```json
{
    "model": {
        "name": "logistic_regression",
        "status": "loaded",
        "type": "pickle"
    }
}
```

### 3.4 å•æ¬¡é¢„æµ‹æ¥å£

**æ¥å£åœ°å€**: `POST /v1/models/{model_name}:predict`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8501/v1/models/logistic_regression:predict \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": {
      "query": "äººå·¥æ™ºèƒ½",
      "doc_id": "test_doc_001",
      "position": 1,
      "score": 0.8,
      "summary": "äººå·¥æ™ºèƒ½æŠ€æœ¯ä»‹ç»"
    }
  }'
```

**è¯·æ±‚å‚æ•°è¯´æ˜**:
- `query`: ç”¨æˆ·æŸ¥è¯¢è¯
- `doc_id`: æ–‡æ¡£ID
- `position`: æ–‡æ¡£ä½ç½®
- `score`: åŸºç¡€åˆ†æ•°
- `summary`: æ–‡æ¡£æ‘˜è¦

**å“åº”ç¤ºä¾‹**:
```json
{
    "outputs": {
        "ctr_score": 0.123456
    }
}
```

### 3.5 æ‰¹é‡é¢„æµ‹æ¥å£

**æ¥å£åœ°å€**: `POST /v1/models/{model_name}/batch_predict`

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8501/v1/models/logistic_regression/batch_predict \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": [
      {
        "query": "æœºå™¨å­¦ä¹ ",
        "doc_id": "doc1",
        "position": 1,
        "score": 0.9,
        "summary": "æœºå™¨å­¦ä¹ ä»‹ç»"
      },
      {
        "query": "æ·±åº¦å­¦ä¹ ", 
        "doc_id": "doc2",
        "position": 2,
        "score": 0.7,
        "summary": "æ·±åº¦å­¦ä¹ ä»‹ç»"
      }
    ]
  }'
```

**å“åº”ç¤ºä¾‹**:
```json
{
    "outputs": [
        {
            "ctr_score": 0.234567
        },
        {
            "ctr_score": 0.345678
        }
    ]
}
```

## 4. æ¨¡å‹ç®¡ç†

### 4.1 æ”¯æŒçš„æ¨¡å‹ç±»å‹

#### 4.1.1 é€»è¾‘å›å½’æ¨¡å‹ (Logistic Regression)
- **æ–‡ä»¶æ ¼å¼**: `.pkl` (pickle)
- **å­˜å‚¨è·¯å¾„**: `models/ctr_model.pkl`
- **ç‰¹å¾ç»´åº¦**: 7ç»´ç‰¹å¾å‘é‡
- **é€‚ç”¨åœºæ™¯**: å¿«é€Ÿé¢„æµ‹ï¼Œèµ„æºæ¶ˆè€—ä½

#### 4.1.2 Wide & Deepæ¨¡å‹
- **æ–‡ä»¶æ ¼å¼**: TensorFlow SavedModel + H5æ ¼å¼
- **å­˜å‚¨è·¯å¾„**: `models/wide_deep_ctr_model.h5` (ä¸»æ¨¡å‹) + `models/wide_deep_ctr_model_tf_serving/` (TF Servingæ ¼å¼)
- **ç‰¹å¾ç»´åº¦**: å¤šç»´åº¦ç‰¹å¾ (Wideç‰¹å¾ + Deepç‰¹å¾)
- **é€‚ç”¨åœºæ™¯**: é«˜ç²¾åº¦é¢„æµ‹ï¼Œæ”¯æŒç‰¹å¾äº¤äº’

### 4.2 æ¨¡å‹åŠ è½½æœºåˆ¶

```python
def get_model_instance(self, model_type: str):
    """è·å–æŒ‡å®šç±»å‹çš„æ¨¡å‹å®ä¾‹"""
    # æ¯æ¬¡éƒ½é‡æ–°åˆ›å»ºå®ä¾‹ï¼Œç¡®ä¿åŠ è½½æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    # è¿™è§£å†³äº†è®­ç»ƒåæ¨¡å‹ä¸åŒæ­¥çš„é—®é¢˜
    self.model_instances[model_type] = self.create_model_instance(model_type)
    return self.model_instances[model_type]

def create_model_instance(self, model_type: str):
    """åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹å®ä¾‹"""
    try:
        if model_type in self.model_instances:
            return self.model_instances[model_type]
        
        if model_type == 'logistic_regression':
            from .training_tab.ctr_model import CTRModel
            model_instance = CTRModel()
            model_file = os.path.join(os.getcwd(), "models", "ctr_model.pkl")
        elif model_type == 'wide_and_deep':
            from .training_tab.ctr_wide_deep_model import WideAndDeepCTRModel
            model_instance = WideAndDeepCTRModel()
            model_file = os.path.join(os.getcwd(), "models", "wide_deep_ctr_model")
        else:
            raise ValueError(f"æœªå®ç°çš„æ¨¡å‹ç±»å‹: {model_type}")
        
        model_instance.load_model(model_file)
        self.model_instances[model_type] = model_instance
        return model_instance
        
    except Exception as e:
        print(f"åˆ›å»ºæ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
        # å›é€€åˆ°é»˜è®¤LRæ¨¡å‹
        from .training_tab.ctr_model import CTRModel
        return CTRModel()
```

### 4.3 ç‰¹å¾å·¥ç¨‹

#### 4.3.1 é€»è¾‘å›å½’æ¨¡å‹ç‰¹å¾
```python
def _prepare_features(self, features: Dict[str, Any]) -> Optional[List[float]]:
    """å‡†å¤‡ç‰¹å¾å‘é‡"""
    try:
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨¡å‹ç‰¹å¾è¿›è¡Œè½¬æ¢
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®è®­ç»ƒæ—¶çš„ç‰¹å¾å·¥ç¨‹é€»è¾‘
        feature_vector = []
        
        # åŸºæœ¬ç‰¹å¾
        feature_vector.append(features.get('position', 1))
        feature_vector.append(features.get('score', 0.0))
        feature_vector.append(features.get('match_score', 0.0))
        feature_vector.append(features.get('query_ctr', 0.1))
        feature_vector.append(features.get('doc_ctr', 0.1))
        
        return feature_vector
    except Exception as e:
        print(f"ç‰¹å¾å‡†å¤‡å¤±è´¥: {e}")
        return None
```

#### 4.3.2 Wide & Deepæ¨¡å‹ç‰¹å¾
```python
def extract_features(self, ctr_data: List[Dict[str, Any]], is_training: bool = True, train_indices: Optional[np.ndarray] = None) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
    """
    ä»CTRæ•°æ®ä¸­æå–Wideå’ŒDeepç‰¹å¾ï¼ˆä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ï¼‰
    
    Args:
        ctr_data: CTRæ•°æ®åˆ—è¡¨
        is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        train_indices: è®­ç»ƒé›†ç´¢å¼•ï¼ˆç”¨äºé¿å…æ•°æ®æ³„éœ²ï¼‰
    
    Returns:
        Tuple[Dict[str, np.ndarray], np.ndarray]: (ç‰¹å¾å­—å…¸, æ ‡ç­¾æ•°ç»„)
    """
    # Wideç‰¹å¾ï¼šçº¿æ€§ç‰¹å¾ï¼Œå¦‚ä½ç½®ã€æ–‡æ¡£é•¿åº¦ç­‰
    # Deepç‰¹å¾ï¼šé«˜ç»´ç‰¹å¾ï¼Œå¦‚æ–‡æœ¬åµŒå…¥ã€ç±»åˆ«ç‰¹å¾ç­‰
    # å…·ä½“å®ç°è¯·å‚è€ƒ ctr_wide_deep_model.py ä¸­çš„ extract_features æ–¹æ³•
```

## 5. éƒ¨ç½²ä¸å¯åŠ¨

### 5.1 è‡ªåŠ¨å¯åŠ¨é›†æˆ

æ¨¡å‹æœåŠ¡å·²é›†æˆåˆ°ç³»ç»Ÿå¯åŠ¨æµç¨‹ä¸­ï¼Œåœ¨`start_system.py`çš„æ­¥éª¤7è‡ªåŠ¨æ£€æŸ¥å’Œå¯åŠ¨ç‹¬ç«‹è¿›ç¨‹ï¼š

```python
def check_and_start_model_service():
    """æ£€æŸ¥å¹¶å¯åŠ¨æ¨¡å‹æœåŠ¡ï¼ˆç‹¬ç«‹è¿›ç¨‹ï¼‰"""
    # 1. æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²è¿è¡Œ
    model_service_url = "http://localhost:8501/health"
    try:
        req = request.Request(model_service_url, method="GET")
        with request.urlopen(req, timeout=2) as resp:
            if 200 <= resp.status < 300:
                print("âœ… æ£€æµ‹åˆ°å·²è¿è¡Œçš„æ¨¡å‹æœåŠ¡ï¼Œç›´æ¥å¤ç”¨")
                return True
    except Exception:
        pass
    
    # 2. å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
    model_service_script = os.path.join(os.path.dirname(__file__), 'start_model_serving.py')
    process = subprocess.Popen(
        [sys.executable, model_service_script],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        cwd=os.path.dirname(__file__)
    )
    
    # 3. ç­‰å¾…æœåŠ¡å¯åŠ¨å¹¶éªŒè¯
    time.sleep(3)
    try:
        req = request.Request(model_service_url, method="GET")
        with request.urlopen(req, timeout=5) as resp:
            if 200 <= resp.status < 300:
                print("âœ… æ¨¡å‹æœåŠ¡ç‹¬ç«‹è¿›ç¨‹å¯åŠ¨æˆåŠŸ")
                return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹æœåŠ¡å¯åŠ¨åå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False
```

### 5.2 ç‹¬ç«‹å¯åŠ¨

#### 5.2.1 ä½¿ç”¨å¯åŠ¨è„šæœ¬
```bash
# å¯åŠ¨æ¨¡å‹æœåŠ¡ç‹¬ç«‹è¿›ç¨‹
python start_model_serving.py

# åå°å¯åŠ¨
python start_model_serving.py &

# ä½¿ç”¨è¿›ç¨‹ç®¡ç†å™¨
python tools/model_service_manager.py start
python tools/model_service_manager.py stop
python tools/model_service_manager.py restart
python tools/model_service_manager.py status
python tools/model_service_manager.py health
```

**å¯åŠ¨è„šæœ¬å†…å®¹**:
```python
#!/usr/bin/env python3
"""
å¯åŠ¨Model Serving APIæœåŠ¡
"""

import sys
import os
import signal

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from search_engine.model_service import ModelService

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
    sys.exit(0)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Model Serving APIæœåŠ¡...")
    print("=" * 50)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡
        model_service = ModelService()
        
        print("ğŸ“‹ æœåŠ¡ä¿¡æ¯:")
        print(f"   åœ°å€: http://0.0.0.0:8501")
        print(f"   å¥åº·æ£€æŸ¥: http://localhost:8501/health")
        print(f"   æ¨¡å‹åˆ—è¡¨: http://localhost:8501/v1/models")
        print("   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
        print("=" * 50)
        
        # å¯åŠ¨æœåŠ¡ï¼ˆè¿™ä¼šé˜»å¡è¿›ç¨‹ï¼‰
        model_service.start_api_server(port=8501)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ å¯åŠ¨æœåŠ¡å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

#### 5.2.2 ç›´æ¥è°ƒç”¨API
```python
import sys
sys.path.append('src')
from search_engine.model_service import ModelService

# åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡
model_service = ModelService()
model_service.start_api_server(port=8501)
```

### 5.3 é…ç½®å‚æ•°

#### 5.3.1 æœåŠ¡é…ç½®
```python
# é»˜è®¤é…ç½®
DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8501
DEFAULT_DEBUG = False
DEFAULT_THREADED = True
```

#### 5.3.2 æ¨¡å‹é…ç½®
```python
# æ¨¡å‹æ–‡ä»¶è·¯å¾„
CTR_MODEL_PATH = "models/ctr_model.pkl"
WIDE_DEEP_MODEL_PATH = "models/wide_deep_ctr_model"  # æ³¨æ„ï¼šæ²¡æœ‰.h5æ‰©å±•å
WIDE_DEEP_H5_PATH = "models/wide_deep_ctr_model.h5"
WIDE_DEEP_TF_SERVING_PATH = "models/wide_deep_ctr_model_tf_serving/"

# ç‰¹å¾é…ç½®
FEATURE_DIMENSIONS = {
    'logistic_regression': 5,  # å®é™…æ˜¯5ç»´ç‰¹å¾
    'wide_and_deep': 'variable'
}
```

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 å¹¶å‘å¤„ç†

#### 6.1.1 å¤šçº¿ç¨‹æ”¯æŒ
```python
    def start_api_server(self, host="0.0.0.0", port=8501, debug=False):
        """å¯åŠ¨Flask APIæœåŠ¡å™¨ï¼ˆç‹¬ç«‹è¿›ç¨‹æ¨¡å¼ï¼‰"""
        try:
            if self.api_running:
                print("âš ï¸ APIæœåŠ¡å™¨å·²åœ¨è¿è¡Œ")
                return True
            
            self.flask_app = Flask(__name__)
            self._setup_api_routes()
            
            self.api_running = True
            print(f"ğŸš€ Model Serving APIå¯åŠ¨åœ¨ {host}:{port}")
            print("ğŸ“‹ å¯ç”¨æ¥å£:")
            print("   - å¥åº·æ£€æŸ¥: http://localhost:8501/health")
            print("   - æ¨¡å‹åˆ—è¡¨: http://localhost:8501/v1/models")
            print("   - é¢„æµ‹æ¥å£: http://localhost:8501/v1/models/<model_name>/predict")
            print("   - æ‰¹é‡é¢„æµ‹: http://localhost:8501/v1/models/<model_name>/batch_predict")
            print("=" * 50)
            
            # ç›´æ¥è¿è¡ŒFlaskæœåŠ¡å™¨ï¼ˆç‹¬ç«‹è¿›ç¨‹æ¨¡å¼ï¼‰
            self.flask_app.run(host=host, port=port, debug=debug, threaded=True, use_reloader=False)
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨APIæœåŠ¡å™¨å¤±è´¥: {e}")
            return False
```

#### 6.1.2 æ‰¹é‡é¢„æµ‹ä¼˜åŒ–
```python
@self.flask_app.route('/v1/models/<model_name>/batch_predict', methods=['POST'])
def batch_predict(model_name):
    """æ‰¹é‡é¢„æµ‹"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No JSON data provided"}), 400
        
        # æå–è¾“å…¥æ•°æ®
        inputs_list = data.get('inputs', [])
        if not inputs_list:
            return jsonify({"error": "No inputs provided"}), 400
        
        # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
        results = []
        for inputs in inputs_list:
            ctr_score = self.predict_ctr(inputs, model_name)
            results.append({"ctr_score": ctr_score})
        
        return jsonify({
            "outputs": results
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```

### 6.2 å†…å­˜ç®¡ç†

#### 6.2.1 æ¨¡å‹ç¼“å­˜
```python
class ModelService:
    def __init__(self, model_file: str = None):
        if model_file is None:
            model_file = os.path.join(os.getcwd(), "models", "ctr_model.pkl")
        self.model_file = model_file
        self.ctr_model = CTRModel()  # é»˜è®¤ä½¿ç”¨LRæ¨¡å‹
        self.current_model_type = "logistic_regression"
        self.model_instances = {}  # å­˜å‚¨ä¸åŒç±»å‹çš„æ¨¡å‹å®ä¾‹
        self._load_model()
        
        # Flask API æœåŠ¡ç›¸å…³
        self.flask_app = None
        self.api_running = False
```

#### 6.2.2 å†…å­˜ç›‘æ§
```python
def get_memory_usage(self) -> Dict[str, float]:
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        "rss": memory_info.rss / 1024 / 1024,  # MB
        "vms": memory_info.vms / 1024 / 1024,  # MB
        "percent": process.memory_percent()
    }
```

## 7. ç›‘æ§ä¸è¿ç»´

### 7.1 å¥åº·æ£€æŸ¥

#### 7.1.1 æœåŠ¡å¥åº·æ£€æŸ¥
```python
def health_check(self) -> Dict[str, Any]:
    """æœåŠ¡å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy" if self.api_running else "unhealthy",
        "model_type": self.current_model_type,
        "model_trained": self.ctr_model.is_trained,
        "uptime": time.time() - self.start_time,
        "memory_usage": self.get_memory_usage()
    }
```

#### 7.1.2 æ¨¡å‹å¥åº·æ£€æŸ¥
```python
def model_health_check(self, model_type: str) -> bool:
    """æ¨¡å‹å¥åº·æ£€æŸ¥"""
    try:
        model_instance = self.get_model_instance(model_type)
        if not model_instance.is_trained:
            return False
        
        # æµ‹è¯•é¢„æµ‹
        test_inputs = {
            'query': 'test',
            'doc_id': 'test',
            'position': 1,
            'score': 0.5,
            'summary': 'test summary'
        }
        result = self.predict_ctr(test_inputs, model_type)
        return isinstance(result, (int, float)) and 0 <= result <= 1
    except Exception:
        return False
```

### 7.2 æ—¥å¿—ç®¡ç†

#### 7.2.1 è¯·æ±‚æ—¥å¿—
```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/model_serving.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

@self.flask_app.before_request
def log_request_info():
    logger.info(f"Request: {request.method} {request.url}")
```

#### 7.2.2 æ€§èƒ½æ—¥å¿—
```python
import time

@self.flask_app.before_request
def before_request():
    request.start_time = time.time()

@self.flask_app.after_request
def after_request(response):
    duration = time.time() - request.start_time
    logger.info(f"Response time: {duration:.3f}s")
    return response
```

### 7.3 é”™è¯¯å¤„ç†

#### 7.3.1 å¼‚å¸¸å¤„ç†
```python
@app.errorhandler(404)
def not_found(error):
    return jsonify({"error": "æ¥å£ä¸å­˜åœ¨"}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"}), 500

@app.errorhandler(ValueError)
def value_error(error):
    return jsonify({"error": str(error)}), 400
```

#### 7.3.2 é™çº§ç­–ç•¥
```python
def predict_ctr(self, features: Dict[str, Any], model_type: Optional[str] = None) -> float:
    """é¢„æµ‹CTR"""
    try:
        # å§‹ç»ˆä½¿ç”¨æŒ‡å®šç±»å‹çš„æ¨¡å‹å®ä¾‹ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹
        if model_type:
            model_instance = self.get_model_instance(model_type)
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨å½“å‰é»˜è®¤æ¨¡å‹ç±»å‹
            model_instance = self.get_model_instance(self.current_model_type)
        
        if not model_instance.is_trained:
            return 0.1  # é»˜è®¤CTR
        
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹çš„predict_ctræ–¹æ³•
        query = features.get('query', '')
        doc_id = features.get('doc_id', '')
        position = features.get('position', 1)
        score = features.get('score', 0.0)
        summary = features.get('summary', '')
        current_timestamp = features.get('timestamp')
        
        return model_instance.predict_ctr(query, doc_id, position, score, summary, current_timestamp)
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        return 0.1  # é»˜è®¤CTRå€¼
```

## 8. æµ‹è¯•ä¸éªŒè¯

### 8.1 å•å…ƒæµ‹è¯•

#### 8.1.1 APIæ¥å£æµ‹è¯•
```python
import unittest
import requests

class TestModelServingAPI(unittest.TestCase):
    def setUp(self):
        self.base_url = "http://localhost:8501"
    
    def test_health_check(self):
        response = requests.get(f"{self.base_url}/health")
        self.assertEqual(response.status_code, 200)
        data = response.json()
        self.assertIn("status", data)
    
    def test_model_list(self):
        response = requests.get(f"{self.base_url}/v1/models")
        self.assertEqual(response.status_code, 200)
        data = response.json()
        self.assertIn("model", data)
    
    def test_predict(self):
        test_data = {
            "inputs": {
                "query": "äººå·¥æ™ºèƒ½",
                "doc_id": "test",
                "position": 1,
                "score": 0.8,
                "summary": "AIä»‹ç»"
            }
        }
        response = requests.post(
            f"{self.base_url}/v1/models/logistic_regression:predict",
            json=test_data
        )
        self.assertEqual(response.status_code, 200)
        data = response.json()
        self.assertIn("outputs", data)
```

### 8.2 æ€§èƒ½æµ‹è¯•

#### 8.2.1 å‹åŠ›æµ‹è¯•
```python
import concurrent.futures
import time

def stress_test():
    """å‹åŠ›æµ‹è¯•"""
    def make_request():
        test_data = {
            "inputs": {
                "query": "test",
                "doc_id": "test",
                "position": 1,
                "score": 0.5,
                "summary": "test"
            }
        }
        start_time = time.time()
        response = requests.post(
            "http://localhost:8501/v1/models/logistic_regression:predict",
            json=test_data
        )
        duration = time.time() - start_time
        return response.status_code, duration
    
    # å¹¶å‘æµ‹è¯•
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(make_request) for _ in range(100)]
        results = [future.result() for future in futures]
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for status, _ in results if status == 200)
    avg_duration = sum(duration for _, duration in results) / len(results)
    
    print(f"æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_duration:.3f}s")
```

### 8.3 é›†æˆæµ‹è¯•

#### 8.3.1 ç«¯åˆ°ç«¯æµ‹è¯•
```python
def test_end_to_end():
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    # 1. å¯åŠ¨æœåŠ¡
    model_service = ModelService()
    model_service.start_api_server(port=8501)
    time.sleep(3)
    
    # 2. å¥åº·æ£€æŸ¥
    response = requests.get("http://localhost:8501/health")
    assert response.status_code == 200
    
    # 3. æ¨¡å‹åˆ—è¡¨
    response = requests.get("http://localhost:8501/v1/models")
    assert response.status_code == 200
    
    # 4. é¢„æµ‹æµ‹è¯•
    test_data = {
        "inputs": {
            "query": "äººå·¥æ™ºèƒ½",
            "doc_id": "test_doc",
            "position": 1,
            "score": 0.8,
            "summary": "AIæŠ€æœ¯ä»‹ç»"
        }
    }
    response = requests.post(
        "http://localhost:8501/v1/models/logistic_regression:predict",
        json=test_data
    )
    assert response.status_code == 200
    
    # 5. åœæ­¢æœåŠ¡ï¼ˆç‹¬ç«‹è¿›ç¨‹ä¼šè‡ªåŠ¨åœæ­¢ï¼‰
```

## 9. æ•…éšœæ’æŸ¥

### 9.1 å¸¸è§é—®é¢˜

#### 9.1.1 ç«¯å£å ç”¨
**é—®é¢˜**: `Address already in use`
**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8501

# æ€æ­»è¿›ç¨‹
kill -9 <PID>

# æˆ–ä½¿ç”¨ä¸åŒç«¯å£
model_service.start_api_server(port=8502)
```

#### 9.1.2 æ¨¡å‹åŠ è½½å¤±è´¥
**é—®é¢˜**: `æ¨¡å‹åŠ è½½å¤±è´¥`
**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
import os
model_path = "models/ctr_model.pkl"
if not os.path.exists(model_path):
    print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æƒé™
if not os.access(model_path, os.R_OK):
    print(f"æ¨¡å‹æ–‡ä»¶æ— è¯»å–æƒé™: {model_path}")
```

#### 9.1.3 é¢„æµ‹ç»“æœå¼‚å¸¸
**é—®é¢˜**: `é¢„æµ‹ç»“æœä¸åœ¨[0,1]èŒƒå›´å†…`
**è§£å†³æ–¹æ¡ˆ**:
```python
def validate_prediction_result(result: float) -> bool:
    """éªŒè¯é¢„æµ‹ç»“æœ"""
    if not isinstance(result, (int, float)):
        return False
    if not (0 <= result <= 1):
        return False
    return True
```

### 9.2 è°ƒè¯•å·¥å…·

#### 9.2.1 è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
model_service.start_api_server(port=8501, debug=True)
```

#### 9.2.2 æ—¥å¿—çº§åˆ«
```python
import logging

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.getLogger().setLevel(logging.DEBUG)
```

#### 9.2.3 æ€§èƒ½åˆ†æ
```python
import cProfile
import pstats

def profile_prediction():
    """æ€§èƒ½åˆ†æ"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    # æ‰§è¡Œé¢„æµ‹
    result = model_service.predict_ctr(test_inputs, "logistic_regression")
    
    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)
```

## 10. æœ€ä½³å®è·µ

### 10.1 å¼€å‘å»ºè®®

1. **é”™è¯¯å¤„ç†**: å§‹ç»ˆåŒ…å«é€‚å½“çš„å¼‚å¸¸å¤„ç†
2. **è¾“å…¥éªŒè¯**: éªŒè¯æ‰€æœ‰è¾“å…¥å‚æ•°
3. **æ—¥å¿—è®°å½•**: è®°å½•å…³é”®æ“ä½œå’Œé”™è¯¯
4. **æ€§èƒ½ç›‘æ§**: ç›‘æ§å“åº”æ—¶é—´å’Œèµ„æºä½¿ç”¨
5. **ç‰ˆæœ¬ç®¡ç†**: ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬å·

### 10.2 éƒ¨ç½²å»ºè®®

1. **ç¯å¢ƒéš”ç¦»**: ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
2. **é…ç½®ç®¡ç†**: ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å‚æ•°
3. **å¥åº·æ£€æŸ¥**: å®ç°å®Œæ•´çš„å¥åº·æ£€æŸ¥æœºåˆ¶
4. **ç›‘æ§å‘Šè­¦**: è®¾ç½®æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦
5. **å¤‡ä»½æ¢å¤**: å®šæœŸå¤‡ä»½æ¨¡å‹æ–‡ä»¶

### 10.3 å®‰å…¨å»ºè®®

1. **è¾“å…¥éªŒè¯**: ä¸¥æ ¼éªŒè¯æ‰€æœ‰è¾“å…¥
2. **è®¿é—®æ§åˆ¶**: å®ç°é€‚å½“çš„è®¿é—®æ§åˆ¶
3. **æ•°æ®åŠ å¯†**: æ•æ„Ÿæ•°æ®ä¼ è¾“åŠ å¯†
4. **æ—¥å¿—å®‰å…¨**: é¿å…åœ¨æ—¥å¿—ä¸­è®°å½•æ•æ„Ÿä¿¡æ¯
5. **å®šæœŸæ›´æ–°**: å®šæœŸæ›´æ–°ä¾èµ–åŒ…

## 11. æ‰©å±•å¼€å‘

### 11.1 æ·»åŠ æ–°æ¨¡å‹

#### 11.1.1 å®ç°æ¨¡å‹ç±»
```python
class NewCTRModel:
    """æ–°CTRæ¨¡å‹å®ç°"""
    
    def __init__(self):
        self.model = None
        self.is_trained = False
    
    def train(self, data):
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    def predict(self, features):
        """é¢„æµ‹"""
        pass
```

#### 11.1.2 é›†æˆåˆ°ModelService
```python
class ModelService:
    def __init__(self):
        self.new_model = NewCTRModel()
    
    def get_model_instance(self, model_type: str):
        if model_type == 'new_model':
            return self.new_model
        # ... å…¶ä»–æ¨¡å‹
```

### 11.2 æ·»åŠ æ–°æ¥å£

#### 11.2.1 å®šä¹‰è·¯ç”±
```python
@self.flask_app.route('/v1/models/<model_name>/evaluate', methods=['POST'])
def evaluate_model(model_name):
    """æ¨¡å‹è¯„ä¼°æ¥å£"""
    try:
        data = request.get_json()
        # å®ç°è¯„ä¼°é€»è¾‘
        return jsonify({"evaluation": "success"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```

### 11.3 æ€§èƒ½ä¼˜åŒ–

#### 11.3.1 ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_predict(features_hash: str, model_type: str) -> float:
    """ç¼“å­˜é¢„æµ‹ç»“æœ"""
    return self.predict_ctr(features, model_type)
```

#### 11.3.2 å¼‚æ­¥å¤„ç†
```python
import asyncio
import aiohttp

async def async_batch_predict(inputs_list, model_type):
    """å¼‚æ­¥æ‰¹é‡é¢„æµ‹"""
    tasks = []
    for inputs in inputs_list:
        task = asyncio.create_task(
            self.predict_ctr_async(inputs, model_type)
        )
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

## 12. æ€»ç»“

æ¨¡å‹æœåŠ¡ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„CTRé¢„æµ‹èƒ½åŠ›ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œçµæ´»çš„APIæ¥å£ã€‚é€šè¿‡Flask-basedæ¶æ„ï¼Œå®ç°äº†é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ¨¡å‹æœåŠ¡ï¼Œä¸ºæœç´¢æ¨èç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æœºå™¨å­¦ä¹ æ”¯æŒã€‚

### 12.1 æ ¸å¿ƒç‰¹æ€§
- âœ… **å¤šæ¨¡å‹æ”¯æŒ**: é€»è¾‘å›å½’ã€Wide & Deep
- âœ… **RESTful API**: æ ‡å‡†åŒ–çš„HTTPæ¥å£
- âœ… **è‡ªåŠ¨å¯åŠ¨**: é›†æˆåˆ°ç³»ç»Ÿå¯åŠ¨æµç¨‹
- âœ… **å¥åº·æ£€æŸ¥**: å®Œæ•´çš„ç›‘æ§æœºåˆ¶
- âœ… **æ‰¹é‡é¢„æµ‹**: é«˜æ•ˆçš„æ‰¹é‡å¤„ç†
- âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†

### 12.2 æŠ€æœ¯ä¼˜åŠ¿
- ğŸš€ **é«˜æ€§èƒ½**: å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
- ğŸ”§ **æ˜“æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°æ¨¡å‹
- ğŸ“Š **å¯ç›‘æ§**: å®Œæ•´çš„æ—¥å¿—å’Œç›‘æ§ä½“ç³»
- ğŸ›¡ï¸ **é«˜å¯é **: é™çº§ç­–ç•¥å’Œé”™è¯¯æ¢å¤
- ğŸ”„ **çƒ­æ›´æ–°**: æ”¯æŒæ¨¡å‹åŠ¨æ€åŠ è½½

### 12.3 åº”ç”¨åœºæ™¯
- æœç´¢æ¨èç³»ç»Ÿçš„CTRé¢„æµ‹
- å¹¿å‘ŠæŠ•æ”¾çš„æ•ˆæœé¢„ä¼°
- å†…å®¹æ¨èçš„ç‚¹å‡»ç‡é¢„æµ‹
- ä¸ªæ€§åŒ–æ’åºçš„æœºå™¨å­¦ä¹ æœåŠ¡

é€šè¿‡æœ¬æŒ‡å—ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿç†è§£å’Œä½¿ç”¨æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œå¹¶æ ¹æ®ä¸šåŠ¡éœ€æ±‚è¿›è¡Œå®šåˆ¶åŒ–å¼€å‘ã€‚
