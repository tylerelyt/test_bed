# ğŸ¤– GUI-Agent ç³»ç»ŸæŠ€æœ¯æŒ‡å— ([è¿”å›README](../README.md))

## ğŸ“‹ ç›®å½•

- [1. ç³»ç»Ÿæ¦‚è¿°](#1-ç³»ç»Ÿæ¦‚è¿°)
- [2. æŠ€æœ¯æ¶æ„è®¾è®¡](#2-æŠ€æœ¯æ¶æ„è®¾è®¡)
- [3. æ ¸å¿ƒç»„ä»¶å®ç°](#3-æ ¸å¿ƒç»„ä»¶å®ç°)
- [4. å…³é”®æŠ€æœ¯è¯¦è§£](#4-å…³é”®æŠ€æœ¯è¯¦è§£)
- [5. ä½¿ç”¨æŒ‡å—](#5-ä½¿ç”¨æŒ‡å—)
- [6. æ€§èƒ½ä¸ä¼˜åŒ–](#6-æ€§èƒ½ä¸ä¼˜åŒ–)
- [7. æ•…éšœæ’é™¤](#7-æ•…éšœæ’é™¤)
- [8. æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)
- [9. è¿›é˜¶æ‰©å±•](#9-è¿›é˜¶æ‰©å±•)

---

## 1. ç³»ç»Ÿæ¦‚è¿°

### 1.1 åŠŸèƒ½ä»‹ç»

GUI-Agent æ˜¯ä¸€ä¸ªåŸºäº OSWorld æ¶æ„çš„æ™ºèƒ½æ¡Œé¢è‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿï¼Œæ”¯æŒï¼š

- ğŸ‘€ **æ™ºèƒ½è§‚å¯Ÿ**ï¼šè‡ªåŠ¨æˆªå–å±å¹•å¹¶ç†è§£å½“å‰çŠ¶æ€
- ğŸ§  **è§†è§‰æ¨ç†**ï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç†è§£ä»»åŠ¡å¹¶å†³ç­–
- ğŸ–±ï¸ **ç²¾ç¡®æ‰§è¡Œ**ï¼šåœ¨è™šæ‹Ÿæœºæˆ–æœ¬åœ°ç³»ç»Ÿä¸­æ‰§è¡Œé¼ æ ‡ã€é”®ç›˜æ“ä½œ
- ğŸ”„ **æŒç»­å¾ªç¯**ï¼šè§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ç›´åˆ°ä»»åŠ¡å®Œæˆ
- ğŸ›¡ï¸ **å®‰å…¨éš”ç¦»**ï¼šæ”¯æŒè™šæ‹Ÿæœºæ¨¡å¼ï¼Œä¿æŠ¤ä¸»æœºç³»ç»Ÿå®‰å…¨

### 1.2 æŠ€æœ¯ç‰¹ç‚¹

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ ¸å¿ƒæŠ€æœ¯** | VLM + ç¯å¢ƒæ§åˆ¶ |
| **ä¸»è¦åŠŸèƒ½** | æ¡Œé¢ä»»åŠ¡è‡ªåŠ¨åŒ– |
| **è¾“å…¥æ–¹å¼** | ä»»åŠ¡æŒ‡ä»¤ + å±å¹•æˆªå›¾ |
| **è¾“å‡ºç»“æœ** | è‡ªåŠ¨åŒ–æ“ä½œåºåˆ— |
| **åº”ç”¨åœºæ™¯** | RPAã€UI æµ‹è¯•ã€ä»»åŠ¡æ‰§è¡Œ |
| **éƒ¨ç½²æ¨¡å¼** | æœ¬åœ°/è™šæ‹Ÿæœºéš”ç¦» |

---

## 2. æŠ€æœ¯æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "ä»»åŠ¡å±‚ - Task Layer"
        A[ç”¨æˆ·ä»»åŠ¡æŒ‡ä»¤] --> B[ä»»åŠ¡è§£æå™¨]
        B --> C[ä»»åŠ¡é…ç½®]
    end
    
    subgraph "Agent å±‚ - Agent Layer"
        C --> D[SimplePromptAgent]
        D --> D1[å†å²è½¨è¿¹ç®¡ç†]
        D --> D2[Prompt æ„å»º]
        D --> D3[åŠ¨ä½œè§£æ]
    end
    
    subgraph "æ¨¡å‹å±‚ - Model Layer"
        D2 --> E[è§†è§‰è¯­è¨€æ¨¡å‹]
        E --> E1[Qwen-VL]
        E --> E2[GPT-4V]
        E --> E3[QVQ]
        E1 --> F[æ€è€ƒè¿‡ç¨‹]
        E2 --> F
        E3 --> F
        F --> G[åŠ¨ä½œåºåˆ—]
    end
    
    subgraph "ç¯å¢ƒå±‚ - Environment Layer"
        G --> H[SimpleDesktopEnv]
        H --> H1[æœ¬åœ°æ§åˆ¶å™¨]
        H --> H2[VM æ§åˆ¶å™¨]
        H1 --> I[PyAutoGUI]
        H2 --> J[Docker API]
    end
    
    subgraph "è§‚å¯Ÿå±‚ - Observation Layer"
        I --> K[å±å¹•æˆªå›¾]
        J --> K
        K --> L[æˆªå›¾ç¼–ç ]
        L --> D
    end
    
    subgraph "æ‰§è¡Œå±‚ - Execution Layer"
        I --> M[æœ¬åœ°åŠ¨ä½œ]
        J --> N[VM åŠ¨ä½œ]
        M --> O[é¼ æ ‡/é”®ç›˜]
        N --> O
    end
    
    O --> P[ç¯å¢ƒçŠ¶æ€æ›´æ–°]
    P --> K
```

### 2.2 OSWorld æ ¸å¿ƒæ€æƒ³

**å‚è€ƒ**: [OSWorld GitHub](https://github.com/xlang-ai/OSWorld)

GUI-Agent åŸºäº OSWorld çš„æ ¸å¿ƒæ¶æ„å®ç°ï¼š

1. **ç¯å¢ƒæŠ½è±¡**ï¼š`SimpleDesktopEnv` å¯¹åº” OSWorld çš„ `DesktopEnv`
2. **Agent è®¾è®¡**ï¼š`SimplePromptAgent` å¯¹åº” OSWorld çš„ `PromptAgent`
3. **è§‚å¯Ÿ-è¡ŒåŠ¨å¾ªç¯**ï¼šæˆªå›¾ â†’ æ¨¡å‹æ¨ç† â†’ åŠ¨ä½œæ‰§è¡Œ â†’ é‡å¤
4. **åŠ¨ä½œç©ºé—´**ï¼šä½¿ç”¨ PyAutoGUI å‘½ä»¤ï¼ˆä¸ OSWorld ä¸€è‡´ï¼‰

---

## 3. æ ¸å¿ƒç»„ä»¶å®ç°

### 3.1 SimpleDesktopEnv - ç¯å¢ƒæ§åˆ¶

```python
# æ–‡ä»¶: src/search_engine/gui_agent_service.py
class SimpleDesktopEnv:
    """ç®€åŒ–ç‰ˆæ¡Œé¢ç¯å¢ƒ - åŸºäº OSWorld DesktopEnv"""
    
    def __init__(
        self,
        provider_name: str = "local",
        os_type: str = "macOS",
        action_space: str = "pyautogui",
        screen_size: Tuple[int, int] = (1920, 1080)
    ):
        self.provider_name = provider_name
        self.os_type = os_type
        self.action_space = action_space
        self.screen_size = screen_size
        
        # åˆå§‹åŒ–æ§åˆ¶å™¨
        self._init_controller()
    
    def reset(self, task_config: Optional[Dict] = None) -> Dict:
        """é‡ç½®ç¯å¢ƒå¹¶åŠ è½½ä»»åŠ¡"""
        self.current_task = task_config or {}
        self.step_count = 0
        self.history = []
        
        # æ•è·åˆå§‹è§‚å¯Ÿ
        obs = self._get_observation()
        obs['instruction'] = self.current_task.get('instruction', 'æœªæŒ‡å®šä»»åŠ¡')
        
        return obs
    
    def step(self, action: str) -> Tuple[Dict, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›æ–°çš„è§‚å¯Ÿ"""
        try:
            # æ‰§è¡ŒåŠ¨ä½œ
            self._execute_action(action)
            
            # è·å–æ–°è§‚å¯Ÿ
            obs = self._get_observation()
            
            # æ›´æ–°å†å²
            self.history.append({
                'step': self.step_count,
                'action': action,
                'timestamp': datetime.now().isoformat()
            })
            
            self.step_count += 1
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            done = action in ['DONE', 'FAIL'] or self.step_count >= self.max_steps
            reward = 1.0 if action == 'DONE' else 0.0
            
            return obs, reward, done, {}
            
        except Exception as e:
            print(f"âŒ åŠ¨ä½œæ‰§è¡Œå¤±è´¥: {e}")
            return self._get_observation(), 0.0, True, {'error': str(e)}
```

### 3.2 SimplePromptAgent - æ™ºèƒ½å†³ç­–

```python
class SimplePromptAgent:
    """åŸºäº Prompt çš„æ™ºèƒ½ Agent - OSWorld é£æ ¼"""
    
    def __init__(
        self,
        model: str = "qwen3-vl-plus",
        api_key: str = None,
        base_url: str = None,
        max_tokens: int = 512,
        temperature: float = 0.0,
        enable_thinking: bool = False,
        use_trajectory: bool = True
    ):
        self.model = model
        self.api_key = api_key or os.getenv('DASHSCOPE_API_KEY')
        self.base_url = base_url
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.enable_thinking = enable_thinking
        self.use_trajectory = use_trajectory
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
        
        # å†å²è½¨è¿¹
        self.trajectory_screenshots = []
        self.trajectory_actions = []
    
    def predict(self, instruction: str, observation: Dict) -> Tuple[str, List[str]]:
        """æ ¹æ®è§‚å¯Ÿé¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œ"""
        
        # 1. æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²è½¨è¿¹ï¼‰
        messages = self._build_messages(instruction, observation)
        
        # 2. è°ƒç”¨æ¨¡å‹
        response_text = self._call_model(messages)
        
        # 3. è§£æåŠ¨ä½œ
        actions = self._parse_actions(response_text)
        
        # 4. æ›´æ–°è½¨è¿¹
        self._update_trajectory(instruction, observation, response_text, actions)
        
        return response_text, actions
    
    def _build_messages(self, instruction: str, observation: Dict) -> List[Dict]:
        """æ„å»º VLM æ¶ˆæ¯ - ä½¿ç”¨ OSWorld å®˜æ–¹ Prompt"""
        
        # ç³»ç»Ÿ Prompt - OSWorld å®˜æ–¹ç‰ˆæœ¬ (SYS_PROMPT_IN_SCREENSHOT_OUT_CODE)
        # å‚è€ƒ: https://github.com/xlang-ai/OSWorld/blob/main/mm_agents/prompts.py
        system_prompt = """You are an agent which follow my instruction and perform desktop computer tasks as instructed.
You have good knowledge of computer and good internet connection and assume your code will run on a computer for controlling the mouse and keyboard.
For each step, you will get an observation of an image, which is the screenshot of the computer screen and you will predict the action of the computer based on the image.

You are required to use `pyautogui` to perform the action grounded to the observation, but DONOT use the `pyautogui.locateCenterOnScreen` function to locate the element you want to operate with since we have no image of the element you want to operate with. DONOT USE `pyautogui.screenshot()` to make screenshot.
Return one line or multiple lines of python code to perform the action each time, be time efficient. When predicting multiple lines of code, make some small sleep like `time.sleep(0.5);` interval so that the machine could take; Each time you need to predict a complete code, no variables or function can be shared from history
You need to to specify the coordinates of by yourself based on your observation of current observation, but you should be careful to ensure that the coordinates are correct.

Specially, it is also allowed to return the following special code:
When you think you have to wait for some time, return ```WAIT```;
When you think the task can not be done, return ```FAIL```, don't easily say ```FAIL```, try your best to do the task;
When you think the task is done, return ```DONE```.

IMPORTANT - Coordinate System Enhancement:
The screenshot has been annotated with 5 red coordinate reference points to help you locate elements accurately:
- Top-left corner: (0, 0)
- Top-right corner: (width-1, 0)
- Bottom-left corner: (0, height-1)
- Bottom-right corner: (width-1, height-1)
- Center point: (width/2, height/2)
The screen resolution is displayed at the top center of the screenshot. Please use these reference points to accurately estimate the coordinates of target elements.

First give the current screenshot and previous things we did a short reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER EVER RETURN ME ANYTHING ELSE.
"""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ å†å²è½¨è¿¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_trajectory and len(self.trajectory_screenshots) > 0:
            # æœ€å¤šä¿ç•™æœ€è¿‘ 3 æ­¥çš„å†å²
            recent_history = min(3, len(self.trajectory_screenshots))
            for i in range(-recent_history, 0):
                # å†å²æˆªå›¾
                hist_content = [
                    {
                        "type": "text",
                        "text": f"[å†å²æ­¥éª¤ {len(self.trajectory_screenshots) + i + 1}]"
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{self.trajectory_screenshots[i]}"}
                    }
                ]
                
                # å†å²åŠ¨ä½œ
                if i < len(self.trajectory_actions):
                    hist_content.append({
                        "type": "text",
                        "text": f"æ‰§è¡Œçš„åŠ¨ä½œ: {', '.join(self.trajectory_actions[i])}"
                    })
                
                messages.append({"role": "user", "content": hist_content})
                messages.append({"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å·²è®°å½•ã€‚"})
        
        # å½“å‰ä»»åŠ¡å’Œæˆªå›¾
        screenshot_b64 = base64.b64encode(observation['screenshot']).decode('utf-8')
        
        current_content = [
            {
                "type": "text",
                "text": f"[å½“å‰ä»»åŠ¡]\nä»»åŠ¡æŒ‡ä»¤: {instruction}\n\nè¯·åˆ†æå½“å‰å±å¹•å¹¶è¿”å›ä¸‹ä¸€æ­¥åŠ¨ä½œï¼š"
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{screenshot_b64}"}
            }
        ]
        
        messages.append({"role": "user", "content": current_content})
        
        return messages
```

### 3.3 æ‰§è¡Œæµç¨‹å›¾

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant UI as Webç•Œé¢
    participant Agent as SimplePromptAgent
    participant Env as SimpleDesktopEnv
    participant VLM as è§†è§‰è¯­è¨€æ¨¡å‹
    participant OS as æ“ä½œç³»ç»Ÿ
    
    U->>UI: è¾“å…¥ä»»åŠ¡æŒ‡ä»¤
    UI->>Env: reset(task_config)
    Env->>OS: æˆªå–åˆå§‹å±å¹•
    OS-->>Env: screenshot
    Env-->>UI: åˆå§‹è§‚å¯Ÿ
    
    loop æ¯ä¸€æ­¥ (æœ€å¤š max_steps)
        UI->>Env: è·å–å½“å‰è§‚å¯Ÿ
        Env->>OS: æˆªå–å±å¹•
        OS-->>Env: screenshot
        Env-->>UI: observation
        
        UI->>Agent: predict(instruction, obs)
        Agent->>Agent: æ„å»ºæ¶ˆæ¯+å†å²
        Agent->>VLM: å‘é€æˆªå›¾+ä»»åŠ¡
        VLM-->>Agent: æ€è€ƒè¿‡ç¨‹+åŠ¨ä½œ
        Agent->>Agent: è§£æåŠ¨ä½œåºåˆ—
        Agent-->>UI: actions
        
        loop æ¯ä¸ªåŠ¨ä½œ
            UI->>Env: step(action)
            Env->>OS: æ‰§è¡Œ PyAutoGUI
            OS-->>Env: æ‰§è¡Œç»“æœ
            Env-->>UI: æ–°è§‚å¯Ÿ
            
            alt åŠ¨ä½œ == DONE
                UI-->>U: âœ… ä»»åŠ¡å®Œæˆ
            else åŠ¨ä½œ == FAIL
                UI-->>U: âŒ ä»»åŠ¡å¤±è´¥
            end
        end
    end
    
    UI-->>U: æ˜¾ç¤ºæ‰§è¡Œå†å²
```

---

## 4. å…³é”®æŠ€æœ¯è¯¦è§£

### 4.1 å†å²è½¨è¿¹ç®¡ç†

```python
def _update_trajectory(self, instruction: str, observation: Dict, 
                       response: str, actions: List[str]):
    """æ›´æ–°è½¨è¿¹å†å² - ä¸ºæ¨¡å‹æä¾›ä¸Šä¸‹æ–‡"""
    if self.use_trajectory:
        # ä¿å­˜æˆªå›¾ï¼ˆBase64 ç¼–ç ï¼‰
        screenshot_b64 = base64.b64encode(observation['screenshot']).decode('utf-8')
        self.trajectory_screenshots.append(screenshot_b64)
        
        # ä¿å­˜åŠ¨ä½œ
        self.trajectory_actions.append(actions)
        
        # é™åˆ¶å†å²é•¿åº¦ï¼ˆæœ€å¤šä¿ç•™ 5 æ­¥ï¼‰
        if len(self.trajectory_screenshots) > 5:
            self.trajectory_screenshots = self.trajectory_screenshots[-5:]
            self.trajectory_actions = self.trajectory_actions[-5:]
```

### 4.2 åŠ¨ä½œè§£æ - OSWorld é£æ ¼

```python
def _parse_actions(self, response_text: str) -> List[str]:
    """ä»æ¨¡å‹å“åº”ä¸­æå– PyAutoGUI åŠ¨ä½œ"""
    actions = []
    
    # æå–æ‰€æœ‰ pyautogui å‘½ä»¤
    lines = response_text.split('\n')
    for line in lines:
        line = line.strip()
        
        # åŒ¹é… pyautogui.xxx(...) æ ¼å¼
        if line.startswith('pyautogui.'):
            actions.append(line)
        
        # åŒ¹é…æ§åˆ¶ç¬¦
        elif line in ['DONE', 'FAIL', 'WAIT']:
            actions.append(line)
    
    return actions
```

### 4.3 åŠ¨ä½œæ‰§è¡Œ - ç›´æ¥ exec()

```python
def _execute_action(self, action: str):
    """æ‰§è¡Œ PyAutoGUI åŠ¨ä½œ - OSWorld æ–¹å¼"""
    if action.startswith('pyautogui.'):
        # åœ¨å®‰å…¨çš„å‘½åç©ºé—´ä¸­æ‰§è¡Œ
        namespace = {'pyautogui': self.controller}
        exec(action, namespace)
    elif action == 'WAIT':
        time.sleep(1.0)
    elif action in ['DONE', 'FAIL']:
        pass  # æ§åˆ¶ç¬¦ï¼Œä¸æ‰§è¡Œ
```

### 4.4 ä»»åŠ¡ä¸­æ–­æœºåˆ¶

**å…¨å±€çŠ¶æ€ç®¡ç†**ï¼š
```python
# ä»»åŠ¡çŠ¶æ€æ ‡å¿—
_task_running = False
_task_stop_flag = False
_task_lock = threading.Lock()

def should_stop_task() -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ä»»åŠ¡"""
    with _task_lock:
        return _task_stop_flag

def set_task_stop_flag(flag: bool):
    """è®¾ç½®åœæ­¢æ ‡å¿—"""
    with _task_lock:
        global _task_stop_flag
        _task_stop_flag = flag
```

**ä¸­æ–­æ£€æŸ¥ç‚¹**ï¼š
```python
while not done and step_count < max_steps:
    # æ£€æŸ¥ç‚¹ 1: æ­¥éª¤å¼€å§‹
    if should_stop_task():
        break
    
    # æ£€æŸ¥ç‚¹ 2: ç­‰å¾…å¼¹çª—æ—¶
    for _ in range(25):
        if should_stop_task():
            break
        time.sleep(0.1)
    
    # æ£€æŸ¥ç‚¹ 3: è°ƒç”¨æ¨¡å‹å‰
    if should_stop_task():
        break
    
    # æ£€æŸ¥ç‚¹ 4: æ‰§è¡ŒåŠ¨ä½œå‰
    for action in actions:
        if should_stop_task():
            break
```

**ESC é”®ç›‘å¬**ï¼š
```python
def _on_esc_pressed():
    """ESC é”®æŒ‰ä¸‹å›è°ƒ"""
    global _task_stop_flag
    if is_task_running():
        set_task_stop_flag(True)
        _show_autopilot_notification("âš ï¸ ESC é”®ä¸­æ–­\n\nä»»åŠ¡æ­£åœ¨åœæ­¢...")

def start_keyboard_listener():
    """å¯åŠ¨é”®ç›˜ç›‘å¬"""
    from pynput import keyboard
    
    def on_press(key):
        if key == keyboard.Key.esc:
            _on_esc_pressed()
    
    listener = keyboard.Listener(on_press=on_press)
    listener.daemon = True
    listener.start()
```

### 4.5 å¼¹çª—é€šçŸ¥ç³»ç»Ÿ

**è·¨å¹³å°å®ç°**ï¼š
```python
def _show_autopilot_notification(message: str):
    """æ˜¾ç¤º Autopilot çŠ¶æ€é€šçŸ¥"""
    system = platform.system()
    
    if system == "Darwin":  # macOS
        # osascript æ˜¾ç¤ºå¯¹è¯æ¡†
        script = f'''
        display dialog "ğŸ¤– Autopilot\\n\\n{message}" \\
        with title "Autopilot æ­£åœ¨æ‰§è¡Œ" \\
        buttons {{"æ‰§è¡Œä¸­..."}} \\
        default button 1 \\
        giving up after 2
        '''
        subprocess.Popen(['osascript', '-e', script])
        
    elif system == "Linux":
        # notify-send é€šçŸ¥
        subprocess.Popen(['notify-send', 'Autopilot', message])
        
    elif system == "Windows":
        # msg å‘½ä»¤
        subprocess.Popen(['msg', '*', f'Autopilot: {message}'])
```

### 4.6 æˆªå›¾ç›®å½•ç®¡ç†

**ä»»åŠ¡ä¸“å±ç›®å½•**ï¼š
```python
# ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹ç›®å½•
task_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
task_id = f"task_{task_timestamp}"
task_screenshot_dir = Path("data/gui_screenshots") / task_id
task_screenshot_dir.mkdir(parents=True, exist_ok=True)

# ä¿å­˜æˆªå›¾
screenshot_path = task_screenshot_dir / f"step_{step_count}_{timestamp}.png"
screenshot.save(screenshot_path)
```

**ç›®å½•ç»“æ„**ï¼š
```
data/gui_screenshots/              # ä¸»æˆªå›¾ç›®å½•ï¼ˆWeb ç•Œé¢ä½¿ç”¨ï¼‰
â”œâ”€â”€ task_20231115_120000/        # ä»»åŠ¡ 1ï¼ˆAutopilot è‡ªåŠ¨æ‰§è¡Œï¼‰
â”‚   â”œâ”€â”€ step_1_20231115_120005.png
â”‚   â”œâ”€â”€ step_2_20231115_120020.png
â”‚   â””â”€â”€ step_3_20231115_120035.png
â”œâ”€â”€ task_20231115_120100/        # ä»»åŠ¡ 2
â”‚   â””â”€â”€ ...
â””â”€â”€ manual_20231115_115900.png   # æ‰‹åŠ¨æˆªå›¾

data/gui_agent/screenshots/       # å¤‡ç”¨æˆªå›¾ç›®å½•ï¼ˆSimpleDesktopEnv ä½¿ç”¨ï¼‰
â””â”€â”€ step_*.png
```

---

## 5. ä½¿ç”¨æŒ‡å—

### 5.1 è™šæ‹Ÿæœºæ¨¡å¼ï¼ˆæ¨èï¼‰

**å¯åŠ¨è™šæ‹Ÿæœº**ï¼š
```bash
# ä½¿ç”¨ OSWorld Docker é•œåƒ
docker run -d \
  --name osworld-vm \
  -p 55000:5000 \
  -p 5901:5900 \
  xlangai/osworld:latest
```

**UI æ“ä½œ**ï¼š
1. è¿›å…¥ `ğŸ¤– GUI-Agent` æ ‡ç­¾é¡µ
2. ç‚¹å‡» "ğŸš€ å¯åŠ¨è™šæ‹Ÿæœº"
3. ç­‰å¾…å®¹å™¨å¯åŠ¨ï¼ˆçº¦ 30 ç§’ï¼‰
4. çœ‹åˆ° "âœ… è¿è¡Œä¸­" çŠ¶æ€

### 5.2 æœ¬åœ°æ¨¡å¼

**æƒé™é…ç½®**ï¼ˆmacOSï¼‰ï¼š
```
1. ç³»ç»Ÿè®¾ç½® â†’ éšç§ä¸å®‰å…¨æ€§ â†’ è¾…åŠ©åŠŸèƒ½
   - æ·»åŠ  Terminal/iTerm2
   - æ·»åŠ  Python
   
2. ç³»ç»Ÿè®¾ç½® â†’ éšç§ä¸å®‰å…¨æ€§ â†’ å±å¹•å½•åˆ¶
   - æ·»åŠ  Terminal/ITerm2
   - æ·»åŠ  Python
   
3. é‡å¯åº”ç”¨ç”Ÿæ•ˆ
```

**UI æ“ä½œ**ï¼š
1. é€‰æ‹© "æœ¬åœ°ç³»ç»Ÿ (Local)"
2. é…ç½®æ¨¡å‹å’Œ API Key
3. è¾“å…¥ä»»åŠ¡æŒ‡ä»¤
4. ç‚¹å‡» "â–¶ï¸ æ‰§è¡Œä»»åŠ¡"

### 5.3 æ‰‹åŠ¨æ§åˆ¶

**ç›´æ¥å‘é€åŠ¨ä½œ**ï¼š
```json
// ç‚¹å‡»
{"x": 500, "y": 300}

// è¾“å…¥æ–‡æœ¬
{"text": "Hello World"}

// æŒ‰é”®
{"key": "enter"}

// è‡ªå®šä¹‰å‘½ä»¤
{"command": "pyautogui.hotkey('command', 't')"}
```

### 5.4 æ¨¡å‹äº¤äº’

**å•æ¬¡äº¤äº’**ï¼š
1. åœ¨ "ğŸ“¸ æ‰‹åŠ¨æˆªå›¾" ä¸­æˆªå–å±å¹•
2. åœ¨ "ğŸ¤– æ¨¡å‹äº¤äº’" ä¸­ï¼š
   - é€‰æ‹©æˆªå›¾æ¥æº
   - è¾“å…¥ä»»åŠ¡æŒ‡ä»¤
   - é…ç½®æ¨¡å‹ï¼ˆQwen-VL æ¨èï¼‰
   - ç‚¹å‡» "ğŸš€ å‘é€ç»™æ¨¡å‹"
3. æŸ¥çœ‹æ¨¡å‹å“åº”å’Œè§£æçš„åŠ¨ä½œ
4. ç‚¹å‡» "â–¶ï¸ æ‰§è¡Œæ¨¡å‹è¿”å›çš„åŠ¨ä½œ"

### 5.5 è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡

**å®Œæ•´æµç¨‹**ï¼š
```python
ä»»åŠ¡ç¤ºä¾‹ï¼š
- "æ‰“å¼€æµè§ˆå™¨å¹¶æœç´¢ Python"
- "åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹å« test"
- "æ‰“å¼€ç»ˆç«¯å¹¶è¾“å…¥ ls"

é…ç½®ï¼š
- æœ€å¤§æ­¥æ•°ï¼š15
- æ¯æ­¥ç­‰å¾…ï¼š1.5 ç§’
- æ¨¡å‹ï¼šqwen3-vl-plus
- å¯ç”¨æ€è€ƒè¿‡ç¨‹ï¼šTrue
- ä½¿ç”¨å†å²è½¨è¿¹ï¼šTrue
```

**æ‰§è¡Œç›‘æ§**ï¼š
- ğŸ“ æ­¥éª¤è¿›åº¦å¼¹çª—
- ğŸ§  æ¨¡å‹æ€è€ƒæç¤º
- ğŸ¤– åŠ¨ä½œæ‰§è¡Œé€šçŸ¥
- âœ…/âŒ ä»»åŠ¡å®ŒæˆçŠ¶æ€

**ä¸­æ–­ä»»åŠ¡**ï¼š
- æŒ‰ ESC é”®ï¼ˆéœ€è¦è¾…åŠ©åŠŸèƒ½æƒé™ï¼‰
- ç‚¹å‡»æ‰§è¡Œæ–°ä»»åŠ¡ï¼ˆè‡ªåŠ¨ä¸­æ–­æ—§ä»»åŠ¡ï¼‰

---

## 6. æ€§èƒ½ä¸ä¼˜åŒ–

### 6.1 æ€§èƒ½æŒ‡æ ‡

| é˜¶æ®µ | è€—æ—¶ | ä¼˜åŒ–æ–¹å‘ |
|------|------|---------|
| **æˆªå›¾** | ~0.1s | é™ä½åˆ†è¾¨ç‡ |
| **æ¨¡å‹æ¨ç†** | 5-15s | ä½¿ç”¨ Flash æ¨¡å‹ |
| **åŠ¨ä½œæ‰§è¡Œ** | ~0.5s | å‡å°‘ç­‰å¾…æ—¶é—´ |
| **æ€»è€—æ—¶/æ­¥** | ~10-20s | å¹¶è¡ŒåŒ–å¤„ç† |

### 6.2 ä¼˜åŒ–ç­–ç•¥

```python
# 1. æˆªå›¾åˆ†è¾¨ç‡ä¼˜åŒ–
screenshot = screenshot.resize((1280, 720))

# 2. ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
model = "qwen3-vl-flash"  # ä»£æ›¿ qwen3-vl-plus

# 3. å‡å°‘å†å²ä¸Šä¸‹æ–‡
MAX_HISTORY = 3  # ä»£æ›¿ 5

# 4. å¼‚æ­¥é€šçŸ¥
subprocess.Popen(['osascript', ...])  # éé˜»å¡
```

### 6.3 æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ | æä¾›å•† | æ€è€ƒè¿‡ç¨‹ | æ¨èåœºæ™¯ |
|------|--------|---------|---------|
| **qwen3-vl-plus** | é˜¿é‡Œäº‘ | âœ… | é€šç”¨ä»»åŠ¡ï¼Œä¸­æ–‡ä¼˜ç§€ |
| **qwen3-vl-flash** | é˜¿é‡Œäº‘ | âœ… | å¿«é€Ÿå“åº” |
| **qvq-max** | é˜¿é‡Œäº‘ | âœ… | å¤æ‚æ¨ç† |
| **qvq-plus** | é˜¿é‡Œäº‘ | âœ… | è§†è§‰é—®ç­” |
| **gpt-4o** | OpenAI | âŒ | é«˜ç²¾åº¦ä»»åŠ¡ |
| **gpt-4-vision-preview** | OpenAI | âŒ | é€šç”¨è§†è§‰ç†è§£ |

---

## 7. æ•…éšœæ’é™¤

### 7.1 è™šæ‹Ÿæœºå¯åŠ¨å¤±è´¥

**é—®é¢˜**ï¼š`âŒ Docker åº“æœªå®‰è£…` æˆ– `âŒ æ— æ³•è¿æ¥åˆ° VM API`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. å®‰è£… Docker
brew install --cask docker  # macOS
sudo apt install docker.io   # Ubuntu

# 2. å¯åŠ¨ Docker æœåŠ¡
sudo systemctl start docker  # Linux
# macOS: å¯åŠ¨ Docker Desktop

# 3. æ‹‰å–é•œåƒ
docker pull xlangai/osworld:latest

# 4. æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps -a | grep osworld
docker logs osworld-vm
```

### 7.2 æƒé™é—®é¢˜ï¼ˆmacOSï¼‰

**é—®é¢˜**ï¼š`âš ï¸ éœ€è¦è¾…åŠ©åŠŸèƒ½æƒé™` æˆ– æˆªå›¾å¤±è´¥

**è§£å†³æ­¥éª¤**ï¼š
```
1. æ‰“å¼€ "ç³»ç»Ÿè®¾ç½®"
2. è¿›å…¥ "éšç§ä¸å®‰å…¨æ€§"
3. ç‚¹å‡» "è¾…åŠ©åŠŸèƒ½"
4. ç‚¹å‡» "+" æ·»åŠ åº”ç”¨ï¼š
   - Terminal (æˆ– iTerm2)
   - Python
5. å‹¾é€‰å…è®¸
6. é‡å¯åº”ç”¨
```

**éªŒè¯æƒé™**ï¼š
```bash
# æµ‹è¯•å±å¹•æˆªå›¾
python3 -c "import pyautogui; pyautogui.screenshot()"

# æµ‹è¯•é”®ç›˜ç›‘å¬
python3 -c "from pynput import keyboard; l = keyboard.Listener(lambda k: None); l.start(); import time; time.sleep(0.5); print('OK' if l.is_alive() else 'FAILED')"
```

### 7.3 æ¨¡å‹è°ƒç”¨å¤±è´¥

**é—®é¢˜**ï¼š`âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥` æˆ– API é”™è¯¯

**æ£€æŸ¥æ¸…å•**ï¼š
- [ ] API Key æ˜¯å¦æ­£ç¡®
- [ ] Base URL æ˜¯å¦æ­£ç¡®
- [ ] ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- [ ] æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
- [ ] API é¢åº¦æ˜¯å¦å……è¶³

**Qwen-VL é…ç½®**ï¼š
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export DASHSCOPE_API_KEY="your_api_key"

# æµ‹è¯• API
curl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
  -H "Authorization: Bearer $DASHSCOPE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl-plus", "messages": [{"role": "user", "content": "test"}]}'
```

### 7.4 åŠ¨ä½œæ‰§è¡Œå¤±è´¥

**é—®é¢˜**ï¼š`âŒ åŠ¨ä½œæ‰§è¡Œå¤±è´¥: list index out of range`

**æ’æŸ¥æ­¥éª¤**ï¼š
1. **VM æ¨¡å¼**ï¼š
   ```bash
   # æ£€æŸ¥ VM çŠ¶æ€
   docker exec osworld-vm ps aux | grep python
   
   # é‡å¯å®¹å™¨
   docker restart osworld-vm
   
   # æŸ¥çœ‹æ—¥å¿—
   docker logs osworld-vm --tail 50
   ```

2. **æœ¬åœ°æ¨¡å¼**ï¼š
   ```python
   # æ£€æŸ¥å±å¹•åˆ†è¾¨ç‡
   import pyautogui
   print(pyautogui.size())  # ç¡®ä¿åæ ‡åœ¨èŒƒå›´å†…
   
   # æµ‹è¯•å•ä¸ªåŠ¨ä½œ
   pyautogui.click(100, 100)
   ```

3. **åæ ‡éªŒè¯**ï¼š
   ```python
   # ç¡®ä¿åæ ‡æœ‰æ•ˆ
   screen_width, screen_height = pyautogui.size()
   if not (0 <= x < screen_width and 0 <= y < screen_height):
       print(f"åæ ‡è¶…å‡ºèŒƒå›´: ({x}, {y})")
   ```

---

## 8. æœ€ä½³å®è·µ

### 8.1 ä»»åŠ¡è®¾è®¡

```python
# âœ… å¥½çš„ä»»åŠ¡æè¿°
instruction = "æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® google.comï¼Œæœç´¢ 'Python tutorial'"

# âŒ ä¸å¥½çš„ä»»åŠ¡æè¿°
instruction = "æœç´¢"  # å¤ªæ¨¡ç³Š
```

### 8.2 æ­¥éª¤æ§åˆ¶

```python
# ç®€å•ä»»åŠ¡
max_steps = 5-10

# å¤æ‚ä»»åŠ¡
max_steps = 15-30

# é¿å…è¿‡é•¿
max_steps > 50  # å¯èƒ½å¯¼è‡´ä½æ•ˆ
```

### 8.3 æ¨¡å‹é€‰æ‹©

```python
# å¿«é€ŸåŸå‹ - ä½¿ç”¨ Flash
model = "qwen3-vl-flash"

# å¤æ‚ä»»åŠ¡ - ä½¿ç”¨ Plus
model = "qwen3-vl-plus"
enable_thinking = True

# æœ€é«˜ç²¾åº¦ - ä½¿ç”¨ GPT-4V
model = "gpt-4o"
```

### 8.4 ç¯å¢ƒå‡†å¤‡

```python
# æœ¬åœ°æ¨¡å¼ï¼šæ¸…ç†æ¡Œé¢
# - å…³é—­ä¸ç›¸å…³çª—å£
# - æ”¾å¤§ç›®æ ‡çª—å£
# - ç¡®ä¿å……è¶³çš„ç©ºé—´

# VM æ¨¡å¼ï¼šå¿«ç…§å¤‡ä»½
docker commit osworld-vm osworld-vm-backup
```

---

## 9. è¿›é˜¶æ‰©å±•

### 9.1 è‡ªå®šä¹‰ Agent

```python
class CustomAgent(SimplePromptAgent):
    """è‡ªå®šä¹‰ Agent - æ·»åŠ è§„åˆ’èƒ½åŠ›"""
    
    def predict_with_planning(self, instruction: str, observation: Dict):
        """å¸¦è§„åˆ’çš„é¢„æµ‹"""
        
        # 1. ç”Ÿæˆé«˜å±‚è®¡åˆ’
        plan = self._generate_plan(instruction, observation)
        
        # 2. æ‰§è¡Œå½“å‰æ­¥éª¤
        current_step = plan[0]
        response, actions = self.predict(current_step, observation)
        
        return response, actions, plan
    
    def _generate_plan(self, instruction: str, observation: Dict):
        """ç”Ÿæˆä»»åŠ¡è®¡åˆ’"""
        plan_prompt = f"""
        å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸ºå…·ä½“æ­¥éª¤ï¼š
        ä»»åŠ¡: {instruction}
        
        è¿”å›æ­¥éª¤åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªæ­¥éª¤ï¼‰ï¼š
        """
        
        response = self._call_model([{"role": "user", "content": plan_prompt}])
        steps = [s.strip() for s in response.split('\n') if s.strip()]
        
        return steps
```

### 9.2 å¤šæ¨¡æ€æ„ŸçŸ¥

```python
class MultimodalAgent(SimplePromptAgent):
    """å¤šæ¨¡æ€ Agent - æ”¯æŒ OCR å’Œç›®æ ‡æ£€æµ‹"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ocr_model = self._init_ocr()
        self.detector = self._init_detector()
    
    def predict(self, instruction: str, observation: Dict):
        """å¢å¼ºçš„é¢„æµ‹ - æ·»åŠ  OCR å’Œæ£€æµ‹ä¿¡æ¯"""
        
        # 1. OCR æå–æ–‡æœ¬
        ocr_text = self.ocr_model.extract(observation['screenshot'])
        
        # 2. ç›®æ ‡æ£€æµ‹
        objects = self.detector.detect(observation['screenshot'])
        
        # 3. å¢å¼ºè§‚å¯Ÿ
        enhanced_obs = observation.copy()
        enhanced_obs['ocr_text'] = ocr_text
        enhanced_obs['detected_objects'] = objects
        
        # 4. è°ƒç”¨åŸºç¡€é¢„æµ‹
        return super().predict(instruction, enhanced_obs)
```

### 9.3 å®‰å…¨æœºåˆ¶

#### 9.3.1 è™šæ‹Ÿæœºéš”ç¦»

**Docker å®¹å™¨æ¨¡å¼**ï¼š
```yaml
# OSWorld Docker é…ç½®
services:
  osworld-vm:
    image: xlangai/osworld:latest
    ports:
      - "55000:5000"  # API ç«¯å£
      - "5901:5900"   # VNC ç«¯å£
    environment:
      - DISPLAY=:1
    volumes:
      - ./data:/data
```

**éš”ç¦»ç‰¹æ€§**ï¼š
- âœ… ä¸ä¸»æœºå®Œå…¨éš”ç¦»
- âœ… å¯ä»¥å®‰å…¨æµ‹è¯•å±é™©æ“ä½œ
- âœ… æ”¯æŒå¿«ç…§å’Œå›æ»š
- âœ… å¤šå®ä¾‹å¹¶è¡Œæ‰§è¡Œ

---

## é™„å½•

### A. é…ç½®å‚è€ƒ

```python
# GUI-Agent é…ç½®
GUI_AGENT_CONFIG = {
    "screenshot_dir": "data/gui_screenshots",
    "max_steps": 50,
    "step_delay": 1.5,
    "max_history": 5,
    "screenshot_quality": 85,
    "screenshot_size": (1280, 720)
}

# Qwen-VL é…ç½®
QWEN_VL_CONFIG = {
    "model": "qwen3-vl-plus",
    "api_key": os.getenv("DASHSCOPE_API_KEY"),
    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "max_tokens": 512,
    "temperature": 0.0,
    "enable_thinking": True
}

# GPT-4V é…ç½®
GPT4V_CONFIG = {
    "model": "gpt-4o",
    "api_key": os.getenv("OPENAI_API_KEY"),
    "base_url": "https://api.openai.com/v1",
    "max_tokens": 1024,
    "temperature": 0.0
}
```

### B. API å‚è€ƒ

```python
# åˆ›å»ºç¯å¢ƒ
env = SimpleDesktopEnv(
    provider_name="local",
    os_type="macOS",
    action_space="pyautogui"
)

# é‡ç½®ç¯å¢ƒ
obs = env.reset(task_config={"instruction": "ä»»åŠ¡æŒ‡ä»¤"})

# æ‰§è¡Œæ­¥éª¤
obs, reward, done, info = env.step("pyautogui.click(x=100, y=200)")

# åˆ›å»º Agent
agent = SimplePromptAgent(
    model="qwen3-vl-plus",
    api_key="your_api_key",
    enable_thinking=True
)

# é¢„æµ‹åŠ¨ä½œ
response, actions = agent.predict(instruction, observation)
```

### C. ç›¸å…³èµ„æº

- [OSWorld é¡¹ç›®](https://github.com/xlang-ai/OSWorld)
- [Qwen-VL æ–‡æ¡£](https://help.aliyun.com/zh/model-studio/visual-reasoning)
- [PyAutoGUI æ–‡æ¡£](https://pyautogui.readthedocs.io/)
- [Gradio æ–‡æ¡£](https://gradio.app/docs/)

